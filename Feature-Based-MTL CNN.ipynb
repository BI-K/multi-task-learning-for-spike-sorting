{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:38:22,967\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import floor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "import helper\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Check if MPS (Apple Silicon) or CUDA (Nvidia) GPU is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Data Files Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(folder_path):\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if item == '.gitignore':\n",
    "            continue\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "clear_folder(\"ray_results\")\n",
    "clear_folder(\"outputs\")\n",
    "clear_folder(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Based MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO activation for now only relu\n",
    "class FeatureBasedMTLModel(nn.Module):\n",
    "    def __init__(self, hidden_layers_shared=[50,30], hidden_layers_outputs=[[10],[10],[10],[10],[10]], activation='relu', dropout_rate=0.3):\n",
    "        super(FeatureBasedMTLModel, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        # shared layers\n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1))\n",
    "        self.conv_layers.append(nn.BatchNorm1d(16))\n",
    "        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1))\n",
    "        self.conv_layers.append(nn.BatchNorm1d(32))\n",
    "        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "\n",
    "        input_dim = 32 * (30 // 4)  # Adjusted input dimension after convolution and pooling\n",
    "\n",
    "        for hidden_layer in hidden_layers_shared:\n",
    "            self.shared_layers.append(nn.Linear(input_dim, hidden_layer))\n",
    "            self.shared_layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_layer\n",
    "\n",
    "        # output layers\n",
    "        self.output_a2 = self.create_output_layers(hidden_layers_outputs[0], input_dim, 2)\n",
    "        self.output_a3 = self.create_output_layers(hidden_layers_outputs[1], input_dim, 2)\n",
    "        self.output_a4 = self.create_output_layers(hidden_layers_outputs[2], input_dim, 3)\n",
    "        self.output_a12 = self.create_output_layers(hidden_layers_outputs[3], input_dim, 6)\n",
    "        self.output_a21 = self.create_output_layers(hidden_layers_outputs[4], input_dim, 2)\n",
    "\n",
    "    def create_output_layers(self, hidden_layers, input_dim, output_dim):\n",
    "        layers = nn.ModuleList()\n",
    "        for hidden_layer in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_layer))\n",
    "            input_dim = hidden_layer\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task_id: str):\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.conv_layers:\n",
    "            if isinstance(conv, nn.Conv1d):\n",
    "                x = conv(x)\n",
    "                x = F.relu(x)\n",
    "            elif isinstance(conv, nn.MaxPool1d):\n",
    "                x = conv(x)\n",
    "            elif isinstance(conv, nn.BatchNorm1d):\n",
    "                x = conv(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the linear layers\n",
    "\n",
    "        # shared layers\n",
    "        for layer in self.shared_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = layer(x)\n",
    "                x = self.apply_activation(x)\n",
    "            elif isinstance(layer, nn.Dropout):\n",
    "                x = layer(x)\n",
    "        \n",
    "        # output layers\n",
    "        if task_id == 'a2':\n",
    "            return self.forward_output_layers(x, self.output_a2)\n",
    "        elif task_id == 'a3':\n",
    "            return self.forward_output_layers(x, self.output_a3)\n",
    "        elif task_id == 'a4':\n",
    "            return self.forward_output_layers(x, self.output_a4)\n",
    "        elif task_id == 'a12':\n",
    "            return self.forward_output_layers(x, self.output_a12)\n",
    "        elif task_id == 'a21':\n",
    "            return self.forward_output_layers(x, self.output_a21)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid task_id: {task_id}')\n",
    "        \n",
    "\n",
    "    def forward_output_layers(self, x, layers):\n",
    "        for layer in layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.apply_activation(x)\n",
    "            \n",
    "        x = layers[-1](x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def apply_activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return F.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid activation: {self.activation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_batches(X_train, y_train, batch_size):\n",
    "    X_train_batches = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n",
    "    y_train_batches = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n",
    "\n",
    "    for dataset in helper.dataset_list:\n",
    "\n",
    "        for i in range(0, floor(len(X_train[dataset])/batch_size)):\n",
    "                X_train_batches[dataset][str(i)] =  X_train[dataset].iloc[i* batch_size:i*batch_size+batch_size]\n",
    "                y_train_batches[dataset][str(i)] =  y_train[dataset].iloc[i* batch_size:i* batch_size+batch_size]\n",
    "\n",
    "    return X_train_batches, y_train_batches\n",
    "\n",
    "# batchsize in this case refers to the size of batch per dataset\n",
    "def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "        max_number_of_batches = max(len(X_train[dataset]) // batch_size for dataset in helper.dataset_list)\n",
    "\n",
    "        X_train, y_train = create_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                losses_per_epoch = {'a2': 0, 'a3': 0, 'a4': 0, 'a12': 0, 'a21': 0}\n",
    "\n",
    "                for batch_number in range(0, max_number_of_batches):\n",
    "\n",
    "                        for dataset in helper.dataset_list:\n",
    "\n",
    "                                if len(X_train[dataset]) > batch_number:\n",
    "\n",
    "                                        X_train_tensor = torch.tensor(X_train[dataset][str(batch_number)].values, dtype=torch.float32, device=device)\n",
    "                                        y_train_tensor= torch.tensor(y_train[dataset][str(batch_number)].values, dtype=torch.float32, device=device)\n",
    "                                        outputs = model(X_train_tensor, task_id = dataset)\n",
    "                                        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "                                        losses_per_epoch[dataset] += loss\n",
    "\n",
    "                for dataset in helper.dataset_list:\n",
    "                        losses_per_epoch[dataset] /= len(X_train[dataset])\n",
    "\n",
    "                # for now sum of average loss per dataset\n",
    "                loss = sum(losses_per_epoch.values())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def mcc(fn, fp, tn, tp):\n",
    "    return (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "def evaluate(model, X_test, y_test, device='cpu'):\n",
    "\n",
    "    eval_dict = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n",
    "    num_classes = {'a2': 2, 'a3': 2, 'a4': 3, 'a12': 6, 'a21': 2}\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for dataset in helper.dataset_list:\n",
    "        X_test_tensor = torch.tensor(X_test[dataset].values, dtype=torch.float32, device=device)\n",
    "        # y_test_tensor = torch.tensor(y_test[dataset].values, dtype=torch.float32)\n",
    "        y_pred_tensor = model(X_test_tensor, task_id=dataset)\n",
    "        y_pred = y_pred_tensor.detach().cpu().numpy()  # Move tensor to CPU before converting to NumPy\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # print(y_test[dataset])\n",
    "        y_test_np = y_test[dataset].to_numpy()\n",
    "        # print(y_test_np)\n",
    "        y_test_np = np.argmax(y_test_np, axis=1)\n",
    "        # print(y_test_np)\n",
    "\n",
    "        # Calculate classification metrics using one-hot encoded targets\n",
    "        eval_dict[dataset]['accuracy'] = accuracy_score(y_test_np, y_pred)\n",
    "        eval_dict[dataset]['micro_f1'] = f1_score(y_test_np, y_pred, average='micro')\n",
    "        eval_dict[dataset]['macro_f1'] = f1_score(y_test_np, y_pred, average='macro')\n",
    "        # gets 0 quite often...???\n",
    "        eval_dict[dataset]['mcc'] = matthews_corrcoef(y_test_np, y_pred)\n",
    "\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset_one_split(split_index, config):\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  helper.get_joined_train_test_folds(split_index)\n",
    "    # print(y_test)\n",
    "\n",
    "    model = FeatureBasedMTLModel(activation=config['activation'], hidden_layers_shared=config['shared_hidden_layers'], hidden_layers_outputs=[\n",
    "        config['a2_output_hidden_layers'], config['a3_output_hidden_layers'], config['a4_output_hidden_layers'], config['a12_output_hidden_layers'], config['a21_output_hidden_layers'],\n",
    "        config['dropout_rate']\n",
    "    ])\n",
    "\n",
    "    # Train the model and collect performance data\n",
    "    model = train(model, X_train, y_train, num_epochs=config['epochs'], batch_size=config['batch_size'], learning_rate=config['learning_rate'],)\n",
    "    # Evaluate the model and collect performance data\n",
    "    eval_dict = evaluate(model, X_test, y_test)\n",
    "\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_spike_1_split = {\n",
    "    'a2': pd.DataFrame(),\n",
    "    'a3': pd.DataFrame(),\n",
    "    'a4': pd.DataFrame(),\n",
    "    'a12': pd.DataFrame(),\n",
    "    'a21': pd.DataFrame()\n",
    "}\n",
    "\n",
    "df_data_spike_full_split = pd.DataFrame()\n",
    "\n",
    "data_spike_exec_1_split_dict = dict()\n",
    "data_spike_exec_full_split_dict = {\n",
    "    'a2': {},\n",
    "    'a3': {},\n",
    "    'a4': {},\n",
    "    'a12': {},\n",
    "    'a21': {}\n",
    "}\n",
    "\n",
    "\n",
    "best_params_list_getting = []\n",
    "\n",
    "def custom_trial_dirname(trial):\n",
    "    return f\"trial_{trial.trial_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config):\n",
    "    \n",
    "    global data_spike_exec_1_split_dict\n",
    "    global data_spike_exec_full_split_dict\n",
    "    global df_data_spike_1_split\n",
    "    global results_dir\n",
    "\n",
    "    overall_result = {\n",
    "    \"a2\": {\n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_macro_scores\": [],\n",
    "        \"f1_micro_scores\": [],\n",
    "        \"mcc_scores\": []\n",
    "    },\n",
    "    \"a3\": {\n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_macro_scores\": [],\n",
    "        \"f1_micro_scores\": [],\n",
    "        \"mcc_scores\": []\n",
    "    },\n",
    "    \"a4\": { \n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_macro_scores\": [],\n",
    "        \"f1_micro_scores\": [],\n",
    "        \"mcc_scores\": []\n",
    "    },\n",
    "    \"a12\": {\n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_macro_scores\": [],\n",
    "        \"f1_micro_scores\": [],\n",
    "        \"mcc_scores\": []\n",
    "    },\n",
    "    \"a21\": {\n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_macro_scores\": [],\n",
    "        \"f1_micro_scores\": [],\n",
    "        \"mcc_scores\": []\n",
    "    }\n",
    "    }\n",
    "\n",
    "    accuracy_scores = []\n",
    "    f1_macro_scores = []\n",
    "    f1_micro_scores = []\n",
    "    mcc_scores = []\n",
    "\n",
    "    data_spike_exec_1_split_dict = {}\n",
    "    for dataset in overall_result.keys():\n",
    "        data_spike_exec_1_split_dict[dataset] = pd.DataFrame()\n",
    "\n",
    "    session_id_for_df = session.get_trial_id()\n",
    "    print(type(session_id_for_df))\n",
    "    \n",
    "    # save individual results for each dataset\n",
    "    for i in range(5):\n",
    "        eval_dict = evaluate_model_on_dataset_one_split(i, config)\n",
    "        # fill all nan values in eval_dict with 0\n",
    "        for dataset in eval_dict.keys():\n",
    "            for key in eval_dict[dataset].keys():\n",
    "                if np.isnan(eval_dict[dataset][key]):\n",
    "                    eval_dict[dataset][key] = 0\n",
    "        \n",
    "        for dataset in overall_result.keys():\n",
    "\n",
    "            overall_result[dataset]['accuracy_scores'].append(eval_dict[dataset]['accuracy'])\n",
    "            overall_result[dataset]['f1_macro_scores'].append(eval_dict[dataset]['macro_f1'])\n",
    "            overall_result[dataset]['f1_micro_scores'].append(eval_dict[dataset]['micro_f1'])\n",
    "            overall_result[dataset]['mcc_scores'].append(eval_dict[dataset]['mcc'])\n",
    "\n",
    "            accuracy_scores.append(eval_dict[dataset]['accuracy'])\n",
    "            f1_macro_scores.append(eval_dict[dataset]['macro_f1'])\n",
    "            f1_micro_scores.append(eval_dict[dataset]['micro_f1'])\n",
    "            mcc_scores.append(eval_dict[dataset]['mcc'])\n",
    "            \n",
    "            new_entry= pd.DataFrame({\n",
    "            #data_spike_exec_1_split_dict[dataset][dataset + \"_\" + str(i+1) + \"_\" + session_id_for_df] = {\n",
    "                \"name\": [dataset + \"_\" + str(i+1) + \"_\" + session_id_for_df],\n",
    "                \"accuracy\": [eval_dict[dataset]['accuracy']],\n",
    "                \"macro f1\": [eval_dict[dataset]['macro_f1']],\n",
    "                \"micro_f1\": [eval_dict[dataset]['micro_f1']],\n",
    "                \"mcc\": [eval_dict[dataset]['mcc']],\n",
    "                \"config\": [str(config)]\n",
    "            })\n",
    "\n",
    "            if i == 0:\n",
    "                df_data_spike_1_split[dataset] = new_entry\n",
    "            else:\n",
    "                df_data_spike_1_split[dataset]= pd.concat([df_data_spike_1_split[dataset], new_entry], ignore_index=True, axis=0)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    for dataset in overall_result.keys():\n",
    "\n",
    "        #data_spike_exec_full_split_dict[dataset][dataset + \"_\" + session_id_for_df] = {\n",
    "        new_data_spike_exec_full_split_dict = pd.DataFrame({\n",
    "                \"name\" : [dataset + \"_\" + session_id_for_df],\n",
    "                \"min_accuracy\": [min(overall_result[dataset]['accuracy_scores'])],\n",
    "                \"max_accuracy\": [max(overall_result[dataset]['accuracy_scores'])],\n",
    "                \"min_f1_macro\": [min(overall_result[dataset]['f1_macro_scores'])],\n",
    "                \"max_f1_macro\": [max(overall_result[dataset]['f1_macro_scores'])],\n",
    "                \"min_mcc\": [min(overall_result[dataset]['mcc_scores'])],\n",
    "                \"max_mcc\": [max(overall_result[dataset]['mcc_scores'])],\n",
    "                \"mean_accuracy\": [np.mean(overall_result[dataset]['accuracy_scores'])],\n",
    "                \"mean_f1_macro\": [np.mean(overall_result[dataset]['f1_macro_scores'])],\n",
    "                \"mean_f1_micro\": [np.mean(overall_result[dataset]['f1_micro_scores'])],\n",
    "                \"mean_mcc\": [np.mean(overall_result[dataset]['mcc_scores'])],\n",
    "                \"std_accuracy\": [np.std(overall_result[dataset]['accuracy_scores'])],\n",
    "                \"std_f1_macro\": [np.std(overall_result[dataset]['f1_macro_scores'])],\n",
    "                \"std_f1_micro\": [np.std(overall_result[dataset]['f1_micro_scores'])],\n",
    "                \"std_mcc\": [np.std(overall_result[dataset]['mcc_scores'])],\n",
    "                \"config\": [str(config)]\n",
    "        })\n",
    "\n",
    "        full_split_path = os.path.join(helper.results_dir, f'Feature_based_output_full_{dataset}.pkl')\n",
    "        if os.path.exists(full_split_path):\n",
    "            df_existing_full = pd.read_pickle(full_split_path)\n",
    "            df_data_spike_full_split = pd.concat([df_existing_full, new_data_spike_exec_full_split_dict], ignore_index=True, axis=0)\n",
    "        else:\n",
    "            print(f\"No existing full split data found at {full_split_path}, creating new file.\")\n",
    "            df_data_spike_full_split = new_data_spike_exec_full_split_dict\n",
    "        \n",
    "        # Save the updated full split data to file\n",
    "        df_data_spike_full_split.to_pickle(full_split_path)\n",
    "\n",
    "\n",
    "        # Save the 1 split data using the full path\n",
    "        split_path = os.path.join(helper.results_dir, f'Feature_based_output_{dataset}.pkl')\n",
    "        if os.path.exists(split_path):\n",
    "            df_existing_1_spike = pd.read_pickle(split_path)\n",
    "            df_data_spike_1_split[dataset] = pd.concat([df_data_spike_1_split[dataset], df_existing_1_spike], ignore_index=True, axis=0)\n",
    "        else:\n",
    "            print(f\"No existing 1 split data found at {split_path}, creating new file\")\n",
    "\n",
    "        # Save the updated 1 split data to file\n",
    "        df_data_spike_1_split[dataset].to_pickle(split_path)\n",
    "\n",
    "    if np.min(accuracy_scores) == 0:\n",
    "        print(f\"Zero accuracy detected in config: {config}\")\n",
    "        print(f\"Accuracy scores: {accuracy_scores}\")\n",
    "\n",
    "    session.report({\n",
    "        \"min_mean_accuracy\": np.min(accuracy_scores),\n",
    "        \"max_mean_accuracy\": np.max(accuracy_scores),\n",
    "        \"mean_mean_accuracy\": np.mean(accuracy_scores),\n",
    "        \"mean_mean_f1_macro\": np.mean(f1_macro_scores),\n",
    "        \"mean_mean_f1_micro\": np.mean(f1_micro_scores),\n",
    "        \"mean_mean_mcc\": np.mean(mcc_scores)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hidden_layers_config(min_layers=1, max_layers=5, min_nodes=10, max_nodes=50, step=10):\n",
    "    possible_layers = []\n",
    "\n",
    "    for num_layers in range(min_layers, max_layers + 1):\n",
    "\n",
    "        shared_layers = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_layers))\n",
    "\n",
    "        max_output_layers = max_layers - num_layers\n",
    "        \n",
    "        output_layers = []\n",
    "        for num_output_layers in range(0, max_output_layers + 1):\n",
    "\n",
    "            output_layers = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_output_layers))\n",
    "\n",
    "        possible_layers.append({'shared': shared_layers, 'output': {'a2': output_layers, 'a3': output_layers, 'a4': output_layers, 'a12': output_layers, 'a21': output_layers}})\n",
    "\n",
    "    return possible_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_fold_cross_validation(num_layers):\n",
    "\n",
    "    num_layers = num_layers - 1\n",
    "\n",
    "    global best_params_list_getting\n",
    "\n",
    "    hidden_layers_options = generate_hidden_layers_config()\n",
    "\n",
    "    config = {\n",
    "        \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\"]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"batch_size\": tune.choice([32, 64, 128]),\n",
    "        \"shared_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['shared']),\n",
    "        \"a2_output_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['output']['a2']),\n",
    "        \"a3_output_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['output']['a3']),\n",
    "        \"a4_output_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['output']['a4']),\n",
    "        \"a12_output_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['output']['a12']),\n",
    "        \"a21_output_hidden_layers\": tune.choice(hidden_layers_options[num_layers]['output']['a21']),\n",
    "        \"epochs\": tune.choice([10, 20, 30, 40, 50]),\n",
    "        \"dropout_rate\": tune.uniform(0.2, 0.5)\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"mean_mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=10,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    \n",
    "    search_alg = OptunaSearch(metric=\"mean_mean_accuracy\", mode=\"max\")\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_and_evaluate),\n",
    "        resources_per_trial={\"cpu\": 10, \"gpu\": 0, \"accelerator_type:RTX\": 0},\n",
    "        config=config,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "        num_samples=32,\n",
    "        verbose=1,\n",
    "        storage_path=helper.ray_results_dir,\n",
    "        trial_dirname_creator=custom_trial_dirname\n",
    "    )\n",
    "\n",
    "    best_config_data_ray_tune = analysis.get_best_config(metric=\"mean_mean_accuracy\", mode=\"max\")\n",
    "    print(\"Best hyperparameters found were: \", best_config_data_ray_tune)\n",
    "    best_params_list_getting.append(best_config_data_ray_tune)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-07 12:29:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:51:10.02        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.3/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=14<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.6422162642748928<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>a12_output_hidden_la\n",
       "yers   </th><th>a21_output_hidden_la\n",
       "yers   </th><th>a2_output_hidden_lay\n",
       "ers   </th><th>a3_output_hidden_lay\n",
       "ers   </th><th>a4_output_hidden_lay\n",
       "ers   </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  learning_rate</th><th>shared_hidden_layers  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_mean_accuracy</th><th style=\"text-align: right;\">  max_mean_accuracy</th><th style=\"text-align: right;\">  mean_mean_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_and_evaluate_cb38cec8</td><td>TERMINATED</td><td>127.0.0.1:76664</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.476365</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000563697</td><td>(40, 40, 30, 40, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        165.686 </td><td style=\"text-align: right;\">           0.199482</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.671895</td></tr>\n",
       "<tr><td>train_and_evaluate_233bab18</td><td>TERMINATED</td><td>127.0.0.1:77049</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.482104</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">    0.00119404 </td><td>(30, 30, 30, 20, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.6664</td><td style=\"text-align: right;\">           0.134715</td><td style=\"text-align: right;\">           0.619048</td><td style=\"text-align: right;\">            0.39884 </td></tr>\n",
       "<tr><td>train_and_evaluate_8f2d14d2</td><td>TERMINATED</td><td>127.0.0.1:77139</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.229871</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">    0.00178952 </td><td>(40, 50, 30, 10, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.8618</td><td style=\"text-align: right;\">           0.136951</td><td style=\"text-align: right;\">           0.8     </td><td style=\"text-align: right;\">            0.475999</td></tr>\n",
       "<tr><td>train_and_evaluate_59b9f1fe</td><td>TERMINATED</td><td>127.0.0.1:77202</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.265983</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">    0.000672356</td><td>(40, 10, 40, 40, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         34.1476</td><td style=\"text-align: right;\">           0.165803</td><td style=\"text-align: right;\">           0.884058</td><td style=\"text-align: right;\">            0.561097</td></tr>\n",
       "<tr><td>train_and_evaluate_7578b5ac</td><td>TERMINATED</td><td>127.0.0.1:77285</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.219255</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">    0.000331594</td><td>(50, 30, 20, 50, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         26.9413</td><td style=\"text-align: right;\">           0.144703</td><td style=\"text-align: right;\">           0.695238</td><td style=\"text-align: right;\">            0.418623</td></tr>\n",
       "<tr><td>train_and_evaluate_9f80cdd9</td><td>TERMINATED</td><td>127.0.0.1:77355</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.379033</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">    0.00431144 </td><td>(10, 10, 20, 40, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         50.1878</td><td style=\"text-align: right;\">           0.147668</td><td style=\"text-align: right;\">           0.742857</td><td style=\"text-align: right;\">            0.411511</td></tr>\n",
       "<tr><td>train_and_evaluate_47577a77</td><td>TERMINATED</td><td>127.0.0.1:77474</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.403317</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">    0.00306019 </td><td>(30, 20, 10, 50, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.2232</td><td style=\"text-align: right;\">           0.139896</td><td style=\"text-align: right;\">           0.72381 </td><td style=\"text-align: right;\">            0.425046</td></tr>\n",
       "<tr><td>train_and_evaluate_ffe52c32</td><td>TERMINATED</td><td>127.0.0.1:77524</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.449205</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">    0.00317042 </td><td>(50, 10, 40, 40, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         27.1535</td><td style=\"text-align: right;\">           0.142487</td><td style=\"text-align: right;\">           0.638095</td><td style=\"text-align: right;\">            0.407993</td></tr>\n",
       "<tr><td>train_and_evaluate_743f7552</td><td>TERMINATED</td><td>127.0.0.1:77595</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.284871</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">    0.00199474 </td><td>(20, 30, 40, 20, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0251</td><td style=\"text-align: right;\">           0.145078</td><td style=\"text-align: right;\">           0.616822</td><td style=\"text-align: right;\">            0.415927</td></tr>\n",
       "<tr><td>train_and_evaluate_b56acc09</td><td>TERMINATED</td><td>127.0.0.1:77628</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.259135</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.00633192 </td><td>(20, 30, 30, 50, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        162.745 </td><td style=\"text-align: right;\">           0.15544 </td><td style=\"text-align: right;\">           0.72381 </td><td style=\"text-align: right;\">            0.414846</td></tr>\n",
       "<tr><td>train_and_evaluate_ee286b02</td><td>TERMINATED</td><td>127.0.0.1:77998</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.48764 </td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">    0.0016886  </td><td>(20, 50, 20, 10, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         27.146 </td><td style=\"text-align: right;\">           0.131783</td><td style=\"text-align: right;\">           0.855072</td><td style=\"text-align: right;\">            0.496236</td></tr>\n",
       "<tr><td>train_and_evaluate_a39593da</td><td>TERMINATED</td><td>127.0.0.1:78073</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.329914</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000189708</td><td>(10, 30, 40, 40, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        163.577 </td><td style=\"text-align: right;\">           0.178295</td><td style=\"text-align: right;\">           0.981308</td><td style=\"text-align: right;\">            0.666922</td></tr>\n",
       "<tr><td>train_and_evaluate_d7ef9015</td><td>TERMINATED</td><td>127.0.0.1:78450</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.312761</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000383694</td><td>(40, 50, 50, 30, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        133.003 </td><td style=\"text-align: right;\">           0.207254</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.673566</td></tr>\n",
       "<tr><td>train_and_evaluate_8564f253</td><td>TERMINATED</td><td>127.0.0.1:78756</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.328033</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000103015</td><td>(50, 20, 50, 40, 30)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        166.473 </td><td style=\"text-align: right;\">           0.204134</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.662392</td></tr>\n",
       "<tr><td>train_and_evaluate_82b76b9c</td><td>TERMINATED</td><td>127.0.0.1:79126</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.327062</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000535618</td><td>(40, 40, 40, 50, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        131.874 </td><td style=\"text-align: right;\">           0.219638</td><td style=\"text-align: right;\">           0.971963</td><td style=\"text-align: right;\">            0.669068</td></tr>\n",
       "<tr><td>train_and_evaluate_8bf2f270</td><td>TERMINATED</td><td>127.0.0.1:79461</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.423337</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000555805</td><td>(50, 50, 30, 40, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        133.328 </td><td style=\"text-align: right;\">           0.212435</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.664511</td></tr>\n",
       "<tr><td>train_and_evaluate_9d7ed2f2</td><td>TERMINATED</td><td>127.0.0.1:79775</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.425734</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000331035</td><td>(10, 20, 10, 20, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        131.617 </td><td style=\"text-align: right;\">           0.199482</td><td style=\"text-align: right;\">           0.971963</td><td style=\"text-align: right;\">            0.653992</td></tr>\n",
       "<tr><td>train_and_evaluate_0c6b012e</td><td>TERMINATED</td><td>127.0.0.1:80098</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.365469</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000296722</td><td>(40, 40, 30, 40, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        130.399 </td><td style=\"text-align: right;\">           0.186047</td><td style=\"text-align: right;\">           0.981308</td><td style=\"text-align: right;\">            0.663186</td></tr>\n",
       "<tr><td>train_and_evaluate_410a5691</td><td>TERMINATED</td><td>127.0.0.1:80388</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.367757</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000240772</td><td>(20, 30, 40, 50, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        131.598 </td><td style=\"text-align: right;\">           0.178756</td><td style=\"text-align: right;\">           0.971014</td><td style=\"text-align: right;\">            0.631052</td></tr>\n",
       "<tr><td>train_and_evaluate_9c74b418</td><td>TERMINATED</td><td>127.0.0.1:80703</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.293637</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000911251</td><td>(10, 30, 40, 30, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         78.7627</td><td style=\"text-align: right;\">           0.170984</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.662845</td></tr>\n",
       "<tr><td>train_and_evaluate_97fc7b58</td><td>TERMINATED</td><td>127.0.0.1:80892</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.283853</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000889931</td><td>(40, 50, 30, 10, 50)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.1346</td><td style=\"text-align: right;\">           0.217617</td><td style=\"text-align: right;\">           0.990654</td><td style=\"text-align: right;\">            0.671223</td></tr>\n",
       "<tr><td>train_and_evaluate_164d0d72</td><td>TERMINATED</td><td>127.0.0.1:81083</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.454584</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000118103</td><td>(40, 40, 50, 20, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        163.051 </td><td style=\"text-align: right;\">           0.181347</td><td style=\"text-align: right;\">           0.981308</td><td style=\"text-align: right;\">            0.66597 </td></tr>\n",
       "<tr><td>train_and_evaluate_75844a5a</td><td>TERMINATED</td><td>127.0.0.1:81472</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.302523</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000874385</td><td>(10, 10, 10, 10, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.2758</td><td style=\"text-align: right;\">           0.160622</td><td style=\"text-align: right;\">           0.985507</td><td style=\"text-align: right;\">            0.640816</td></tr>\n",
       "<tr><td>train_and_evaluate_6f6714b4</td><td>TERMINATED</td><td>127.0.0.1:81657</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.302992</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000879967</td><td>(50, 50, 20, 40, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.5566</td><td style=\"text-align: right;\">           0.181347</td><td style=\"text-align: right;\">           0.962617</td><td style=\"text-align: right;\">            0.654889</td></tr>\n",
       "<tr><td>train_and_evaluate_44366b67</td><td>TERMINATED</td><td>127.0.0.1:81869</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.244926</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000514385</td><td>(50, 40, 50, 40, 30)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.6767</td><td style=\"text-align: right;\">           0.196891</td><td style=\"text-align: right;\">           0.962617</td><td style=\"text-align: right;\">            0.664476</td></tr>\n",
       "<tr><td>train_and_evaluate_e85f2a2b</td><td>TERMINATED</td><td>127.0.0.1:82054</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.200163</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">    0.000415781</td><td>(50, 10, 30, 10, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.5874</td><td style=\"text-align: right;\">           0.199482</td><td style=\"text-align: right;\">           0.971963</td><td style=\"text-align: right;\">            0.663421</td></tr>\n",
       "<tr><td>train_and_evaluate_65d75bca</td><td>TERMINATED</td><td>127.0.0.1:82272</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.344205</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">    0.000397947</td><td>(20, 40, 30, 20, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.3764</td><td style=\"text-align: right;\">           0.194301</td><td style=\"text-align: right;\">           0.962617</td><td style=\"text-align: right;\">            0.629799</td></tr>\n",
       "<tr><td>train_and_evaluate_48084630</td><td>TERMINATED</td><td>127.0.0.1:82351</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.343868</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">    0.00133565 </td><td>(40, 30, 30, 20, 20)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.6739</td><td style=\"text-align: right;\">           0.173575</td><td style=\"text-align: right;\">           0.942029</td><td style=\"text-align: right;\">            0.603656</td></tr>\n",
       "<tr><td>train_and_evaluate_a55a1e86</td><td>TERMINATED</td><td>127.0.0.1:82476</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.276144</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.00124521 </td><td>(50, 20, 40, 30, 30)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        132.354 </td><td style=\"text-align: right;\">           0.222798</td><td style=\"text-align: right;\">           0.971014</td><td style=\"text-align: right;\">            0.674086</td></tr>\n",
       "<tr><td>train_and_evaluate_3b936601</td><td>TERMINATED</td><td>127.0.0.1:82681</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.278862</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000226092</td><td>(40, 50, 30, 50, 40)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        132.382 </td><td style=\"text-align: right;\">           0.194301</td><td style=\"text-align: right;\">           0.981308</td><td style=\"text-align: right;\">            0.643617</td></tr>\n",
       "<tr><td>train_and_evaluate_e6ab500f</td><td>TERMINATED</td><td>127.0.0.1:82891</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.468026</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.00018545 </td><td>(20, 20, 50, 50, 10)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        131.359 </td><td style=\"text-align: right;\">           0.170984</td><td style=\"text-align: right;\">           0.934579</td><td style=\"text-align: right;\">            0.514198</td></tr>\n",
       "<tr><td>train_and_evaluate_b6f1e915</td><td>TERMINATED</td><td>127.0.0.1:83095</td><td>()</td><td>()</td><td>()</td><td>()</td><td>()</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.470912</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    0.000151191</td><td>(20, 50, 10, 50, 30)  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        132.334 </td><td style=\"text-align: right;\">           0.142487</td><td style=\"text-align: right;\">           0.733333</td><td style=\"text-align: right;\">            0.400075</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:41:11,795\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 40, 30, 40, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_full_a2.pkl, creating new file.\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_a2.pkl, creating new file\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_full_a3.pkl, creating new file.\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_a3.pkl, creating new file\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_full_a4.pkl, creating new file.\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_a4.pkl, creating new file\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_full_a12.pkl, creating new file.\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_a12.pkl, creating new file\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_full_a21.pkl, creating new file.\n",
      "\u001b[36m(train_and_evaluate pid=76664)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/Feature_based_output_a21.pkl, creating new file\n",
      "\u001b[36m(train_and_evaluate pid=77049)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:41:48,293\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (30, 30, 30, 20, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77139)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:42:10,413\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 50, 30, 10, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77202)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:42:47,752\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 10, 40, 40, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77285)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:43:17,539\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 30, 20, 50, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77355)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:44:10,808\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (10, 10, 20, 40, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77474)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:44:23,988\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (30, 20, 10, 50, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77524)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:44:53,931\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 10, 40, 40, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77595)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:45:06,853\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 30, 40, 20, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77628)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:47:52,654\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 30, 30, 50, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=77998)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:48:22,636\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 50, 20, 10, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=78073)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:51:08,755\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (10, 30, 40, 40, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=78450)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:53:24,943\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 50, 50, 30, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=78756)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:56:14,583\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 20, 50, 40, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=79126)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 11:58:29,230\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 40, 40, 50, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=79461)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:00:45,990\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 50, 30, 40, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=79775)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:03:00,578\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (10, 20, 10, 20, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=80098)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:05:14,677\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 40, 30, 40, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=80388)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:07:29,861\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 30, 40, 50, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=80703)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:08:52,296\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (10, 30, 40, 30, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=80892)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:10:19,569\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 50, 30, 10, 50), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=81083)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:13:05,508\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 40, 50, 20, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=81472)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:14:30,194\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (10, 10, 10, 10, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=81657)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:15:56,657\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 50, 20, 40, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=81869)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:17:22,693\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 40, 50, 40, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82054)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:18:48,538\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 10, 30, 10, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82272)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:19:24,671\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 40, 30, 20, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82351)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:20:33,914\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 30, 30, 20, 20), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82476)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:22:49,002\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (50, 20, 40, 30, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82681)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:25:04,244\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (40, 50, 30, 50, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=82891)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:27:18,645\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 20, 50, 50, 10), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_and_evaluate pid=83095)\u001b[0m <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:29:33,699\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'shared_hidden_layers': (20, 50, 10, 50, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': ()}\n",
      "2024-08-07 12:29:33,718\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-07_11-38-23' in 0.0171s.\n",
      "2024-08-07 12:29:33,726\tINFO tune.py:1041 -- Total run time: 3070.11 seconds (3070.00 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.0012452118788656846, 'batch_size': 32, 'shared_hidden_layers': (50, 20, 40, 30, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': (), 'epochs': 40, 'dropout_rate': 0.27614417651659295}\n"
     ]
    }
   ],
   "source": [
    "analysis = five_fold_cross_validation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_A2 = pd.read_pickle(\"results/Feature_based_output_A2.pkl\")\n",
    "df_dict_A3 = pd.read_pickle(\"results/Feature_based_output_A3.pkl\")\n",
    "df_dict_A4 = pd.read_pickle(\"results/Feature_based_output_A4.pkl\")\n",
    "df_dict_A12 = pd.read_pickle(\"results/Feature_based_output_A12.pkl\")\n",
    "df_dict_A21 = pd.read_pickle(\"results/Feature_based_output_A21.pkl\")\n",
    "\n",
    "df_dict = pd.concat([df_dict_A2, df_dict_A3, df_dict_A4, df_dict_A12, df_dict_A21])\n",
    "\n",
    "df_dict = df_dict.loc[:,~df_dict.columns.duplicated()].copy()\n",
    "df_dict = df_dict.drop_duplicates()\n",
    "\n",
    "df_dict.to_csv('outputs/Feature_based_one_split_metrics_system.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_full_A2 = pd.read_pickle(\"results/Feature_based_output_full_A2.pkl\")\n",
    "df_dict_full_A3 = pd.read_pickle(\"results/Feature_based_output_full_A3.pkl\")\n",
    "df_dict_full_A4 = pd.read_pickle(\"results/Feature_based_output_full_A4.pkl\")\n",
    "df_dict_full_A12 = pd.read_pickle(\"results/Feature_based_output_full_A12.pkl\")\n",
    "df_dict_full_A21 = pd.read_pickle(\"results/Feature_based_output_full_A21.pkl\")\n",
    "\n",
    "df_full_dict = pd.concat([df_dict_full_A2, df_dict_full_A3, df_dict_full_A4, df_dict_full_A12, df_dict_full_A21])\n",
    "\n",
    "df_full_dict = df_full_dict.loc[:, ~df_full_dict.columns.duplicated()].copy()\n",
    "df_full_dict = df_full_dict.drop_duplicates()\n",
    "\n",
    "df_full_dict.to_csv(\"outputs/Feature_based__full_split_metrics_system.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_full_A2.to_csv(\"outputs/Feature_based_full_split_metrics_A2.csv\")\n",
    "df_dict_full_A3.to_csv(\"outputs/Feature_based_full_split_metrics_A3.csv\")\n",
    "df_dict_full_A4.to_csv(\"outputs/Feature_based_full_split_metrics_A4.csv\")\n",
    "df_dict_full_A12.to_csv(\"outputs/Feature_based_full_split_metrics_A12.csv\")\n",
    "df_dict_full_A21.to_csv(\"outputs/Feature_based_full_split_metrics_A21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.0012452118788656846, 'batch_size': 32, 'shared_hidden_layers': (50, 20, 40, 30, 30), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': (), 'epochs': 40, 'dropout_rate': 0.27614417651659295} is 0.6740860602234837\n",
      "Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.0028824222182442364, 'batch_size': 128, 'shared_hidden_layers': (50, 40, 50, 50), 'a2_output_hidden_layers': (30,), 'a3_output_hidden_layers': (20,), 'a4_output_hidden_layers': (40,), 'a12_output_hidden_layers': (40,), 'a21_output_hidden_layers': (40,), 'epochs': 50, 'dropout_rate': 0.26279839925953097} is 0.6748552240004464\n",
      "Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.00018291692613022902, 'batch_size': 64, 'shared_hidden_layers': (50, 40, 30), 'a2_output_hidden_layers': (40, 30), 'a3_output_hidden_layers': (10, 30), 'a4_output_hidden_layers': (30, 40), 'a12_output_hidden_layers': (10, 40), 'a21_output_hidden_layers': (50, 30), 'epochs': 50, 'dropout_rate': 0.4320852934863742} is 0.6766179922727174\n",
      "Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.0005948567447165425, 'batch_size': 128, 'shared_hidden_layers': (40, 40), 'a2_output_hidden_layers': (50, 50, 20), 'a3_output_hidden_layers': (10, 20, 10), 'a4_output_hidden_layers': (30, 20, 40), 'a12_output_hidden_layers': (50, 20, 50), 'a21_output_hidden_layers': (50, 20, 20), 'epochs': 50, 'dropout_rate': 0.4949465749359502} is 0.688013421076455\n",
      "Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.0010403297388100502, 'batch_size': 32, 'shared_hidden_layers': (40,), 'a2_output_hidden_layers': (50, 30, 10, 10), 'a3_output_hidden_layers': (20, 30, 10, 20), 'a4_output_hidden_layers': (40, 30, 40, 20), 'a12_output_hidden_layers': (40, 50, 30, 50), 'a21_output_hidden_layers': (20, 20, 20, 20), 'epochs': 50, 'dropout_rate': 0.3364741105922103} is 0.6904200512645706\n",
      "Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.000818691995074505, 'batch_size': 32, 'shared_hidden_layers': (50, 40, 30, 30, 40), 'a2_output_hidden_layers': (), 'a3_output_hidden_layers': (), 'a4_output_hidden_layers': (), 'a12_output_hidden_layers': (), 'a21_output_hidden_layers': (), 'epochs': 40, 'dropout_rate': 0.2100022481853727} is 0.6720032287853899\n",
      "The best config is {'activation': 'relu', 'learning_rate': 0.0010403297388100502, 'batch_size': 32, 'shared_hidden_layers': (40,), 'a2_output_hidden_layers': (50, 30, 10, 10), 'a3_output_hidden_layers': (20, 30, 10, 20), 'a4_output_hidden_layers': (40, 30, 40, 20), 'a12_output_hidden_layers': (40, 50, 30, 50), 'a21_output_hidden_layers': (20, 20, 20, 20), 'epochs': 50, 'dropout_rate': 0.3364741105922103} with a mean accuracy of 0.6904200512645706\n",
      "             name  mean_accuracy  std_accuracy  mean_f1_macro  std_f1_macro  \\\n",
      "159   a2_73a95122       0.940308      0.026143       0.940300      0.026150   \n",
      "351   a3_73a95122       0.921988      0.028328       0.921897      0.028393   \n",
      "543   a4_73a95122       0.613994      0.062876       0.605536      0.063132   \n",
      "735  a12_73a95122       0.274320      0.024966       0.225295      0.022401   \n",
      "927  a21_73a95122       0.701491      0.045487       0.660043      0.041946   \n",
      "\n",
      "     mean_f1_micro  std_f1_micro  mean_mcc   std_mcc  \n",
      "159       0.940308      0.026143  0.880731  0.052323  \n",
      "351       0.921988      0.028328  0.845377  0.055120  \n",
      "543       0.613994      0.062876  0.431376  0.083801  \n",
      "735       0.274320      0.024966  0.137446  0.032843  \n",
      "927       0.701491      0.045487  0.365964  0.084790  \n"
     ]
    }
   ],
   "source": [
    "# print the configs and results for the hyperparemters wiht the highest mean accuracy\n",
    "# read df_full_dict and print columns with configs in best_params_list_getting\n",
    "df_full_dict = pd.read_csv(\"outputs/Feature_based__full_split_metrics_system.csv\")\n",
    "\n",
    "config_acc_dict = {}\n",
    "for good_param in best_params_list_getting:\n",
    "    # get the row witht the highest mean mean accuracy\n",
    "    mean_accuracy = df_full_dict.loc[df_full_dict['config'] == str(good_param)]['mean_accuracy'].mean()\n",
    "    config_acc_dict[str(good_param)] = mean_accuracy\n",
    "    print(f\"Mean accuracy for config {good_param} is {mean_accuracy}\")\n",
    "\n",
    "# get the best config\n",
    "best_config = max(config_acc_dict, key=config_acc_dict.get)\n",
    "print(f\"The best config is {best_config} with a mean accuracy of {config_acc_dict[best_config]}\")\n",
    "\n",
    "#print aggregate results for the best config\n",
    "metrics = ['name', 'mean_accuracy', 'std_accuracy', 'mean_f1_macro', 'std_f1_macro', 'mean_f1_micro', 'std_f1_micro', 'mean_mcc', 'std_mcc']\n",
    "print(df_full_dict[metrics].loc[df_full_dict['config'] == best_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
