{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Prep"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["## Please run them if you don't have the following libraries or have created a new environment.\n","\n","# import sys\n","# !{sys.executable} -m pip install numpy pandas scikit-learn matplotlib optuna hyperopt jupyter"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# !{sys.executable} -m pip install tensorflow"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# !{sys.executable} -m pip install -U \"ray[data,train,tune,serve]\""]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:03.334868Z","iopub.status.busy":"2024-07-10T10:30:03.334549Z","iopub.status.idle":"2024-07-10T10:30:13.163038Z","shell.execute_reply":"2024-07-10T10:30:13.161739Z","shell.execute_reply.started":"2024-07-10T10:30:03.334833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-22 18:21:29,044\tINFO worker.py:1621 -- Calling ray.init() again after it has already been called.\n"]},{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import f1_score, matthews_corrcoef\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n","\n","import psutil\n","import time\n","import logging\n","from datetime import datetime\n","import random\n","import os\n","import shutil\n","import itertools\n","\n","import ray\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.air import session\n","from ray.tune.integration.keras import TuneReportCallback\n","from ray.tune.search.optuna import OptunaSearch\n","from ray.tune.tuner import Tuner, TuneConfig\n","from ray.train import RunConfig\n","from ray.tune import Trainable\n","\n","import helper\n","\n","# Initialize Ray\n","ray.init(ignore_reinit_error=True)\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","tf.debugging.set_log_device_placement(True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","files_to_delete = [\"results/output_A2.pkl\", \"results/output_A3.pkl\", \"results/output_A4.pkl\", \"results/output_A12.pkl\", \"results/output_A21.pkl\"]\n","\n","for file in files_to_delete:\n","    if os.path.exists(file):\n","        os.remove(file)\n","    else:\n","        print(f\"The file {file} does not exist.\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def delete_folder_contents(folder_path):\n","    # Check if the folder exists\n","    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n","        # List all the files and directories in the folder\n","        for filename in os.listdir(folder_path):\n","            # Skip .gitignore files\n","            if filename == '.gitignore':\n","                continue\n","            \n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                # Check if it's a file or directory and remove accordingly\n","                if os.path.isfile(file_path) or os.path.islink(file_path):\n","                    os.unlink(file_path)\n","                elif os.path.isdir(file_path):\n","                    shutil.rmtree(file_path)\n","            except Exception as e:\n","                print(f'Failed to delete {file_path}. Reason: {e}')\n","    else:\n","        print(f'The folder {folder_path} does not exist or is not a directory.')\n","\n","folder_path = 'results'\n","\n","# Call the function to delete all contents\n","delete_folder_contents(folder_path)\n","\n","folder_path = 'ray_results'\n","\n","# Call the function to delete all contents\n","delete_folder_contents(folder_path)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["seed = 42\n","\n","tf.random.set_seed(seed)\n","tf.keras.utils.set_random_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Simple FNN"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:16.513190Z","iopub.status.busy":"2024-07-10T10:30:16.512565Z","iopub.status.idle":"2024-07-10T10:30:16.519877Z","shell.execute_reply":"2024-07-10T10:30:16.518848Z","shell.execute_reply.started":"2024-07-10T10:30:16.513154Z"},"trusted":true},"outputs":[],"source":["# Neural Network Definition\n","def create_model(num_classes, activation='relu', learning_rate=0.001, hidden_layers=[30, 15], dropout_rate=0.5):\n","    model = Sequential()\n","    model.add(Dense(hidden_layers[0], input_shape=(30,), activation=activation))\n","    if len(hidden_layers) > 1:\n","        model.add(Dropout(dropout_rate))\n","    for units in hidden_layers[1:]:\n","        model.add(Dense(units, activation=activation))\n","        model.add(Dropout(dropout_rate))\n","\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:17.291825Z","iopub.status.busy":"2024-07-10T10:30:17.290934Z","iopub.status.idle":"2024-07-10T10:30:17.298631Z","shell.execute_reply":"2024-07-10T10:30:17.297583Z","shell.execute_reply.started":"2024-07-10T10:30:17.291782Z"},"trusted":true},"outputs":[],"source":["def train(model, X_train, y_train, num_epochs, batch_size):\n","    model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=0)\n","    return model\n","\n","def evaluate(model, X_test, y_test):\n","    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","    y_pred = model.predict(X_test)\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","    y_test_classes = np.argmax(y_test, axis=1)\n","\n","    f1_macro = f1_score(y_test_classes, y_pred_classes, average='macro')\n","    f1_micro = f1_score(y_test_classes, y_pred_classes, average='micro')\n","    mcc = matthews_corrcoef(y_test_classes, y_pred_classes)\n","    \n","\n","    return accuracy, f1_macro, f1_micro, mcc"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:17.753453Z","iopub.status.busy":"2024-07-10T10:30:17.753116Z","iopub.status.idle":"2024-07-10T10:30:17.765149Z","shell.execute_reply":"2024-07-10T10:30:17.764141Z","shell.execute_reply.started":"2024-07-10T10:30:17.753425Z"},"trusted":true},"outputs":[],"source":["def evaluate_model_on_dataset_one_split(config, split, split_index):\n","    test_set = split[split_index]\n","    indices = [0, 1, 2, 3, 4]\n","    indices.remove(split_index)\n","    train_splits = [split[i] for i in indices]\n","    train_set = pd.concat(train_splits, axis=0)\n","\n","    classes = train_set['track'].unique()\n","    num_classes = len(classes)\n","    #print(\"number of classes: \" + str(num_classes))\n","\n","    one_hot_columns = train_set['track'].unique()\n","    one_hot = pd.get_dummies(train_set['track'])\n","    train_set = train_set.drop('track', axis=1)\n","    train_set = train_set.join(one_hot).astype(float)\n","\n","    one_hot_columns = test_set['track'].unique()\n","    one_hot = pd.get_dummies(test_set['track'])\n","    test_set = test_set.drop('track', axis=1)\n","    test_set = test_set.join(one_hot).astype(float)\n","\n","    X_train = train_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_train = train_set[one_hot_columns].values.reshape(-1, num_classes)\n","    X_test = test_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_test = test_set[one_hot_columns].values.reshape(-1, num_classes)\n","\n","    model = create_model(num_classes, activation=config['activation'], learning_rate=config['learning_rate'], hidden_layers=config['hidden_layers'], dropout_rate=config['dropout_rate'])\n","\n","    # Train the model and collect performance data\n","    model= train(model, X_train, y_train, config['epochs'], config['batch_size'])\n","    # Evaluate the model and collect performance data\n","    accuracy, f1_macro, f1_micro, mcc = evaluate(model, X_test, y_test)\n","\n","\n","    return accuracy, f1_macro, f1_micro, mcc, num_classes"]},{"cell_type":"markdown","metadata":{},"source":["## 5-Fold-Cross Validation"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["df_data_spike_1_split = pd.DataFrame()\n","df_data_spike_full_split = pd.DataFrame()\n","\n","data_spike_exec_1_split_dict = dict()\n","data_spike_exec_full_split_dict = dict()\n","\n","\n","best_params_list_getting = []\n","\n","def custom_trial_dirname(trial):\n","    return f\"trial_{trial.trial_id}\""]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def train_and_evaluate(config, splits, splits_name):\n","    \n","    global data_spike_exec_1_split_dict\n","    global data_spike_exec_full_split_dict\n","    global df_data_spike_1_split\n","    global results_dir\n","    \n","    min_accuracy = 1\n","    max_accuracy = 0\n","    avg_accuracy = 0\n","    min_f1_macro = 1\n","    max_f1_macro = 0\n","    avg_f1_macro = 0\n","    min_f1_micro = 1\n","    max_f1_micro = 0\n","    avg_f1_micro = 0\n","    min_mcc = 1\n","    max_mcc = -1\n","    avg_mcc = 0\n","\n","    num_classes = 0\n","    \n","    accuracies = []\n","    f1_macro_scores = []\n","    f1_micro_scores = []\n","    mcc_scores = []\n","\n","    session_id_for_df = session.get_trial_id()\n","    \n","    for i in range(5):\n","        accuracy, f1_macro, f1_micro, mcc, num_classes = evaluate_model_on_dataset_one_split(config, splits, i)\n","        accuracies.append(accuracy)\n","        f1_macro_scores.append(f1_macro)\n","        f1_micro_scores.append(f1_micro)\n","        mcc_scores.append(mcc)\n","        \n","        avg_accuracy += accuracy\n","        avg_f1_macro += f1_macro\n","        avg_f1_micro += f1_micro\n","        min_accuracy = min(min_accuracy, accuracy)\n","        max_accuracy = max(max_accuracy, accuracy)\n","        min_f1_macro = min(min_f1_macro, f1_macro)\n","        max_f1_macro = max(max_f1_macro, f1_macro)\n","        min_f1_micro = min(min_f1_micro, f1_micro)\n","        max_f1_micro = max(max_f1_micro, f1_micro)\n","        avg_mcc += mcc\n","        min_mcc = min(min_mcc, mcc)\n","        max_mcc = max(max_mcc, mcc)\n","\n","\n","        \n","        data_spike_exec_1_split_dict[splits_name + \"_\" + str(i+1) + \"_\" + session_id_for_df] = {\n","            \"number of classes\": num_classes,\n","            \"accuracy\": accuracy,\n","            \"macro f1\": f1_macro,\n","            \"micro_f1\": f1_micro,\n","            \"mcc\": mcc,\n","            \"config\": str(config)\n","        }\n","        \n","       \n","    temp_df = pd.DataFrame.from_dict(data_spike_exec_1_split_dict, orient='index')\n","    df_data_spike_1_split = pd.concat([df_data_spike_1_split, temp_df], axis=0)\n","\n","        # df_data_spike_1_split = pd.concat([df_data_spike_1_split, pd.DataFrame.from_dict(data_spike_exec_1_split_dict)], axis=1)\n","\n","        \n","    avg_accuracy /= 5\n","    avg_f1_macro /= 5\n","    avg_f1_micro /= 5\n","    avg_mcc /= 5\n","    \n","    data_spike_exec_full_split_dict[splits_name + \"_\" + session_id_for_df] = {\n","        \"number of classes\": num_classes,\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"min_mcc\": min_mcc,\n","        \"max_mcc\": max_mcc,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"mean_f1_micro\": np.mean(f1_micro_scores),\n","        \"mean_mcc\": np.mean(mcc_scores),\n","        \"std_accuracy\": np.std(accuracies),\n","        \"std_f1_macro\": np.std(f1_macro_scores),\n","        \"std_f1_micro\": np.std(f1_micro_scores),\n","        \"std_mcc\": np.std(mcc_scores),\n","        \"config\": str(config)\n","    }\n","    \n","\n","    df_data_spike_full_split = pd.DataFrame.from_dict(data_spike_exec_full_split_dict, orient='index')\n","\n","    # Load existing data from file if it exists and append new data\n","    full_split_path = os.path.join(helper.results_dir, f'output_full_{splits_name}.pkl')\n","    if os.path.exists(full_split_path):\n","        df_existing_full = pd.read_pickle(full_split_path)\n","        df_data_spike_full_split = pd.concat([df_existing_full, df_data_spike_full_split], axis=0)\n","    else:\n","        print(f\"No existing full split data found at {full_split_path}, creating new file.\")\n","    \n","    # Save the updated full split data to file\n","    df_data_spike_full_split.to_pickle(full_split_path)\n","\n","    # Save the 1 split data using the full path\n","    split_path = os.path.join(helper.results_dir, f'output_{splits_name}.pkl')\n","    if os.path.exists(split_path):\n","        df_existing_1_spike = pd.read_pickle(split_path)\n","        df_data_spike_1_split = pd.concat([df_existing_1_spike, df_data_spike_1_split], axis=0)\n","    else:\n","        print(f\"No existing 1 split data found at {split_path}, creating new file\")\n","\n","    # Save the updated 1 split data to file\n","    df_data_spike_1_split.to_pickle(split_path)\n","\n","    time.sleep(5)\n","    \n","    session.report({\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"mean_f1_micro\": np.mean(f1_micro_scores)\n","    })"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def generate_hidden_layers_config(min_layers=2, max_layers=5, min_nodes=5, max_nodes=50, step=5):\n","    possible_layers = []\n","    for num_layers in range(min_layers, max_layers + 1):\n","        layer_configurations = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_layers))\n","        possible_layers.extend(layer_configurations)\n","    return possible_layers\n","\n","def five_fold_cross_validation(splits, splits_name):\n","\n","    global best_params_list_getting\n","\n","    hidden_layers_options = generate_hidden_layers_config()\n","\n","    config = {\n","        \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\"]),\n","        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n","        \"batch_size\": tune.choice([32, 64, 128]),\n","        \"hidden_layers\": tune.choice(hidden_layers_options),\n","        \"epochs\": tune.choice([10, 20, 30, 40, 50]),\n","        \"dropout_rate\": tune.uniform(0.2, 0.5)\n","    }\n","    \n","    scheduler = ASHAScheduler(\n","        metric=\"mean_accuracy\",\n","        mode=\"max\",\n","        max_t=10,\n","        grace_period=1,\n","        reduction_factor=2\n","    )\n","    \n","    search_alg = OptunaSearch(metric=\"mean_accuracy\", mode=\"max\")\n","    \n","    analysis = tune.run(\n","        tune.with_parameters(train_and_evaluate, splits=splits, splits_name=splits_name),\n","        resources_per_trial={\"cpu\": 12, \"gpu\": 1, \"accelerator_type:RTX\": 1},\n","        config=config,\n","        scheduler=scheduler,\n","        search_alg=search_alg,\n","        num_samples=32,\n","        verbose=1,\n","        storage_path=helper.ray_results_dir,\n","        trial_dirname_creator=custom_trial_dirname\n","    )\n","\n","    best_config_data_ray_tune = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n","    print(\"Best hyperparameters found were: \", best_config_data_ray_tune)\n","    best_params_list_getting.append(best_config_data_ray_tune)\n","    \n","    return analysis"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-07-22 20:50:13</td></tr>\n","<tr><td>Running for: </td><td>02:28:42.79        </td></tr>\n","<tr><td>Memory:      </td><td>5.8/15.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=8<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.8228885412216187<br>Logical resource usage: 12.0/12 CPUs, 1.0/1 GPUs (1.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_25abde00</td><td>TERMINATED</td><td>172.27.13.81:81174 </td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.393344</td><td style=\"text-align: right;\">      10</td><td>(10, 50, 30, 5, 15) </td><td style=\"text-align: right;\">    0.00364854 </td><td style=\"text-align: right;\">0.557944</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.3261</td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.794393</td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_98a6ea08</td><td>TERMINATED</td><td>172.27.13.81:83122 </td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.298417</td><td style=\"text-align: right;\">      50</td><td>(20, 30, 15, 15, 15)</td><td style=\"text-align: right;\">    0.00883034 </td><td style=\"text-align: right;\">0.852717</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.9206</td><td style=\"text-align: right;\">      0.796296</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.796226</td></tr>\n","<tr><td>train_and_evaluate_a08ebbcb</td><td>TERMINATED</td><td>172.27.13.81:88204 </td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.367652</td><td style=\"text-align: right;\">      10</td><td>(35, 40, 50, 30, 45)</td><td style=\"text-align: right;\">    0.00100164 </td><td style=\"text-align: right;\">0.513084</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.3011</td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.560748</td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_b9904f24</td><td>TERMINATED</td><td>172.27.13.81:90202 </td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.235047</td><td style=\"text-align: right;\">      40</td><td>(20, 30, 15, 20, 5) </td><td style=\"text-align: right;\">    0.00359456 </td><td style=\"text-align: right;\">0.837833</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.9505</td><td style=\"text-align: right;\">      0.759259</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.757177</td></tr>\n","<tr><td>train_and_evaluate_3e2c8d37</td><td>TERMINATED</td><td>172.27.13.81:94497 </td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.353068</td><td style=\"text-align: right;\">      40</td><td>(20, 45, 30, 25, 10)</td><td style=\"text-align: right;\">    0.000713396</td><td style=\"text-align: right;\">0.720232</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         79.9112</td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.831776</td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_abf20170</td><td>TERMINATED</td><td>172.27.13.81:98908 </td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.256644</td><td style=\"text-align: right;\">      20</td><td>(40, 5, 15, 35, 30) </td><td style=\"text-align: right;\">    0.000186114</td><td style=\"text-align: right;\">0.496262</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.6297</td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.5     </td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_10861ca1</td><td>TERMINATED</td><td>172.27.13.81:101673</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.273406</td><td style=\"text-align: right;\">      40</td><td>(25, 45, 40, 35, 45)</td><td style=\"text-align: right;\">    0.0055603  </td><td style=\"text-align: right;\">0.850848</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.5581</td><td style=\"text-align: right;\">      0.796296</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.795172</td></tr>\n","<tr><td>train_and_evaluate_ca97ac48</td><td>TERMINATED</td><td>172.27.13.81:106142</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.382854</td><td style=\"text-align: right;\">      10</td><td>(10, 50, 25, 5)     </td><td style=\"text-align: right;\">    0.000831896</td><td style=\"text-align: right;\">0.705452</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.4275</td><td style=\"text-align: right;\">      0.583333</td><td style=\"text-align: right;\">      0.813084</td><td style=\"text-align: right;\">      0.581576</td></tr>\n","<tr><td>train_and_evaluate_2c1cc29a</td><td>TERMINATED</td><td>172.27.13.81:108020</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.403316</td><td style=\"text-align: right;\">      30</td><td>(5, 45, 5, 40, 10)  </td><td style=\"text-align: right;\">    0.00220911 </td><td style=\"text-align: right;\">0.744514</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.3658</td><td style=\"text-align: right;\">      0.685185</td><td style=\"text-align: right;\">      0.831776</td><td style=\"text-align: right;\">      0.684753</td></tr>\n","<tr><td>train_and_evaluate_f91a061e</td><td>TERMINATED</td><td>172.27.13.81:111574</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.222823</td><td style=\"text-align: right;\">      50</td><td>(10, 30, 15, 10, 15)</td><td style=\"text-align: right;\">    0.000481668</td><td style=\"text-align: right;\">0.766978</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.4481</td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.831776</td><td style=\"text-align: right;\">      0.65483 </td></tr>\n","<tr><td>train_and_evaluate_a0955f07</td><td>TERMINATED</td><td>172.27.13.81:116748</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.35654 </td><td style=\"text-align: right;\">      10</td><td>(35, 35, 45, 50, 35)</td><td style=\"text-align: right;\">    0.000224943</td><td style=\"text-align: right;\">0.525995</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       6727.47  </td><td style=\"text-align: right;\">      0.46729 </td><td style=\"text-align: right;\">      0.592593</td><td style=\"text-align: right;\">      0.346512</td></tr>\n","<tr><td>train_and_evaluate_a0c4dfa5</td><td>TERMINATED</td><td>172.27.13.81:119115</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.473088</td><td style=\"text-align: right;\">      50</td><td>(25, 35, 20, 50, 15)</td><td style=\"text-align: right;\">    0.00844311 </td><td style=\"text-align: right;\">0.777968</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.7345</td><td style=\"text-align: right;\">      0.663551</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.617857</td></tr>\n","<tr><td>train_and_evaluate_e27544f0</td><td>TERMINATED</td><td>172.27.13.81:124201</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.296853</td><td style=\"text-align: right;\">      50</td><td>(25, 45, 35, 10)    </td><td style=\"text-align: right;\">    0.0071015  </td><td style=\"text-align: right;\">0.856456</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.3636</td><td style=\"text-align: right;\">      0.796296</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.796226</td></tr>\n","<tr><td>train_and_evaluate_7f2eedb7</td><td>TERMINATED</td><td>172.27.13.81:129283</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.293891</td><td style=\"text-align: right;\">      50</td><td>(5, 10, 50, 50, 15) </td><td style=\"text-align: right;\">    0.00748086 </td><td style=\"text-align: right;\">0.819142</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.5489</td><td style=\"text-align: right;\">      0.759259</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.758514</td></tr>\n","<tr><td>train_and_evaluate_f0a6c260</td><td>TERMINATED</td><td>172.27.13.81:134381</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.303074</td><td style=\"text-align: right;\">      50</td><td>(30, 15, 45, 40, 30)</td><td style=\"text-align: right;\">    0.00938057 </td><td style=\"text-align: right;\">0.84152 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.9474</td><td style=\"text-align: right;\">      0.787037</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.787019</td></tr>\n","<tr><td>train_and_evaluate_c3480ef2</td><td>TERMINATED</td><td>172.27.13.81:139499</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.30942 </td><td style=\"text-align: right;\">      50</td><td>(35, 15, 10, 10, 10)</td><td style=\"text-align: right;\">    0.00213848 </td><td style=\"text-align: right;\">0.843406</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         56.592 </td><td style=\"text-align: right;\">      0.777778</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.77709 </td></tr>\n","<tr><td>train_and_evaluate_ef22c976</td><td>TERMINATED</td><td>172.27.13.81:144533</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.303199</td><td style=\"text-align: right;\">      50</td><td>(45, 35, 25, 15, 20)</td><td style=\"text-align: right;\">    0.00244889 </td><td style=\"text-align: right;\">0.867705</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         78.3436</td><td style=\"text-align: right;\">      0.777778</td><td style=\"text-align: right;\">      0.906542</td><td style=\"text-align: right;\">      0.776552</td></tr>\n","<tr><td>train_and_evaluate_03265bcb</td><td>TERMINATED</td><td>172.27.13.81:149699</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.321708</td><td style=\"text-align: right;\">      20</td><td>(15, 40, 45, 30, 5) </td><td style=\"text-align: right;\">    0.00174271 </td><td style=\"text-align: right;\">0.750035</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.0852</td><td style=\"text-align: right;\">      0.607477</td><td style=\"text-align: right;\">      0.869159</td><td style=\"text-align: right;\">      0.584812</td></tr>\n","<tr><td>train_and_evaluate_06e193bc</td><td>TERMINATED</td><td>172.27.13.81:152404</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.448681</td><td style=\"text-align: right;\">      20</td><td>(25, 45, 30, 35, 10)</td><td style=\"text-align: right;\">    0.00196738 </td><td style=\"text-align: right;\">0.542766</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.4768</td><td style=\"text-align: right;\">      0.504673</td><td style=\"text-align: right;\">      0.62037 </td><td style=\"text-align: right;\">      0.335404</td></tr>\n","<tr><td>train_and_evaluate_7afb2001</td><td>TERMINATED</td><td>172.27.13.81:155122</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.43836 </td><td style=\"text-align: right;\">      30</td><td>(25, 10, 50, 35, 15)</td><td style=\"text-align: right;\">    0.00401853 </td><td style=\"text-align: right;\">0.820993</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.8442</td><td style=\"text-align: right;\">      0.768519</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.759423</td></tr>\n","<tr><td>train_and_evaluate_259af2d9</td><td>TERMINATED</td><td>172.27.13.81:158623</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.253901</td><td style=\"text-align: right;\">      30</td><td>(20, 20, 5, 5, 35)  </td><td style=\"text-align: right;\">    0.004319   </td><td style=\"text-align: right;\">0.834078</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.2605</td><td style=\"text-align: right;\">      0.768519</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.768021</td></tr>\n","<tr><td>train_and_evaluate_9da15810</td><td>TERMINATED</td><td>172.27.13.81:162103</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.200966</td><td style=\"text-align: right;\">      50</td><td>(35, 40, 40, 30)    </td><td style=\"text-align: right;\">    0.000112054</td><td style=\"text-align: right;\">0.688629</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.1639</td><td style=\"text-align: right;\">      0.583333</td><td style=\"text-align: right;\">      0.794393</td><td style=\"text-align: right;\">      0.572747</td></tr>\n","<tr><td>train_and_evaluate_fd070e0e</td><td>TERMINATED</td><td>172.27.13.81:167210</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.331379</td><td style=\"text-align: right;\">      50</td><td>(5, 25, 5, 20)      </td><td style=\"text-align: right;\">    0.00595934 </td><td style=\"text-align: right;\">0.787366</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.1706</td><td style=\"text-align: right;\">      0.738318</td><td style=\"text-align: right;\">      0.859813</td><td style=\"text-align: right;\">      0.736453</td></tr>\n","<tr><td>train_and_evaluate_c2804375</td><td>TERMINATED</td><td>172.27.13.81:172290</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.323816</td><td style=\"text-align: right;\">      50</td><td>(30, 10, 25, 25, 5) </td><td style=\"text-align: right;\">    0.0057841  </td><td style=\"text-align: right;\">0.834026</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.6925</td><td style=\"text-align: right;\">      0.796296</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.796016</td></tr>\n","<tr><td>train_and_evaluate_768373e9</td><td>TERMINATED</td><td>172.27.13.81:177404</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.27913 </td><td style=\"text-align: right;\">      50</td><td>(10, 20, 45, 15, 20)</td><td style=\"text-align: right;\">    0.00275113 </td><td style=\"text-align: right;\">0.824784</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.5558</td><td style=\"text-align: right;\">      0.740741</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.740741</td></tr>\n","<tr><td>train_and_evaluate_71567bce</td><td>TERMINATED</td><td>172.27.13.81:182477</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.282681</td><td style=\"text-align: right;\">      50</td><td>(20, 20, 15, 45, 50)</td><td style=\"text-align: right;\">    0.00994568 </td><td style=\"text-align: right;\">0.828522</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.0062</td><td style=\"text-align: right;\">      0.740741</td><td style=\"text-align: right;\">      0.869159</td><td style=\"text-align: right;\">      0.740652</td></tr>\n","<tr><td>train_and_evaluate_07d66dd0</td><td>TERMINATED</td><td>172.27.13.81:187582</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.276171</td><td style=\"text-align: right;\">      50</td><td>(25, 30, 50, 45, 45)</td><td style=\"text-align: right;\">    0.00133301 </td><td style=\"text-align: right;\">0.862046</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.8779</td><td style=\"text-align: right;\">      0.805556</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.805405</td></tr>\n","<tr><td>train_and_evaluate_c5d22dfa</td><td>TERMINATED</td><td>172.27.13.81:192826</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.337109</td><td style=\"text-align: right;\">      50</td><td>(40, 35, 50, 15)    </td><td style=\"text-align: right;\">    0.00142159 </td><td style=\"text-align: right;\">0.824818</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.0904</td><td style=\"text-align: right;\">      0.722222</td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.72069 </td></tr>\n","<tr><td>train_and_evaluate_ac014a7f</td><td>TERMINATED</td><td>172.27.13.81:197938</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.256573</td><td style=\"text-align: right;\">      50</td><td>(15, 15, 50, 40, 15)</td><td style=\"text-align: right;\">    0.00130531 </td><td style=\"text-align: right;\">0.85649 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.4286</td><td style=\"text-align: right;\">      0.777778</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.777473</td></tr>\n","<tr><td>train_and_evaluate_f17f9be9</td><td>TERMINATED</td><td>172.27.13.81:203017</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.258868</td><td style=\"text-align: right;\">      50</td><td>(50, 10, 25, 15, 30)</td><td style=\"text-align: right;\">    0.000463209</td><td style=\"text-align: right;\">0.811803</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.4296</td><td style=\"text-align: right;\">      0.685185</td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.683448</td></tr>\n","<tr><td>train_and_evaluate_a00e0011</td><td>TERMINATED</td><td>172.27.13.81:208145</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.255877</td><td style=\"text-align: right;\">      50</td><td>(30, 20, 25, 45, 20)</td><td style=\"text-align: right;\">    0.00122675 </td><td style=\"text-align: right;\">0.875095</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.8104</td><td style=\"text-align: right;\">      0.824074</td><td style=\"text-align: right;\">      0.925234</td><td style=\"text-align: right;\">      0.823938</td></tr>\n","<tr><td>train_and_evaluate_187cbabb</td><td>TERMINATED</td><td>172.27.13.81:213247</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.23516 </td><td style=\"text-align: right;\">      20</td><td>(50, 30, 50, 10, 40)</td><td style=\"text-align: right;\">    0.00112388 </td><td style=\"text-align: right;\">0.830322</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.8408</td><td style=\"text-align: right;\">      0.777778</td><td style=\"text-align: right;\">      0.869159</td><td style=\"text-align: right;\">      0.77709 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-22 18:22:39,397\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 50, 30, 5, 15)}\n","2024-07-22 18:23:44,248\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 30, 15, 15, 15)}\n","2024-07-22 18:24:55,027\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 40, 50, 30, 45)}\n","2024-07-22 18:26:03,398\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 30, 15, 20, 5)}\n","2024-07-22 18:27:30,156\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 45, 30, 25, 10)}\n","2024-07-22 18:28:43,554\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 5, 15, 35, 30)}\n","2024-07-22 18:30:13,233\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 45, 40, 35, 45)}\n","2024-07-22 18:31:12,683\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 50, 25, 5)}\n","2024-07-22 18:32:24,938\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 45, 5, 40, 10)}\n","2024-07-22 18:33:45,254\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 30, 15, 10, 15)}\n","2024-07-22 20:25:58,574\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 35, 45, 50, 35)}\n","2024-07-22 20:27:07,160\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 35, 20, 50, 15)}\n","2024-07-22 20:28:09,030\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 45, 35, 10)}\n","2024-07-22 20:29:14,304\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 10, 50, 50, 15)}\n","2024-07-22 20:30:22,776\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 15, 45, 40, 30)}\n","2024-07-22 20:31:23,482\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 15, 10, 10, 10)}\n","2024-07-22 20:32:45,727\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 35, 25, 15, 20)}\n","2024-07-22 20:33:53,935\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 40, 45, 30, 5)}\n","2024-07-22 20:35:01,052\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 45, 30, 35, 10)}\n","2024-07-22 20:36:10,860\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 10, 50, 35, 15)}\n","2024-07-22 20:37:19,690\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 20, 5, 5, 35)}\n","2024-07-22 20:38:21,859\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 40, 40, 30)}\n","2024-07-22 20:39:26,177\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 25, 5, 20)}\n","2024-07-22 20:40:42,811\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 10, 25, 25, 5)}\n","2024-07-22 20:41:49,919\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 20, 45, 15, 20)}\n","2024-07-22 20:42:55,139\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 20, 15, 45, 50)}\n","2024-07-22 20:44:24,930\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 30, 50, 45, 45)}\n","2024-07-22 20:45:32,924\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 50, 15)}\n","2024-07-22 20:46:37,636\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 15, 50, 40, 15)}\n","2024-07-22 20:47:53,917\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 10, 25, 15, 30)}\n","2024-07-22 20:49:04,597\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 20, 25, 45, 20)}\n","2024-07-22 20:50:13,335\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 30, 50, 10, 40)}\n","2024-07-22 20:50:13,959\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/c/Users/Mayra Elwes/Documents/MyWork/Multi-Task-Learning/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-07-22_18-21-30' in 0.6205s.\n","2024-07-22 20:50:13,973\tINFO tune.py:1041 -- Total run time: 8923.92 seconds (8922.17 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.001226746141092834, 'batch_size': 64, 'hidden_layers': (30, 20, 25, 45, 20), 'epochs': 50, 'dropout_rate': 0.25587746853049803}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a2_splits, \"A2\")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-07-22 21:32:04</td></tr>\n","<tr><td>Running for: </td><td>00:41:48.89        </td></tr>\n","<tr><td>Memory:      </td><td>5.8/15.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=14<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.7631469964981079<br>Logical resource usage: 12.0/12 CPUs, 1.0/1 GPUs (1.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_f48fa8d1</td><td>TERMINATED</td><td>172.27.13.81:215983</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.412832</td><td style=\"text-align: right;\">      50</td><td>(45, 50, 20, 50, 5) </td><td style=\"text-align: right;\">    0.000550521</td><td style=\"text-align: right;\">0.774493</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.1242</td><td style=\"text-align: right;\">      0.724638</td><td style=\"text-align: right;\">      0.8     </td><td style=\"text-align: right;\">      0.720886</td></tr>\n","<tr><td>train_and_evaluate_bd3dd1a3</td><td>TERMINATED</td><td>172.27.13.81:221074</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.464273</td><td style=\"text-align: right;\">      40</td><td>(5, 35, 5, 25, 5)   </td><td style=\"text-align: right;\">    0.00554311 </td><td style=\"text-align: right;\">0.679213</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.8364</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_7533afaf</td><td>TERMINATED</td><td>172.27.13.81:225381</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.251443</td><td style=\"text-align: right;\">      10</td><td>(20, 25, 50, 15, 45)</td><td style=\"text-align: right;\">    0.000600224</td><td style=\"text-align: right;\">0.748861</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.9647</td><td style=\"text-align: right;\">      0.642857</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.6422  </td></tr>\n","<tr><td>train_and_evaluate_55abae56</td><td>TERMINATED</td><td>172.27.13.81:227403</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.4846  </td><td style=\"text-align: right;\">      10</td><td>(35, 50, 50, 25)    </td><td style=\"text-align: right;\">    0.00388583 </td><td style=\"text-align: right;\">0.673085</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         48.1883</td><td style=\"text-align: right;\">      0.550725</td><td style=\"text-align: right;\">      0.785714</td><td style=\"text-align: right;\">      0.462157</td></tr>\n","<tr><td>train_and_evaluate_579a3bf3</td><td>TERMINATED</td><td>172.27.13.81:229253</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.325123</td><td style=\"text-align: right;\">      40</td><td>(5, 40, 50, 5, 10)  </td><td style=\"text-align: right;\">    0.000479617</td><td style=\"text-align: right;\">0.69648 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.0533</td><td style=\"text-align: right;\">      0.594203</td><td style=\"text-align: right;\">      0.768116</td><td style=\"text-align: right;\">      0.583621</td></tr>\n","<tr><td>train_and_evaluate_dc476184</td><td>TERMINATED</td><td>172.27.13.81:233639</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.352743</td><td style=\"text-align: right;\">      50</td><td>(5, 50, 20, 45)     </td><td style=\"text-align: right;\">    0.000139499</td><td style=\"text-align: right;\">0.497101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.0634</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_f5dd0474</td><td>TERMINATED</td><td>172.27.13.81:238755</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.303992</td><td style=\"text-align: right;\">      20</td><td>(20, 25, 45, 35, 40)</td><td style=\"text-align: right;\">    0.000269082</td><td style=\"text-align: right;\">0.583892</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.8796</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.652174</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_99cd9389</td><td>TERMINATED</td><td>172.27.13.81:241594</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.477646</td><td style=\"text-align: right;\">      20</td><td>(40, 45, 20, 30, 20)</td><td style=\"text-align: right;\">    0.000208238</td><td style=\"text-align: right;\">0.505797</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.1592</td><td style=\"text-align: right;\">      0.5     </td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.333333</td></tr>\n","<tr><td>train_and_evaluate_4c5c8f00</td><td>TERMINATED</td><td>172.27.13.81:244285</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.437218</td><td style=\"text-align: right;\">      40</td><td>(25, 50, 40, 40, 15)</td><td style=\"text-align: right;\">    0.00110775 </td><td style=\"text-align: right;\">0.502899</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.26  </td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_8b10105c</td><td>TERMINATED</td><td>172.27.13.81:248607</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.429775</td><td style=\"text-align: right;\">      30</td><td>(45, 40, 30, 5)     </td><td style=\"text-align: right;\">    0.00168868 </td><td style=\"text-align: right;\">0.806335</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.5708</td><td style=\"text-align: right;\">      0.753623</td><td style=\"text-align: right;\">      0.855072</td><td style=\"text-align: right;\">      0.753623</td></tr>\n","<tr><td>train_and_evaluate_5e4cb816</td><td>TERMINATED</td><td>172.27.13.81:252101</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.27939 </td><td style=\"text-align: right;\">      30</td><td>(20, 20, 35, 30, 40)</td><td style=\"text-align: right;\">    0.000163103</td><td style=\"text-align: right;\">0.497101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.5591</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_38815760</td><td>TERMINATED</td><td>172.27.13.81:255603</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.381284</td><td style=\"text-align: right;\">      30</td><td>(40, 15, 20, 25, 25)</td><td style=\"text-align: right;\">    0.00201636 </td><td style=\"text-align: right;\">0.783437</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.5583</td><td style=\"text-align: right;\">      0.714286</td><td style=\"text-align: right;\">      0.855072</td><td style=\"text-align: right;\">      0.688889</td></tr>\n","<tr><td>train_and_evaluate_4b3baaa1</td><td>TERMINATED</td><td>172.27.13.81:259065</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.390397</td><td style=\"text-align: right;\">      30</td><td>(35, 20, 45, 5, 50) </td><td style=\"text-align: right;\">    0.0017728  </td><td style=\"text-align: right;\">0.803437</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.816 </td><td style=\"text-align: right;\">      0.753623</td><td style=\"text-align: right;\">      0.855072</td><td style=\"text-align: right;\">      0.753416</td></tr>\n","<tr><td>train_and_evaluate_50aa7770</td><td>TERMINATED</td><td>172.27.13.81:262707</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.383235</td><td style=\"text-align: right;\">      30</td><td>(20, 40, 15, 25, 15)</td><td style=\"text-align: right;\">    0.00216254 </td><td style=\"text-align: right;\">0.751801</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         59.8308</td><td style=\"text-align: right;\">      0.628571</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.569129</td></tr>\n","<tr><td>train_and_evaluate_0fd52757</td><td>TERMINATED</td><td>172.27.13.81:266146</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.393497</td><td style=\"text-align: right;\">      30</td><td>(40, 45, 35, 45, 25)</td><td style=\"text-align: right;\">    0.00204934 </td><td style=\"text-align: right;\">0.791884</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.5497</td><td style=\"text-align: right;\">      0.753623</td><td style=\"text-align: right;\">      0.826087</td><td style=\"text-align: right;\">      0.752792</td></tr>\n","<tr><td>train_and_evaluate_a4ef54c1</td><td>TERMINATED</td><td>172.27.13.81:269725</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.42602 </td><td style=\"text-align: right;\">      30</td><td>(45, 40, 20, 50, 30)</td><td style=\"text-align: right;\">    0.00153358 </td><td style=\"text-align: right;\">0.817805</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.6854</td><td style=\"text-align: right;\">      0.782609</td><td style=\"text-align: right;\">      0.857143</td><td style=\"text-align: right;\">      0.782426</td></tr>\n","<tr><td>train_and_evaluate_40e4c41d</td><td>TERMINATED</td><td>172.27.13.81:273380</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.218012</td><td style=\"text-align: right;\">      30</td><td>(35, 45, 25, 35, 10)</td><td style=\"text-align: right;\">    0.00111334 </td><td style=\"text-align: right;\">0.788903</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.8901</td><td style=\"text-align: right;\">      0.73913 </td><td style=\"text-align: right;\">      0.828571</td><td style=\"text-align: right;\">      0.732328</td></tr>\n","<tr><td>train_and_evaluate_4bf44c8f</td><td>TERMINATED</td><td>172.27.13.81:276895</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.448814</td><td style=\"text-align: right;\">      30</td><td>(25, 10, 5, 10, 10) </td><td style=\"text-align: right;\">    0.0074341  </td><td style=\"text-align: right;\">0.597764</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.7403</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.771429</td><td style=\"text-align: right;\">      0.336538</td></tr>\n","<tr><td>train_and_evaluate_324ccfa8</td><td>TERMINATED</td><td>172.27.13.81:280366</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.436248</td><td style=\"text-align: right;\">      30</td><td>(40, 10, 15, 35, 25)</td><td style=\"text-align: right;\">    0.00954803 </td><td style=\"text-align: right;\">0.705466</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.6083</td><td style=\"text-align: right;\">      0.57971 </td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.510159</td></tr>\n","<tr><td>train_and_evaluate_9bd3114f</td><td>TERMINATED</td><td>172.27.13.81:283928</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.422123</td><td style=\"text-align: right;\">      30</td><td>(30, 45, 5, 5, 35)  </td><td style=\"text-align: right;\">    0.00351763 </td><td style=\"text-align: right;\">0.687619</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.4044</td><td style=\"text-align: right;\">      0.565217</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.486607</td></tr>\n","<tr><td>train_and_evaluate_5589b18b</td><td>TERMINATED</td><td>172.27.13.81:287534</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.495509</td><td style=\"text-align: right;\">      10</td><td>(10, 30, 20, 25, 35)</td><td style=\"text-align: right;\">    0.00354499 </td><td style=\"text-align: right;\">0.578095</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.7701</td><td style=\"text-align: right;\">      0.478261</td><td style=\"text-align: right;\">      0.681159</td><td style=\"text-align: right;\">      0.464655</td></tr>\n","<tr><td>train_and_evaluate_f2b0adb4</td><td>TERMINATED</td><td>172.27.13.81:289493</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.496937</td><td style=\"text-align: right;\">      10</td><td>(30, 5, 15, 35, 10) </td><td style=\"text-align: right;\">    0.000714571</td><td style=\"text-align: right;\">0.534907</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         88.2346</td><td style=\"text-align: right;\">      0.457143</td><td style=\"text-align: right;\">      0.623188</td><td style=\"text-align: right;\">      0.397645</td></tr>\n","<tr><td>train_and_evaluate_9f864245</td><td>TERMINATED</td><td>172.27.13.81:291490</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.357777</td><td style=\"text-align: right;\">      30</td><td>(20, 40, 25, 10, 25)</td><td style=\"text-align: right;\">    0.00147895 </td><td style=\"text-align: right;\">0.791925</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.7727</td><td style=\"text-align: right;\">      0.768116</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.767677</td></tr>\n","<tr><td>train_and_evaluate_3bc619de</td><td>TERMINATED</td><td>172.27.13.81:294960</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.36593 </td><td style=\"text-align: right;\">      30</td><td>(15, 10, 30, 35, 10)</td><td style=\"text-align: right;\">    0.00169789 </td><td style=\"text-align: right;\">0.809275</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.4055</td><td style=\"text-align: right;\">      0.782609</td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.781876</td></tr>\n","<tr><td>train_and_evaluate_db860722</td><td>TERMINATED</td><td>172.27.13.81:298462</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.39572 </td><td style=\"text-align: right;\">      30</td><td>(50, 35, 40, 25, 35)</td><td style=\"text-align: right;\">    0.00165227 </td><td style=\"text-align: right;\">0.800455</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.0372</td><td style=\"text-align: right;\">      0.753623</td><td style=\"text-align: right;\">      0.842857</td><td style=\"text-align: right;\">      0.753623</td></tr>\n","<tr><td>train_and_evaluate_5bbeab67</td><td>TERMINATED</td><td>172.27.13.81:302016</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.329013</td><td style=\"text-align: right;\">      30</td><td>(30, 5, 25, 30, 45) </td><td style=\"text-align: right;\">    0.000860479</td><td style=\"text-align: right;\">0.800538</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.8696</td><td style=\"text-align: right;\">      0.73913 </td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.739076</td></tr>\n","<tr><td>train_and_evaluate_b7b82b00</td><td>TERMINATED</td><td>172.27.13.81:305557</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.330085</td><td style=\"text-align: right;\">      50</td><td>(45, 25, 35, 50, 40)</td><td style=\"text-align: right;\">    0.00108179 </td><td style=\"text-align: right;\">0.823685</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.061 </td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.811436</td></tr>\n","<tr><td>train_and_evaluate_0b97dd45</td><td>TERMINATED</td><td>172.27.13.81:310850</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.415294</td><td style=\"text-align: right;\">      20</td><td>(20, 15, 5, 10, 30) </td><td style=\"text-align: right;\">    0.000336225</td><td style=\"text-align: right;\">0.571884</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         88.0047</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.7     </td><td style=\"text-align: right;\">      0.418155</td></tr>\n","<tr><td>train_and_evaluate_3fe3e269</td><td>TERMINATED</td><td>172.27.13.81:313649</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.283711</td><td style=\"text-align: right;\">      50</td><td>(30, 20, 15, 15, 20)</td><td style=\"text-align: right;\">    0.00311225 </td><td style=\"text-align: right;\">0.826418</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.5555</td><td style=\"text-align: right;\">      0.797101</td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.797059</td></tr>\n","<tr><td>train_and_evaluate_e3c6f1ca</td><td>TERMINATED</td><td>172.27.13.81:318719</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.294417</td><td style=\"text-align: right;\">      50</td><td>(10, 15, 35, 35)    </td><td style=\"text-align: right;\">    0.00304816 </td><td style=\"text-align: right;\">0.794824</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.443 </td><td style=\"text-align: right;\">      0.768116</td><td style=\"text-align: right;\">      0.826087</td><td style=\"text-align: right;\">      0.767677</td></tr>\n","<tr><td>train_and_evaluate_7e7453de</td><td>TERMINATED</td><td>172.27.13.81:323805</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.270747</td><td style=\"text-align: right;\">      50</td><td>(30, 15, 25, 50, 40)</td><td style=\"text-align: right;\">    0.00298241 </td><td style=\"text-align: right;\">0.820787</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.606 </td><td style=\"text-align: right;\">      0.797101</td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.796717</td></tr>\n","<tr><td>train_and_evaluate_c5d17c42</td><td>TERMINATED</td><td>172.27.13.81:329087</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.25974 </td><td style=\"text-align: right;\">      50</td><td>(30, 45, 40, 20, 40)</td><td style=\"text-align: right;\">    0.00267399 </td><td style=\"text-align: right;\">0.835279</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.0001</td><td style=\"text-align: right;\">      0.826087</td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.825169</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-22 20:51:21,029\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 50, 20, 50, 5)}\n","2024-07-22 20:52:34,039\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 35, 5, 25, 5)}\n","2024-07-22 20:54:11,854\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 25, 50, 15, 45)}\n","2024-07-22 20:55:03,492\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 50, 50, 25)}\n","2024-07-22 20:56:28,252\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 40, 50, 5, 10)}\n","2024-07-22 20:57:40,032\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 50, 20, 45)}\n","2024-07-22 20:59:10,409\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 25, 45, 35, 40)}\n","2024-07-22 21:00:16,604\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 45, 20, 30, 20)}\n","2024-07-22 21:01:22,740\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 50, 40, 40, 15)}\n","2024-07-22 21:02:27,068\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 40, 30, 5)}\n","2024-07-22 21:03:35,002\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 20, 35, 30, 40)}\n","2024-07-22 21:04:43,002\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 15, 20, 25, 25)}\n","2024-07-22 21:06:19,463\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 20, 45, 5, 50)}\n","2024-07-22 21:07:23,309\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 40, 15, 25, 15)}\n","2024-07-22 21:08:40,704\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 45, 35, 45, 25)}\n","2024-07-22 21:10:23,261\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 40, 20, 50, 30)}\n","2024-07-22 21:11:34,526\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 45, 25, 35, 10)}\n","2024-07-22 21:12:38,876\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 10, 5, 10, 10)}\n","2024-07-22 21:14:06,264\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 10, 15, 35, 25)}\n","2024-07-22 21:15:34,123\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 5, 5, 35)}\n","2024-07-22 21:17:00,444\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 30, 20, 25, 35)}\n","2024-07-22 21:18:31,983\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 5, 15, 35, 10)}\n","2024-07-22 21:19:38,562\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 40, 25, 10, 25)}\n","2024-07-22 21:20:49,281\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 10, 30, 35, 10)}\n","2024-07-22 21:22:04,821\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 35, 40, 25, 35)}\n","2024-07-22 21:23:22,468\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 5, 25, 30, 45)}\n","2024-07-22 21:25:08,335\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 25, 35, 50, 40)}\n","2024-07-22 21:26:42,425\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 15, 5, 10, 30)}\n","2024-07-22 21:27:53,265\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 20, 15, 15, 20)}\n","2024-07-22 21:29:00,810\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 15, 35, 35)}\n","2024-07-22 21:30:48,531\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 15, 25, 50, 40)}\n","2024-07-22 21:32:03,992\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 40, 20, 40)}\n","2024-07-22 21:32:04,688\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/c/Users/Mayra Elwes/Documents/MyWork/Multi-Task-Learning/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-07-22_20-50-14' in 0.6933s.\n","2024-07-22 21:32:04,706\tINFO tune.py:1041 -- Total run time: 2510.18 seconds (2508.20 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.002673992605245546, 'batch_size': 64, 'hidden_layers': (30, 45, 40, 20, 40), 'epochs': 50, 'dropout_rate': 0.25974001156704213}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a3_splits, \"A3\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-07-22 22:14:10</td></tr>\n","<tr><td>Running for: </td><td>00:42:04.87        </td></tr>\n","<tr><td>Memory:      </td><td>5.8/15.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=19<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5242220997810364<br>Logical resource usage: 12.0/12 CPUs, 1.0/1 GPUs (1.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_ee520a9d</td><td>TERMINATED</td><td>172.27.13.81:334237</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.24957 </td><td style=\"text-align: right;\">      30</td><td>(50, 50, 35, 15, 45)</td><td style=\"text-align: right;\">    0.00914258 </td><td style=\"text-align: right;\">0.6282  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.3984</td><td style=\"text-align: right;\">      0.586294</td><td style=\"text-align: right;\">      0.653944</td><td style=\"text-align: right;\">      0.567145</td></tr>\n","<tr><td>train_and_evaluate_b685f2d9</td><td>TERMINATED</td><td>172.27.13.81:337807</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.289763</td><td style=\"text-align: right;\">      30</td><td>(10, 45, 15, 30, 45)</td><td style=\"text-align: right;\">    0.000360956</td><td style=\"text-align: right;\">0.540727</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.2946</td><td style=\"text-align: right;\">      0.472081</td><td style=\"text-align: right;\">      0.569975</td><td style=\"text-align: right;\">      0.446686</td></tr>\n","<tr><td>train_and_evaluate_a9c37310</td><td>TERMINATED</td><td>172.27.13.81:341310</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.495531</td><td style=\"text-align: right;\">      10</td><td>(45, 35, 40, 30, 5) </td><td style=\"text-align: right;\">    0.000197084</td><td style=\"text-align: right;\">0.42068 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.9437</td><td style=\"text-align: right;\">      0.362944</td><td style=\"text-align: right;\">      0.491094</td><td style=\"text-align: right;\">      0.29737 </td></tr>\n","<tr><td>train_and_evaluate_d24a9143</td><td>TERMINATED</td><td>172.27.13.81:343256</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.263293</td><td style=\"text-align: right;\">      50</td><td>(30, 15, 15, 45, 40)</td><td style=\"text-align: right;\">    0.0010308  </td><td style=\"text-align: right;\">0.613446</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        106.204 </td><td style=\"text-align: right;\">      0.569975</td><td style=\"text-align: right;\">      0.679389</td><td style=\"text-align: right;\">      0.476406</td></tr>\n","<tr><td>train_and_evaluate_b7ee5927</td><td>TERMINATED</td><td>172.27.13.81:348600</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.453707</td><td style=\"text-align: right;\">      40</td><td>(45, 20, 30, 5, 25) </td><td style=\"text-align: right;\">    0.000616836</td><td style=\"text-align: right;\">0.349426</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.3021</td><td style=\"text-align: right;\">      0.333333</td><td style=\"text-align: right;\">      0.378173</td><td style=\"text-align: right;\">      0.166667</td></tr>\n","<tr><td>train_and_evaluate_31444e30</td><td>TERMINATED</td><td>172.27.13.81:352928</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.464311</td><td style=\"text-align: right;\">      50</td><td>(10, 15, 35, 5, 15) </td><td style=\"text-align: right;\">    0.000150414</td><td style=\"text-align: right;\">0.38051 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.7267</td><td style=\"text-align: right;\">      0.296954</td><td style=\"text-align: right;\">      0.43257 </td><td style=\"text-align: right;\">      0.2875  </td></tr>\n","<tr><td>train_and_evaluate_90967c34</td><td>TERMINATED</td><td>172.27.13.81:358069</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.496042</td><td style=\"text-align: right;\">      30</td><td>(35, 20, 20, 5, 35) </td><td style=\"text-align: right;\">    0.0010838  </td><td style=\"text-align: right;\">0.466457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.6155</td><td style=\"text-align: right;\">      0.411168</td><td style=\"text-align: right;\">      0.501272</td><td style=\"text-align: right;\">      0.358443</td></tr>\n","<tr><td>train_and_evaluate_55fcff3d</td><td>TERMINATED</td><td>172.27.13.81:361578</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.471338</td><td style=\"text-align: right;\">      20</td><td>(35, 30, 25, 10, 50)</td><td style=\"text-align: right;\">    0.000252773</td><td style=\"text-align: right;\">0.341285</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.6964</td><td style=\"text-align: right;\">      0.284987</td><td style=\"text-align: right;\">      0.375635</td><td style=\"text-align: right;\">      0.165711</td></tr>\n","<tr><td>train_and_evaluate_a02c2db3</td><td>TERMINATED</td><td>172.27.13.81:364271</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.460786</td><td style=\"text-align: right;\">      50</td><td>(40, 25, 10, 25, 45)</td><td style=\"text-align: right;\">    0.00422136 </td><td style=\"text-align: right;\">0.579365</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.9891</td><td style=\"text-align: right;\">      0.548223</td><td style=\"text-align: right;\">      0.636132</td><td style=\"text-align: right;\">      0.476205</td></tr>\n","<tr><td>train_and_evaluate_5bfc69da</td><td>TERMINATED</td><td>172.27.13.81:369473</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.383785</td><td style=\"text-align: right;\">      50</td><td>(40, 30, 15, 35)    </td><td style=\"text-align: right;\">    0.00029993 </td><td style=\"text-align: right;\">0.535658</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.6618</td><td style=\"text-align: right;\">      0.431472</td><td style=\"text-align: right;\">      0.59542 </td><td style=\"text-align: right;\">      0.407123</td></tr>\n","<tr><td>train_and_evaluate_2f712da3</td><td>TERMINATED</td><td>172.27.13.81:374591</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.463754</td><td style=\"text-align: right;\">      50</td><td>(15, 20, 10, 50, 5) </td><td style=\"text-align: right;\">    0.00893896 </td><td style=\"text-align: right;\">0.486315</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        111.112 </td><td style=\"text-align: right;\">      0.390863</td><td style=\"text-align: right;\">      0.529262</td><td style=\"text-align: right;\">      0.317977</td></tr>\n","<tr><td>train_and_evaluate_93273fa8</td><td>TERMINATED</td><td>172.27.13.81:379918</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.204265</td><td style=\"text-align: right;\">      30</td><td>(15, 50, 30, 20, 45)</td><td style=\"text-align: right;\">    0.00823399 </td><td style=\"text-align: right;\">0.598716</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.4557</td><td style=\"text-align: right;\">      0.522843</td><td style=\"text-align: right;\">      0.676845</td><td style=\"text-align: right;\">      0.487845</td></tr>\n","<tr><td>train_and_evaluate_4d8a9d97</td><td>TERMINATED</td><td>172.27.13.81:383562</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.206452</td><td style=\"text-align: right;\">      30</td><td>(5, 35, 40, 35, 45) </td><td style=\"text-align: right;\">    0.00209703 </td><td style=\"text-align: right;\">0.498516</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.23  </td><td style=\"text-align: right;\">      0.416244</td><td style=\"text-align: right;\">      0.531807</td><td style=\"text-align: right;\">      0.381368</td></tr>\n","<tr><td>train_and_evaluate_692de81a</td><td>TERMINATED</td><td>172.27.13.81:387397</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.246074</td><td style=\"text-align: right;\">      10</td><td>(25, 15, 30, 45, 15)</td><td style=\"text-align: right;\">    0.00185997 </td><td style=\"text-align: right;\">0.484276</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.6648</td><td style=\"text-align: right;\">      0.398477</td><td style=\"text-align: right;\">      0.526718</td><td style=\"text-align: right;\">      0.33871 </td></tr>\n","<tr><td>train_and_evaluate_9ae3c343</td><td>TERMINATED</td><td>172.27.13.81:389320</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.266767</td><td style=\"text-align: right;\">      10</td><td>(15, 15, 40, 30, 45)</td><td style=\"text-align: right;\">    0.00200028 </td><td style=\"text-align: right;\">0.487295</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.3996</td><td style=\"text-align: right;\">      0.452926</td><td style=\"text-align: right;\">      0.552163</td><td style=\"text-align: right;\">      0.367179</td></tr>\n","<tr><td>train_and_evaluate_5fc03a7e</td><td>TERMINATED</td><td>172.27.13.81:391309</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.322567</td><td style=\"text-align: right;\">      40</td><td>(35, 50, 50, 10)    </td><td style=\"text-align: right;\">    0.00335326 </td><td style=\"text-align: right;\">0.637359</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.165 </td><td style=\"text-align: right;\">      0.587786</td><td style=\"text-align: right;\">      0.699746</td><td style=\"text-align: right;\">      0.582183</td></tr>\n","<tr><td>train_and_evaluate_b2bf00a0</td><td>TERMINATED</td><td>172.27.13.81:395631</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.326068</td><td style=\"text-align: right;\">      20</td><td>(10, 20, 35, 45)    </td><td style=\"text-align: right;\">    0.0048383  </td><td style=\"text-align: right;\">0.530571</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.709 </td><td style=\"text-align: right;\">      0.428934</td><td style=\"text-align: right;\">      0.592875</td><td style=\"text-align: right;\">      0.350687</td></tr>\n","<tr><td>train_and_evaluate_4bd103c6</td><td>TERMINATED</td><td>172.27.13.81:398294</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.332281</td><td style=\"text-align: right;\">      40</td><td>(20, 10, 50, 30)    </td><td style=\"text-align: right;\">    0.00457969 </td><td style=\"text-align: right;\">0.632778</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.2477</td><td style=\"text-align: right;\">      0.591371</td><td style=\"text-align: right;\">      0.664122</td><td style=\"text-align: right;\">      0.576804</td></tr>\n","<tr><td>train_and_evaluate_9501ff19</td><td>TERMINATED</td><td>172.27.13.81:402601</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.343663</td><td style=\"text-align: right;\">      40</td><td>(15, 15, 35, 35, 35)</td><td style=\"text-align: right;\">    0.00420638 </td><td style=\"text-align: right;\">0.546352</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.9054</td><td style=\"text-align: right;\">      0.418782</td><td style=\"text-align: right;\">      0.600509</td><td style=\"text-align: right;\">      0.368878</td></tr>\n","<tr><td>train_and_evaluate_27a63760</td><td>TERMINATED</td><td>172.27.13.81:406914</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.377981</td><td style=\"text-align: right;\">      40</td><td>(35, 40, 25, 15, 30)</td><td style=\"text-align: right;\">    0.00395192 </td><td style=\"text-align: right;\">0.539226</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         79.6826</td><td style=\"text-align: right;\">      0.42132 </td><td style=\"text-align: right;\">      0.603053</td><td style=\"text-align: right;\">      0.366318</td></tr>\n","<tr><td>train_and_evaluate_01ce371a</td><td>TERMINATED</td><td>172.27.13.81:411281</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.385075</td><td style=\"text-align: right;\">      40</td><td>(15, 5, 40, 20, 40) </td><td style=\"text-align: right;\">    0.0030713  </td><td style=\"text-align: right;\">0.529563</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.1604</td><td style=\"text-align: right;\">      0.408629</td><td style=\"text-align: right;\">      0.64631 </td><td style=\"text-align: right;\">      0.345189</td></tr>\n","<tr><td>train_and_evaluate_30139a8f</td><td>TERMINATED</td><td>172.27.13.81:415637</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.406311</td><td style=\"text-align: right;\">      40</td><td>(5, 25, 35, 15, 50) </td><td style=\"text-align: right;\">    0.00291754 </td><td style=\"text-align: right;\">0.456796</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.4992</td><td style=\"text-align: right;\">      0.35369 </td><td style=\"text-align: right;\">      0.559796</td><td style=\"text-align: right;\">      0.174185</td></tr>\n","<tr><td>train_and_evaluate_fbe1f8fa</td><td>TERMINATED</td><td>172.27.13.81:420037</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.30926 </td><td style=\"text-align: right;\">      40</td><td>(25, 45, 10, 10, 20)</td><td style=\"text-align: right;\">    0.00669413 </td><td style=\"text-align: right;\">0.535091</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.4804</td><td style=\"text-align: right;\">      0.48855 </td><td style=\"text-align: right;\">      0.582697</td><td style=\"text-align: right;\">      0.387869</td></tr>\n","<tr><td>train_and_evaluate_693cdf73</td><td>TERMINATED</td><td>172.27.13.81:424420</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.3062  </td><td style=\"text-align: right;\">      40</td><td>(10, 30, 35, 30, 20)</td><td style=\"text-align: right;\">    0.00649501 </td><td style=\"text-align: right;\">0.52702 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         79.2251</td><td style=\"text-align: right;\">      0.406091</td><td style=\"text-align: right;\">      0.590331</td><td style=\"text-align: right;\">      0.364399</td></tr>\n","<tr><td>train_and_evaluate_9bae99fd</td><td>TERMINATED</td><td>172.27.13.81:428779</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.234644</td><td style=\"text-align: right;\">      40</td><td>(35, 45, 20, 5, 10) </td><td style=\"text-align: right;\">    0.00622186 </td><td style=\"text-align: right;\">0.535587</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.503 </td><td style=\"text-align: right;\">      0.506361</td><td style=\"text-align: right;\">      0.571066</td><td style=\"text-align: right;\">      0.411618</td></tr>\n","<tr><td>train_and_evaluate_86f76a6a</td><td>TERMINATED</td><td>172.27.13.81:433162</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.236153</td><td style=\"text-align: right;\">      30</td><td>(40, 35, 35, 30, 45)</td><td style=\"text-align: right;\">    0.00991765 </td><td style=\"text-align: right;\">0.619032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.0165</td><td style=\"text-align: right;\">      0.521628</td><td style=\"text-align: right;\">      0.694656</td><td style=\"text-align: right;\">      0.439681</td></tr>\n","<tr><td>train_and_evaluate_6a9fc8fa</td><td>TERMINATED</td><td>172.27.13.81:436809</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.355184</td><td style=\"text-align: right;\">      30</td><td>(30, 35, 15, 45, 20)</td><td style=\"text-align: right;\">    0.00266317 </td><td style=\"text-align: right;\">0.521424</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.0422</td><td style=\"text-align: right;\">      0.401015</td><td style=\"text-align: right;\">      0.631043</td><td style=\"text-align: right;\">      0.35383 </td></tr>\n","<tr><td>train_and_evaluate_fc84f442</td><td>TERMINATED</td><td>172.27.13.81:440335</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.349189</td><td style=\"text-align: right;\">      20</td><td>(45, 30, 35, 20, 35)</td><td style=\"text-align: right;\">    0.00145127 </td><td style=\"text-align: right;\">0.518361</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.3748</td><td style=\"text-align: right;\">      0.42132 </td><td style=\"text-align: right;\">      0.636132</td><td style=\"text-align: right;\">      0.368259</td></tr>\n","<tr><td>train_and_evaluate_67fb77d1</td><td>TERMINATED</td><td>172.27.13.81:443057</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.283533</td><td style=\"text-align: right;\">      20</td><td>(10, 50, 5, 30, 10) </td><td style=\"text-align: right;\">    0.00156926 </td><td style=\"text-align: right;\">0.374856</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.6336</td><td style=\"text-align: right;\">      0.333333</td><td style=\"text-align: right;\">      0.430025</td><td style=\"text-align: right;\">      0.166667</td></tr>\n","<tr><td>train_and_evaluate_4e2999a8</td><td>TERMINATED</td><td>172.27.13.81:445741</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.289417</td><td style=\"text-align: right;\">      40</td><td>(30, 5, 45, 35, 50) </td><td style=\"text-align: right;\">    0.000585989</td><td style=\"text-align: right;\">0.481704</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.0065</td><td style=\"text-align: right;\">      0.451777</td><td style=\"text-align: right;\">      0.524173</td><td style=\"text-align: right;\">      0.382188</td></tr>\n","<tr><td>train_and_evaluate_c9345f38</td><td>TERMINATED</td><td>172.27.13.81:450190</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.294563</td><td style=\"text-align: right;\">      30</td><td>(5, 50, 30, 15, 35) </td><td style=\"text-align: right;\">    0.000443109</td><td style=\"text-align: right;\">0.324553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.2222</td><td style=\"text-align: right;\">      0.253807</td><td style=\"text-align: right;\">      0.35369 </td><td style=\"text-align: right;\">      0.134953</td></tr>\n","<tr><td>train_and_evaluate_153be955</td><td>TERMINATED</td><td>172.27.13.81:453740</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.41059 </td><td style=\"text-align: right;\">      30</td><td>(25, 30, 30, 5, 5)  </td><td style=\"text-align: right;\">    0.0053796  </td><td style=\"text-align: right;\">0.446597</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.4149</td><td style=\"text-align: right;\">      0.333333</td><td style=\"text-align: right;\">      0.493639</td><td style=\"text-align: right;\">      0.166667</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-22 21:33:23,033\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 50, 35, 15, 45)}\n","2024-07-22 21:34:30,124\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 45, 15, 30, 45)}\n","2024-07-22 21:35:40,947\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 35, 40, 30, 5)}\n","2024-07-22 21:37:30,416\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 15, 15, 45, 40)}\n","2024-07-22 21:38:45,936\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 20, 30, 5, 25)}\n","2024-07-22 21:40:03,776\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 15, 35, 5, 15)}\n","2024-07-22 21:41:09,674\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 20, 20, 5, 35)}\n","2024-07-22 21:42:15,938\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 30, 25, 10, 50)}\n","2024-07-22 21:43:45,132\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 25, 10, 25, 45)}\n","2024-07-22 21:44:57,809\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 30, 15, 35)}\n","2024-07-22 21:46:54,737\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 20, 10, 50, 5)}\n","2024-07-22 21:48:25,094\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 50, 30, 20, 45)}\n","2024-07-22 21:50:11,487\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 35, 40, 35, 45)}\n","2024-07-22 21:51:15,942\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 15, 30, 45, 15)}\n","2024-07-22 21:52:24,679\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 15, 40, 30, 45)}\n","2024-07-22 21:53:37,492\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 50, 50, 10)}\n","2024-07-22 21:54:37,008\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 20, 35, 45)}\n","2024-07-22 21:55:49,740\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 10, 50, 30)}\n","2024-07-22 21:57:02,153\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 15, 35, 35, 35)}\n","2024-07-22 21:58:25,199\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 40, 25, 15, 30)}\n","2024-07-22 21:59:42,758\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 5, 40, 20, 40)}\n","2024-07-22 22:01:11,167\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 25, 35, 15, 50)}\n","2024-07-22 22:02:40,613\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 45, 10, 10, 20)}\n","2024-07-22 22:04:04,520\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 30, 35, 30, 20)}\n","2024-07-22 22:05:29,713\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 45, 20, 5, 10)}\n","2024-07-22 22:07:00,009\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 35, 30, 45)}\n","2024-07-22 22:08:14,764\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 35, 15, 45, 20)}\n","2024-07-22 22:09:23,030\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 30, 35, 20, 35)}\n","2024-07-22 22:10:27,302\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 50, 5, 30, 10)}\n","2024-07-22 22:11:55,665\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 5, 45, 35, 50)}\n","2024-07-22 22:13:04,938\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 50, 30, 15, 35)}\n","2024-07-22 22:14:10,125\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 30, 30, 5, 5)}\n","2024-07-22 22:14:10,576\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/c/Users/Mayra Elwes/Documents/MyWork/Multi-Task-Learning/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-07-22_21-32-05' in 0.4490s.\n","2024-07-22 22:14:10,589\tINFO tune.py:1041 -- Total run time: 2525.41 seconds (2524.42 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'sigmoid', 'learning_rate': 0.003353256725980568, 'batch_size': 64, 'hidden_layers': (35, 50, 50, 10), 'epochs': 40, 'dropout_rate': 0.3225671724019045}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a4_splits, \"A4\")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-07-22 22:53:55</td></tr>\n","<tr><td>Running for: </td><td>00:39:43.95        </td></tr>\n","<tr><td>Memory:      </td><td>5.8/15.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=11<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.23006386309862137<br>Logical resource usage: 12.0/12 CPUs, 1.0/1 GPUs (1.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_1ab59eca</td><td>TERMINATED</td><td>172.27.13.81:457219</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.29265 </td><td style=\"text-align: right;\">      50</td><td>(20, 30, 50, 35, 15)</td><td style=\"text-align: right;\">    0.000410873</td><td style=\"text-align: right;\">0.186342</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.6544</td><td style=\"text-align: right;\">      0.165803</td><td style=\"text-align: right;\">      0.209845</td><td style=\"text-align: right;\">     0.047619 </td></tr>\n","<tr><td>train_and_evaluate_6f39c04f</td><td>TERMINATED</td><td>172.27.13.81:462387</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.416186</td><td style=\"text-align: right;\">      10</td><td>(15, 30, 35, 15, 10)</td><td style=\"text-align: right;\">    0.000106417</td><td style=\"text-align: right;\">0.164085</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         57.4598</td><td style=\"text-align: right;\">      0.15285 </td><td style=\"text-align: right;\">      0.186529</td><td style=\"text-align: right;\">     0.128781 </td></tr>\n","<tr><td>train_and_evaluate_90be14ca</td><td>TERMINATED</td><td>172.27.13.81:464263</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.32356 </td><td style=\"text-align: right;\">      20</td><td>(30, 15, 40, 10, 30)</td><td style=\"text-align: right;\">    0.00873625 </td><td style=\"text-align: right;\">0.191516</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.6592</td><td style=\"text-align: right;\">      0.170543</td><td style=\"text-align: right;\">      0.212435</td><td style=\"text-align: right;\">     0.0485651</td></tr>\n","<tr><td>train_and_evaluate_d0519f93</td><td>TERMINATED</td><td>172.27.13.81:466999</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.24527 </td><td style=\"text-align: right;\">      20</td><td>(30, 30, 15, 35, 10)</td><td style=\"text-align: right;\">    0.00696362 </td><td style=\"text-align: right;\">0.231858</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.1698</td><td style=\"text-align: right;\">      0.196891</td><td style=\"text-align: right;\">      0.26615 </td><td style=\"text-align: right;\">     0.122734 </td></tr>\n","<tr><td>train_and_evaluate_1bb342ec</td><td>TERMINATED</td><td>172.27.13.81:469686</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.409069</td><td style=\"text-align: right;\">      30</td><td>(40, 25, 30, 50, 35)</td><td style=\"text-align: right;\">    0.000316896</td><td style=\"text-align: right;\">0.234985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.3215</td><td style=\"text-align: right;\">      0.207254</td><td style=\"text-align: right;\">      0.246114</td><td style=\"text-align: right;\">     0.146746 </td></tr>\n","<tr><td>train_and_evaluate_0b9ef4ad</td><td>TERMINATED</td><td>172.27.13.81:473227</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.491014</td><td style=\"text-align: right;\">      50</td><td>(10, 5, 40, 20, 30) </td><td style=\"text-align: right;\">    0.00550822 </td><td style=\"text-align: right;\">0.232401</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.546 </td><td style=\"text-align: right;\">      0.204663</td><td style=\"text-align: right;\">      0.272021</td><td style=\"text-align: right;\">     0.116797 </td></tr>\n","<tr><td>train_and_evaluate_b178ddce</td><td>TERMINATED</td><td>172.27.13.81:478506</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.260832</td><td style=\"text-align: right;\">      20</td><td>(20, 35, 45, 25, 35)</td><td style=\"text-align: right;\">    0.000346371</td><td style=\"text-align: right;\">0.177027</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.7584</td><td style=\"text-align: right;\">      0.165803</td><td style=\"text-align: right;\">      0.194301</td><td style=\"text-align: right;\">     0.0474074</td></tr>\n","<tr><td>train_and_evaluate_5a39d1ca</td><td>TERMINATED</td><td>172.27.13.81:481280</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.462692</td><td style=\"text-align: right;\">      40</td><td>(45, 10, 10, 15, 10)</td><td style=\"text-align: right;\">    0.000344892</td><td style=\"text-align: right;\">0.215839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.1165</td><td style=\"text-align: right;\">      0.209845</td><td style=\"text-align: right;\">      0.225389</td><td style=\"text-align: right;\">     0.163437 </td></tr>\n","<tr><td>train_and_evaluate_ece64c58</td><td>TERMINATED</td><td>172.27.13.81:485683</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.28856 </td><td style=\"text-align: right;\">      40</td><td>(20, 20, 50, 5, 25) </td><td style=\"text-align: right;\">    0.00391505 </td><td style=\"text-align: right;\">0.237569</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.2215</td><td style=\"text-align: right;\">      0.225389</td><td style=\"text-align: right;\">      0.25323 </td><td style=\"text-align: right;\">     0.117946 </td></tr>\n","<tr><td>train_and_evaluate_f5d15ace</td><td>TERMINATED</td><td>172.27.13.81:490003</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.28499 </td><td style=\"text-align: right;\">      40</td><td>(10, 25, 5, 40, 10) </td><td style=\"text-align: right;\">    0.00565898 </td><td style=\"text-align: right;\">0.247927</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.431 </td><td style=\"text-align: right;\">      0.23057 </td><td style=\"text-align: right;\">      0.272021</td><td style=\"text-align: right;\">     0.171615 </td></tr>\n","<tr><td>train_and_evaluate_2318ad69</td><td>TERMINATED</td><td>172.27.13.81:494481</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.30676 </td><td style=\"text-align: right;\">      30</td><td>(25, 30, 30, 40, 5) </td><td style=\"text-align: right;\">    0.000607438</td><td style=\"text-align: right;\">0.1677  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.1136</td><td style=\"text-align: right;\">      0.165803</td><td style=\"text-align: right;\">      0.170543</td><td style=\"text-align: right;\">     0.0474074</td></tr>\n","<tr><td>train_and_evaluate_644d7651</td><td>TERMINATED</td><td>172.27.13.81:497966</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.361278</td><td style=\"text-align: right;\">      40</td><td>(20, 45, 10, 30, 5) </td><td style=\"text-align: right;\">    0.00183773 </td><td style=\"text-align: right;\">0.198249</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         77.3949</td><td style=\"text-align: right;\">      0.165803</td><td style=\"text-align: right;\">      0.279793</td><td style=\"text-align: right;\">     0.0474074</td></tr>\n","<tr><td>train_and_evaluate_c20b12c2</td><td>TERMINATED</td><td>172.27.13.81:502314</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.205489</td><td style=\"text-align: right;\">      40</td><td>(20, 40, 40, 5, 45) </td><td style=\"text-align: right;\">    0.00195273 </td><td style=\"text-align: right;\">0.272772</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.3923</td><td style=\"text-align: right;\">      0.26615 </td><td style=\"text-align: right;\">      0.284238</td><td style=\"text-align: right;\">     0.213746 </td></tr>\n","<tr><td>train_and_evaluate_db4eb09b</td><td>TERMINATED</td><td>172.27.13.81:506625</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.200177</td><td style=\"text-align: right;\">      40</td><td>(35, 25, 25, 35, 5) </td><td style=\"text-align: right;\">    0.00262905 </td><td style=\"text-align: right;\">0.232921</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.5596</td><td style=\"text-align: right;\">      0.219638</td><td style=\"text-align: right;\">      0.259067</td><td style=\"text-align: right;\">     0.120907 </td></tr>\n","<tr><td>train_and_evaluate_7b9da166</td><td>TERMINATED</td><td>172.27.13.81:510908</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.204425</td><td style=\"text-align: right;\">      40</td><td>(35, 5, 45, 10, 15) </td><td style=\"text-align: right;\">    0.00195226 </td><td style=\"text-align: right;\">0.23552 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.1886</td><td style=\"text-align: right;\">      0.204134</td><td style=\"text-align: right;\">      0.26943 </td><td style=\"text-align: right;\">     0.142575 </td></tr>\n","<tr><td>train_and_evaluate_488abb02</td><td>TERMINATED</td><td>172.27.13.81:515222</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.207286</td><td style=\"text-align: right;\">      40</td><td>(15, 40, 15, 15, 45)</td><td style=\"text-align: right;\">    0.00137059 </td><td style=\"text-align: right;\">0.259853</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.8988</td><td style=\"text-align: right;\">      0.235142</td><td style=\"text-align: right;\">      0.284974</td><td style=\"text-align: right;\">     0.1633   </td></tr>\n","<tr><td>train_and_evaluate_0546a222</td><td>TERMINATED</td><td>172.27.13.81:519512</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.242853</td><td style=\"text-align: right;\">      10</td><td>(15, 45, 15, 25, 20)</td><td style=\"text-align: right;\">    0.00111137 </td><td style=\"text-align: right;\">0.182691</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         59.3959</td><td style=\"text-align: right;\">      0.167959</td><td style=\"text-align: right;\">      0.237726</td><td style=\"text-align: right;\">     0.0526275</td></tr>\n","<tr><td>train_and_evaluate_eeee2e89</td><td>TERMINATED</td><td>172.27.13.81:521398</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.237505</td><td style=\"text-align: right;\">      10</td><td>(30, 25, 10, 25, 40)</td><td style=\"text-align: right;\">    0.00107514 </td><td style=\"text-align: right;\">0.194614</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         51.3981</td><td style=\"text-align: right;\">      0.147668</td><td style=\"text-align: right;\">      0.220207</td><td style=\"text-align: right;\">     0.130314 </td></tr>\n","<tr><td>train_and_evaluate_549b7909</td><td>TERMINATED</td><td>172.27.13.81:523230</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.358426</td><td style=\"text-align: right;\">      40</td><td>(45, 30, 10, 50, 15)</td><td style=\"text-align: right;\">    0.00102694 </td><td style=\"text-align: right;\">0.197743</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.8609</td><td style=\"text-align: right;\">      0.160622</td><td style=\"text-align: right;\">      0.253886</td><td style=\"text-align: right;\">     0.0574823</td></tr>\n","<tr><td>train_and_evaluate_be1a9356</td><td>TERMINATED</td><td>172.27.13.81:527547</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.359256</td><td style=\"text-align: right;\">      40</td><td>(10, 20, 5, 45, 40) </td><td style=\"text-align: right;\">    0.00328259 </td><td style=\"text-align: right;\">0.225662</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.1852</td><td style=\"text-align: right;\">      0.212435</td><td style=\"text-align: right;\">      0.248062</td><td style=\"text-align: right;\">     0.137168 </td></tr>\n","<tr><td>train_and_evaluate_a12a1dc0</td><td>TERMINATED</td><td>172.27.13.81:531888</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.223027</td><td style=\"text-align: right;\">      40</td><td>(10, 20, 50, 40, 45)</td><td style=\"text-align: right;\">    0.00289921 </td><td style=\"text-align: right;\">0.259833</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.6909</td><td style=\"text-align: right;\">      0.243523</td><td style=\"text-align: right;\">      0.287565</td><td style=\"text-align: right;\">     0.218711 </td></tr>\n","<tr><td>train_and_evaluate_0d4fe8b2</td><td>TERMINATED</td><td>172.27.13.81:536232</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.217997</td><td style=\"text-align: right;\">      30</td><td>(35, 20, 15, 50, 50)</td><td style=\"text-align: right;\">    0.000667557</td><td style=\"text-align: right;\">0.22827 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.7514</td><td style=\"text-align: right;\">      0.206718</td><td style=\"text-align: right;\">      0.256477</td><td style=\"text-align: right;\">     0.172463 </td></tr>\n","<tr><td>train_and_evaluate_47a1e45e</td><td>TERMINATED</td><td>172.27.13.81:539752</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.224639</td><td style=\"text-align: right;\">      40</td><td>(35, 40, 10, 25, 10)</td><td style=\"text-align: right;\">    0.00169227 </td><td style=\"text-align: right;\">0.226187</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.4398</td><td style=\"text-align: right;\">      0.204663</td><td style=\"text-align: right;\">      0.245478</td><td style=\"text-align: right;\">     0.141407 </td></tr>\n","<tr><td>train_and_evaluate_d417f89a</td><td>TERMINATED</td><td>172.27.13.81:544075</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.22797 </td><td style=\"text-align: right;\">      40</td><td>(40, 25, 45, 50)    </td><td style=\"text-align: right;\">    0.00171028 </td><td style=\"text-align: right;\">0.284674</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.2543</td><td style=\"text-align: right;\">      0.274611</td><td style=\"text-align: right;\">      0.299742</td><td style=\"text-align: right;\">     0.254763 </td></tr>\n","<tr><td>train_and_evaluate_788e961e</td><td>TERMINATED</td><td>172.27.13.81:548360</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.2602  </td><td style=\"text-align: right;\">      40</td><td>(10, 30, 40, 10, 40)</td><td style=\"text-align: right;\">    0.00258565 </td><td style=\"text-align: right;\">0.244315</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.4841</td><td style=\"text-align: right;\">      0.235142</td><td style=\"text-align: right;\">      0.256477</td><td style=\"text-align: right;\">     0.166029 </td></tr>\n","<tr><td>train_and_evaluate_6e93583d</td><td>TERMINATED</td><td>172.27.13.81:552687</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.250118</td><td style=\"text-align: right;\">      50</td><td>(15, 40, 45, 20, 45)</td><td style=\"text-align: right;\">    0.00139859 </td><td style=\"text-align: right;\">0.253629</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.451 </td><td style=\"text-align: right;\">      0.232558</td><td style=\"text-align: right;\">      0.274611</td><td style=\"text-align: right;\">     0.202544 </td></tr>\n","<tr><td>train_and_evaluate_dc221f5a</td><td>TERMINATED</td><td>172.27.13.81:557836</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.262481</td><td style=\"text-align: right;\">      50</td><td>(45, 20, 50, 40)    </td><td style=\"text-align: right;\">    0.00140446 </td><td style=\"text-align: right;\">0.280007</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.7908</td><td style=\"text-align: right;\">      0.251295</td><td style=\"text-align: right;\">      0.299742</td><td style=\"text-align: right;\">     0.228508 </td></tr>\n","<tr><td>train_and_evaluate_ac9dcee0</td><td>TERMINATED</td><td>172.27.13.81:562958</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.274434</td><td style=\"text-align: right;\">      40</td><td>(50, 15, 40, 10, 25)</td><td style=\"text-align: right;\">    0.000674187</td><td style=\"text-align: right;\">0.213245</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         88.3681</td><td style=\"text-align: right;\">      0.189119</td><td style=\"text-align: right;\">      0.232558</td><td style=\"text-align: right;\">     0.0954856</td></tr>\n","<tr><td>train_and_evaluate_4fe704dd</td><td>TERMINATED</td><td>172.27.13.81:567360</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.329039</td><td style=\"text-align: right;\">      50</td><td>(45, 10, 50, 15, 40)</td><td style=\"text-align: right;\">    0.000661388</td><td style=\"text-align: right;\">0.22204 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.8654</td><td style=\"text-align: right;\">      0.202073</td><td style=\"text-align: right;\">      0.242894</td><td style=\"text-align: right;\">     0.13809  </td></tr>\n","<tr><td>train_and_evaluate_70bed8aa</td><td>TERMINATED</td><td>172.27.13.81:572506</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.334696</td><td style=\"text-align: right;\">      50</td><td>(35, 10, 50, 5, 20) </td><td style=\"text-align: right;\">    0.000174757</td><td style=\"text-align: right;\">0.168221</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         75.485 </td><td style=\"text-align: right;\">      0.158031</td><td style=\"text-align: right;\">      0.183938</td><td style=\"text-align: right;\">     0.047619 </td></tr>\n","<tr><td>train_and_evaluate_6530f5f2</td><td>TERMINATED</td><td>172.27.13.81:577662</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.313416</td><td style=\"text-align: right;\">      50</td><td>(30, 45, 25, 45, 45)</td><td style=\"text-align: right;\">    0.00479717 </td><td style=\"text-align: right;\">0.258805</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.0973</td><td style=\"text-align: right;\">      0.219638</td><td style=\"text-align: right;\">      0.286822</td><td style=\"text-align: right;\">     0.181476 </td></tr>\n","<tr><td>train_and_evaluate_6e0d1541</td><td>TERMINATED</td><td>172.27.13.81:582826</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.304658</td><td style=\"text-align: right;\">      50</td><td>(30, 45, 25, 45, 45)</td><td style=\"text-align: right;\">    0.00393277 </td><td style=\"text-align: right;\">0.254142</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.1405</td><td style=\"text-align: right;\">      0.238342</td><td style=\"text-align: right;\">      0.272021</td><td style=\"text-align: right;\">     0.157852 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-22 22:15:28,773\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 30, 50, 35, 15)}\n","2024-07-22 22:16:30,135\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 30, 35, 15, 10)}\n","2024-07-22 22:17:48,434\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 15, 40, 10, 30)}\n","2024-07-22 22:18:55,906\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 30, 15, 35, 10)}\n","2024-07-22 22:20:09,164\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 25, 30, 50, 35)}\n","2024-07-22 22:21:57,392\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 5, 40, 20, 30)}\n","2024-07-22 22:23:14,015\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 35, 45, 25, 35)}\n","2024-07-22 22:24:46,197\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 10, 10, 15, 10)}\n","2024-07-22 22:25:56,288\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 20, 50, 5, 25)}\n","2024-07-22 22:27:43,173\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 25, 5, 40, 10)}\n","2024-07-22 22:28:52,926\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 30, 30, 40, 5)}\n","2024-07-22 22:30:15,911\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 45, 10, 30, 5)}\n","2024-07-22 22:31:21,485\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 40, 40, 5, 45)}\n","2024-07-22 22:32:30,691\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 25, 25, 35, 5)}\n","2024-07-22 22:33:37,162\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 5, 45, 10, 15)}\n","2024-07-22 22:34:42,069\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 40, 15, 15, 45)}\n","2024-07-22 22:35:44,521\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 45, 15, 25, 20)}\n","2024-07-22 22:36:39,568\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 25, 10, 25, 40)}\n","2024-07-22 22:37:49,010\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 30, 10, 50, 15)}\n","2024-07-22 22:38:58,508\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 20, 5, 45, 40)}\n","2024-07-22 22:40:09,064\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 20, 50, 40, 45)}\n","2024-07-22 22:41:15,960\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 20, 15, 50, 50)}\n","2024-07-22 22:42:27,751\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 40, 10, 25, 10)}\n","2024-07-22 22:43:29,398\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 25, 45, 50)}\n","2024-07-22 22:44:42,866\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (10, 30, 40, 10, 40)}\n","2024-07-22 22:45:56,800\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 40, 45, 20, 45)}\n","2024-07-22 22:47:06,407\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 20, 50, 40)}\n","2024-07-22 22:48:38,962\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 15, 40, 10, 25)}\n","2024-07-22 22:49:55,460\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 10, 50, 15, 40)}\n","2024-07-22 22:51:15,014\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 10, 50, 5, 20)}\n","2024-07-22 22:52:29,782\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 25, 45, 45)}\n","2024-07-22 22:53:54,674\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 25, 45, 45)}\n","2024-07-22 22:53:55,626\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/c/Users/Mayra Elwes/Documents/MyWork/Multi-Task-Learning/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-07-22_22-14-11' in 0.9494s.\n","2024-07-22 22:53:55,640\tINFO tune.py:1041 -- Total run time: 2384.57 seconds (2383.00 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.0017102815105530136, 'batch_size': 128, 'hidden_layers': (40, 25, 45, 50), 'epochs': 40, 'dropout_rate': 0.22797009619569603}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a12_splits, \"A12\")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-07-22 23:30:29</td></tr>\n","<tr><td>Running for: </td><td>00:36:32.65        </td></tr>\n","<tr><td>Memory:      </td><td>5.8/15.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5903414368629456<br>Logical resource usage: 12.0/12 CPUs, 1.0/1 GPUs (1.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_c10369c8</td><td>TERMINATED</td><td>172.27.13.81:588036</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.323953</td><td style=\"text-align: right;\">      30</td><td>(15, 45, 10, 25, 5) </td><td style=\"text-align: right;\">    0.000263225</td><td style=\"text-align: right;\">0.492273</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.7445</td><td style=\"text-align: right;\">      0.266667</td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.210526</td></tr>\n","<tr><td>train_and_evaluate_a2f8cc1e</td><td>TERMINATED</td><td>172.27.13.81:591542</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.393489</td><td style=\"text-align: right;\">      50</td><td>(15, 25, 5, 30, 50) </td><td style=\"text-align: right;\">    0.000318363</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         80.8977</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_73d84b35</td><td>TERMINATED</td><td>172.27.13.81:596731</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.382577</td><td style=\"text-align: right;\">      40</td><td>(15, 40, 15, 30, 20)</td><td style=\"text-align: right;\">    0.000182211</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.9649</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_07941559</td><td>TERMINATED</td><td>172.27.13.81:601038</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.319628</td><td style=\"text-align: right;\">      10</td><td>(50, 5, 5, 45, 25)  </td><td style=\"text-align: right;\">    0.000680146</td><td style=\"text-align: right;\">0.577987</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.9655</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.695238</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_9528de1d</td><td>TERMINATED</td><td>172.27.13.81:602938</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.384691</td><td style=\"text-align: right;\">      50</td><td>(35, 45, 35, 20, 35)</td><td style=\"text-align: right;\">    0.000392624</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.3781</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_455a3bf3</td><td>TERMINATED</td><td>172.27.13.81:608179</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.278024</td><td style=\"text-align: right;\">      20</td><td>(15, 50, 30, 10, 20)</td><td style=\"text-align: right;\">    0.000196942</td><td style=\"text-align: right;\">0.591285</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.7132</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.714286</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_a4805dcc</td><td>TERMINATED</td><td>172.27.13.81:610912</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.301384</td><td style=\"text-align: right;\">      30</td><td>(15, 5, 20, 25, 5)  </td><td style=\"text-align: right;\">    0.000203308</td><td style=\"text-align: right;\">0.579892</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         56.3111</td><td style=\"text-align: right;\">      0.514286</td><td style=\"text-align: right;\">      0.714286</td><td style=\"text-align: right;\">      0.356448</td></tr>\n","<tr><td>train_and_evaluate_b86851f7</td><td>TERMINATED</td><td>172.27.13.81:614354</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.382055</td><td style=\"text-align: right;\">      20</td><td>(40, 10, 45, 35, 30)</td><td style=\"text-align: right;\">    0.00259291 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.6462</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_da652be8</td><td>TERMINATED</td><td>172.27.13.81:617133</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.212914</td><td style=\"text-align: right;\">      40</td><td>(5, 40, 50, 30, 25) </td><td style=\"text-align: right;\">    0.00148402 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.4104</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_a85721e3</td><td>TERMINATED</td><td>172.27.13.81:621502</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.296338</td><td style=\"text-align: right;\">      10</td><td>(20, 15, 30, 30, 30)</td><td style=\"text-align: right;\">    0.00191438 </td><td style=\"text-align: right;\">0.587457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         48.9443</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.685714</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_f312a10b</td><td>TERMINATED</td><td>172.27.13.81:623302</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.44057 </td><td style=\"text-align: right;\">      40</td><td>(25, 40, 45, 10)    </td><td style=\"text-align: right;\">    0.000107881</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         54.2918</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_095f92da</td><td>TERMINATED</td><td>172.27.13.81:627548</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.49805 </td><td style=\"text-align: right;\">      20</td><td>(35, 30, 5, 50, 35) </td><td style=\"text-align: right;\">    0.00950356 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.7205</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_459baaa4</td><td>TERMINATED</td><td>172.27.13.81:630327</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.246631</td><td style=\"text-align: right;\">      20</td><td>(25, 5, 35, 10, 5)  </td><td style=\"text-align: right;\">    0.00653773 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         57.5471</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_24a0ab54</td><td>TERMINATED</td><td>172.27.13.81:632991</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.246687</td><td style=\"text-align: right;\">      10</td><td>(50, 25, 15, 50, 15)</td><td style=\"text-align: right;\">    0.0026747  </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         54.9362</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_85d3da22</td><td>TERMINATED</td><td>172.27.13.81:634830</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.268762</td><td style=\"text-align: right;\">      10</td><td>(35, 15, 45, 10, 20)</td><td style=\"text-align: right;\">    0.00280412 </td><td style=\"text-align: right;\">0.596981</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.336 </td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.647619</td><td style=\"text-align: right;\">      0.387593</td></tr>\n","<tr><td>train_and_evaluate_3b82e09a</td><td>TERMINATED</td><td>172.27.13.81:636688</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.278512</td><td style=\"text-align: right;\">      10</td><td>(20, 10, 5, 30, 45) </td><td style=\"text-align: right;\">    0.000795331</td><td style=\"text-align: right;\">0.589398</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.8383</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.742857</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_38345ed1</td><td>TERMINATED</td><td>172.27.13.81:638620</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.261728</td><td style=\"text-align: right;\">      20</td><td>(30, 35, 40, 25, 25)</td><td style=\"text-align: right;\">    0.00073292 </td><td style=\"text-align: right;\">0.615993</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.0311</td><td style=\"text-align: right;\">      0.52381 </td><td style=\"text-align: right;\">      0.695238</td><td style=\"text-align: right;\">      0.376781</td></tr>\n","<tr><td>train_and_evaluate_76fe20cc</td><td>TERMINATED</td><td>172.27.13.81:641428</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.207782</td><td style=\"text-align: right;\">      20</td><td>(40, 20, 10, 30, 5) </td><td style=\"text-align: right;\">    0.00518464 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.3202</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_9a54f444</td><td>TERMINATED</td><td>172.27.13.81:644152</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.203775</td><td style=\"text-align: right;\">      20</td><td>(20, 35, 10, 10, 10)</td><td style=\"text-align: right;\">    0.0043215  </td><td style=\"text-align: right;\">0.629308</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.1678</td><td style=\"text-align: right;\">      0.571429</td><td style=\"text-align: right;\">      0.685714</td><td style=\"text-align: right;\">      0.480141</td></tr>\n","<tr><td>train_and_evaluate_34c18ba3</td><td>TERMINATED</td><td>172.27.13.81:646824</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.243957</td><td style=\"text-align: right;\">      10</td><td>(35, 25, 5, 50, 50) </td><td style=\"text-align: right;\">    0.00116028 </td><td style=\"text-align: right;\">0.627475</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.4043</td><td style=\"text-align: right;\">      0.542857</td><td style=\"text-align: right;\">      0.780952</td><td style=\"text-align: right;\">      0.415584</td></tr>\n","<tr><td>train_and_evaluate_6f5d0e9e</td><td>TERMINATED</td><td>172.27.13.81:648770</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.224391</td><td style=\"text-align: right;\">      20</td><td>(40, 10, 5, 50, 20) </td><td style=\"text-align: right;\">    0.000521229</td><td style=\"text-align: right;\">0.608374</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.7399</td><td style=\"text-align: right;\">      0.533333</td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.422494</td></tr>\n","<tr><td>train_and_evaluate_c1ad348b</td><td>TERMINATED</td><td>172.27.13.81:651505</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.200149</td><td style=\"text-align: right;\">      50</td><td>(40, 25, 25, 45, 50)</td><td style=\"text-align: right;\">    0.00439857 </td><td style=\"text-align: right;\">0.678814</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.749 </td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.742857</td><td style=\"text-align: right;\">      0.546347</td></tr>\n","<tr><td>train_and_evaluate_dbcad535</td><td>TERMINATED</td><td>172.27.13.81:656673</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.241649</td><td style=\"text-align: right;\">      20</td><td>(35, 25, 35, 40, 45)</td><td style=\"text-align: right;\">    0.00117665 </td><td style=\"text-align: right;\">0.631249</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.7647</td><td style=\"text-align: right;\">      0.59434 </td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.516781</td></tr>\n","<tr><td>train_and_evaluate_15a8d1be</td><td>TERMINATED</td><td>172.27.13.81:659503</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.205063</td><td style=\"text-align: right;\">      50</td><td>(30, 40, 25, 10)    </td><td style=\"text-align: right;\">    0.00424025 </td><td style=\"text-align: right;\">0.669272</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.7797</td><td style=\"text-align: right;\">      0.628571</td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.600137</td></tr>\n","<tr><td>train_and_evaluate_5fb5fe46</td><td>TERMINATED</td><td>172.27.13.81:664548</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.201905</td><td style=\"text-align: right;\">      50</td><td>(15, 25, 15, 45, 50)</td><td style=\"text-align: right;\">    0.00437546 </td><td style=\"text-align: right;\">0.654016</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.1862</td><td style=\"text-align: right;\">      0.561905</td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.513889</td></tr>\n","<tr><td>train_and_evaluate_1f1b59f9</td><td>TERMINATED</td><td>172.27.13.81:669672</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.23293 </td><td style=\"text-align: right;\">      50</td><td>(25, 10, 50, 5, 40) </td><td style=\"text-align: right;\">    0.00384556 </td><td style=\"text-align: right;\">0.650135</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.8571</td><td style=\"text-align: right;\">      0.571429</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.535147</td></tr>\n","<tr><td>train_and_evaluate_0ecf2e79</td><td>TERMINATED</td><td>172.27.13.81:674823</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.203001</td><td style=\"text-align: right;\">      50</td><td>(45, 50, 25, 10, 30)</td><td style=\"text-align: right;\">    0.00957884 </td><td style=\"text-align: right;\">0.648284</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.0609</td><td style=\"text-align: right;\">      0.609524</td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.563786</td></tr>\n","<tr><td>train_and_evaluate_c58eb422</td><td>TERMINATED</td><td>172.27.13.81:679975</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.203391</td><td style=\"text-align: right;\">      50</td><td>(50, 50, 40, 50, 50)</td><td style=\"text-align: right;\">    0.00901574 </td><td style=\"text-align: right;\">0.65779 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.3462</td><td style=\"text-align: right;\">      0.590476</td><td style=\"text-align: right;\">      0.742857</td><td style=\"text-align: right;\">      0.535924</td></tr>\n","<tr><td>train_and_evaluate_9798e82a</td><td>TERMINATED</td><td>172.27.13.81:685157</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.346183</td><td style=\"text-align: right;\">      50</td><td>(40, 45, 25, 10, 20)</td><td style=\"text-align: right;\">    0.00595626 </td><td style=\"text-align: right;\">0.663594</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.5699</td><td style=\"text-align: right;\">      0.609524</td><td style=\"text-align: right;\">      0.752381</td><td style=\"text-align: right;\">      0.535147</td></tr>\n","<tr><td>train_and_evaluate_871e09cf</td><td>TERMINATED</td><td>172.27.13.81:690287</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.355375</td><td style=\"text-align: right;\">      50</td><td>(40, 45, 30, 50, 35)</td><td style=\"text-align: right;\">    0.00658026 </td><td style=\"text-align: right;\">0.633082</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.6922</td><td style=\"text-align: right;\">      0.571429</td><td style=\"text-align: right;\">      0.695238</td><td style=\"text-align: right;\">      0.480141</td></tr>\n","<tr><td>train_and_evaluate_98e37421</td><td>TERMINATED</td><td>172.27.13.81:695485</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.348241</td><td style=\"text-align: right;\">      50</td><td>(15, 10, 15, 35, 25)</td><td style=\"text-align: right;\">    0.00681171 </td><td style=\"text-align: right;\">0.627385</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.8152</td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.505565</td></tr>\n","<tr><td>train_and_evaluate_e0bec25a</td><td>TERMINATED</td><td>172.27.13.81:700556</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.420364</td><td style=\"text-align: right;\">      30</td><td>(15, 25, 5, 5, 30)  </td><td style=\"text-align: right;\">    0.00324055 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.5652</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-22 22:55:04,247\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 45, 10, 25, 5)}\n","2024-07-22 22:56:28,286\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 25, 5, 30, 50)}\n","2024-07-22 22:57:33,559\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 40, 15, 30, 20)}\n","2024-07-22 22:58:36,447\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 5, 5, 45, 25)}\n","2024-07-22 22:59:53,906\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 45, 35, 20, 35)}\n","2024-07-22 23:01:02,302\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 50, 30, 10, 20)}\n","2024-07-22 23:02:01,783\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 5, 20, 25, 5)}\n","2024-07-22 23:03:08,265\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 10, 45, 35, 30)}\n","2024-07-22 23:04:23,915\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (5, 40, 50, 30, 25)}\n","2024-07-22 23:05:16,638\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 15, 30, 30, 30)}\n","2024-07-22 23:06:14,361\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 40, 45, 10)}\n","2024-07-22 23:07:19,524\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 30, 5, 50, 35)}\n","2024-07-22 23:08:20,398\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 5, 35, 10, 5)}\n","2024-07-22 23:09:18,772\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 25, 15, 50, 15)}\n","2024-07-22 23:10:23,105\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 15, 45, 10, 20)}\n","2024-07-22 23:11:34,775\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 10, 5, 30, 45)}\n","2024-07-22 23:12:46,455\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 35, 40, 25, 25)}\n","2024-07-22 23:13:56,495\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 20, 10, 30, 5)}\n","2024-07-22 23:15:00,865\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (20, 35, 10, 10, 10)}\n","2024-07-22 23:16:11,810\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 25, 5, 50, 50)}\n","2024-07-22 23:17:21,692\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 10, 5, 50, 20)}\n","2024-07-22 23:18:38,747\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 25, 25, 45, 50)}\n","2024-07-22 23:19:50,960\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 25, 35, 40, 45)}\n","2024-07-22 23:20:49,853\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 40, 25, 10)}\n","2024-07-22 23:22:01,212\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 25, 15, 45, 50)}\n","2024-07-22 23:23:15,016\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (25, 10, 50, 5, 40)}\n","2024-07-22 23:24:30,299\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 50, 25, 10, 30)}\n","2024-07-22 23:25:48,105\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 50, 40, 50, 50)}\n","2024-07-22 23:27:01,878\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 45, 25, 10, 20)}\n","2024-07-22 23:28:18,518\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 45, 30, 50, 35)}\n","2024-07-22 23:29:24,859\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 10, 15, 35, 25)}\n","2024-07-22 23:30:28,806\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (15, 25, 5, 5, 30)}\n","2024-07-22 23:30:29,314\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/c/Users/Mayra Elwes/Documents/MyWork/Multi-Task-Learning/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-07-22_22-53-56' in 0.5038s.\n","2024-07-22 23:30:29,330\tINFO tune.py:1041 -- Total run time: 2193.37 seconds (2192.15 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.004398572300594925, 'batch_size': 128, 'hidden_layers': (40, 25, 25, 45, 50), 'epochs': 50, 'dropout_rate': 0.2001488958418336}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a21_splits, \"A21\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["df_dict_A2 = pd.read_pickle(\"results/output_A2.pkl\")\n","df_dict_A3 = pd.read_pickle(\"results/output_A3.pkl\")\n","df_dict_A4 = pd.read_pickle(\"results/output_A4.pkl\")\n","df_dict_A12 = pd.read_pickle(\"results/output_A12.pkl\")\n","df_dict_A21 = pd.read_pickle(\"results/output_A21.pkl\")\n","\n","df_dict = pd.concat([df_dict_A2, df_dict_A3, df_dict_A4, df_dict_A12, df_dict_A21])\n","\n","df_dict = df_dict.loc[:,~df_dict.columns.duplicated()].copy()\n","df_dict = df_dict.drop_duplicates()\n","\n","df_dict.to_csv('outputs/Classic_FFN_one_split_metrics_system.csv')"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["df_dict_full_A2 = pd.read_pickle(\"results/output_full_A2.pkl\")\n","df_dict_full_A3 = pd.read_pickle(\"results/output_full_A3.pkl\")\n","df_dict_full_A4 = pd.read_pickle(\"results/output_full_A4.pkl\")\n","df_dict_full_A12 = pd.read_pickle(\"results/output_full_A12.pkl\")\n","df_dict_full_A21 = pd.read_pickle(\"results/output_full_A21.pkl\")\n","\n","df_full_dict = pd.concat([df_dict_full_A2, df_dict_full_A3, df_dict_full_A4, df_dict_full_A12, df_dict_full_A21])\n","\n","df_full_dict = df_full_dict.loc[:, ~df_full_dict.columns.duplicated()].copy()\n","df_full_dict = df_full_dict.drop_duplicates()\n","\n","df_full_dict.to_csv(\"outputs/Classic_FFN_full_split_metrics_system.csv\")\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["df_dict_full_A2.to_csv(\"outputs/Classic_FFN_full_split_metrics_A2.csv\")\n","df_dict_full_A3.to_csv(\"outputs/Classic_FFN_full_split_metrics_A3.csv\")\n","df_dict_full_A4.to_csv(\"outputs/Classic_FFN_full_split_metrics_A4.csv\")\n","df_dict_full_A12.to_csv(\"outputs/Classic_FFN_full_split_metrics_A12.csv\")\n","df_dict_full_A21.to_csv(\"outputs/Classic_FFN_full_split_metrics_A21.csv\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5354302,"sourceId":8905505,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
