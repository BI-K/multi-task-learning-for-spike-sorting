{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Prep"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:03.334868Z","iopub.status.busy":"2024-07-10T10:30:03.334549Z","iopub.status.idle":"2024-07-10T10:30:13.163038Z","shell.execute_reply":"2024-07-10T10:30:13.161739Z","shell.execute_reply.started":"2024-07-10T10:30:03.334833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-24 14:07:03,747\tINFO worker.py:1621 -- Calling ray.init() again after it has already been called.\n"]},{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import os\n","import itertools\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, matthews_corrcoef\n","from sklearn.preprocessing import LabelBinarizer\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, ReLU, Input\n","from tensorflow.keras.optimizers import Adam\n","from math import floor\n","\n","import psutil\n","import time\n","import logging\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","import ray\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.air import session\n","from ray.tune.integration.keras import TuneReportCallback\n","from ray.tune.search.optuna import OptunaSearch\n","\n","import helper\n","\n","# Initialize Ray\n","ray.init(ignore_reinit_error=True)\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","import torch\n","\n","# Neural Network Definition\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#tf.debugging.set_log_device_placement(True)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The file results/Feature_based/exp_24_08_2024/output_A2.pkl does not exist.\n","The file results/Feature_based/exp_24_08_2024/output_A3.pkl does not exist.\n","The file results/Feature_based/exp_24_08_2024/output_A4.pkl does not exist.\n","The file results/Feature_based/exp_24_08_2024/output_A12.pkl does not exist.\n","The file results/Feature_based/exp_24_08_2024/output_A21.pkl does not exist.\n"]}],"source":["files_to_delete = [\"results/Feature_based/exp_24_08_2024/output_A2.pkl\", \"results/Feature_based/exp_24_08_2024/output_A3.pkl\", \"results/Feature_based/exp_24_08_2024/output_A4.pkl\", \"results/Feature_based/exp_24_08_2024/output_A12.pkl\", \"results/Feature_based/exp_24_08_2024/output_A21.pkl\"]\n","\n","for file in files_to_delete:\n","    if os.path.exists(file):\n","        os.remove(file)\n","    else:\n","        print(f\"The file {file} does not exist.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Feature-Based MTL model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:16.513190Z","iopub.status.busy":"2024-07-10T10:30:16.512565Z","iopub.status.idle":"2024-07-10T10:30:16.519877Z","shell.execute_reply":"2024-07-10T10:30:16.518848Z","shell.execute_reply.started":"2024-07-10T10:30:16.513154Z"},"trusted":true},"outputs":[],"source":["\n","class FeatureBasedMTLModelCNN(nn.Module):\n","    def __init__(self, hidden_layers_shared=[50,30], hidden_layers_outputs=[[10],[10],[10],[10],[10]], activation='relu', dropout=0.3):\n","        super(FeatureBasedMTLModelCNN, self).__init__()\n","\n","        self.activation = activation\n","\n","        # shared layers\n","        self.shared_layers = nn.ModuleList()\n","        self.conv_layers = nn.ModuleList()\n","\n","        # Define convolutional layers\n","        self.conv_layers.append(nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1))\n","        self.conv_layers.append(nn.BatchNorm1d(16))\n","        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n","        self.conv_layers.append(nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1))\n","        self.conv_layers.append(nn.BatchNorm1d(32))\n","        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n","\n","        input_dim = 32 * (30 // 4)  # Adjusted input dimension after convolution and pooling\n","\n","        for hidden_layer in hidden_layers_shared:\n","            self.shared_layers.append(nn.Linear(input_dim, hidden_layer))\n","            self.shared_layers.append(nn.Dropout(dropout))\n","            input_dim = hidden_layer\n","\n","        # output layers\n","        self.output_a2 = self.create_output_layers(hidden_layers_outputs[0], input_dim, 2)\n","        self.output_a3 = self.create_output_layers(hidden_layers_outputs[1], input_dim, 2)\n","        self.output_a4 = self.create_output_layers(hidden_layers_outputs[2], input_dim, 3)\n","        self.output_a12 = self.create_output_layers(hidden_layers_outputs[3], input_dim, 6)\n","        self.output_a21 = self.create_output_layers(hidden_layers_outputs[4], input_dim, 2)\n","\n","    def create_output_layers(self, hidden_layers, input_dim, output_dim):\n","        layers = nn.ModuleList()\n","        for hidden_layer in hidden_layers:\n","            layers.append(nn.Linear(input_dim, hidden_layer))\n","            input_dim = hidden_layer\n","        layers.append(nn.Linear(input_dim, output_dim))\n","        return layers\n","\n","    def forward(self, x: torch.Tensor, task_id: str):\n","        \n","        # Convolutional layers\n","        x = x.unsqueeze(1)  # Add channel dimension\n","        for conv in self.conv_layers:\n","            if isinstance(conv, nn.Conv1d):\n","                x = conv(x)\n","                x = F.relu(x)\n","            elif isinstance(conv, nn.MaxPool1d):\n","                x = conv(x)\n","            elif isinstance(conv, nn.BatchNorm1d):\n","                x = conv(x)\n","        \n","        x = x.view(x.size(0), -1)  # Flatten the output for the linear layers\n","\n","        # shared layers\n","        for layer in self.shared_layers:\n","            if isinstance(layer, nn.Linear):\n","                x = layer(x)\n","                x = self.apply_activation(x)\n","            elif isinstance(layer, nn.Dropout):\n","                x = layer(x)\n","        \n","        # output layers\n","        if task_id == 'a2':\n","            return self.forward_output_layers(x, self.output_a2)\n","        elif task_id == 'a3':\n","            return self.forward_output_layers(x, self.output_a3)\n","        elif task_id == 'a4':\n","            return self.forward_output_layers(x, self.output_a4)\n","        elif task_id == 'a12':\n","            return self.forward_output_layers(x, self.output_a12)\n","        elif task_id == 'a21':\n","            return self.forward_output_layers(x, self.output_a21)\n","        else:\n","            raise ValueError(f'Invalid task_id: {task_id}')\n","        \n","\n","    def forward_output_layers(self, x, layers):\n","        for layer in layers[:-1]:\n","            x = layer(x)\n","            x = self.apply_activation(x)\n","            \n","        x = layers[-1](x)\n","        return F.softmax(x, dim=1)\n","    \n","    def apply_activation(self, x):\n","        if self.activation == 'relu':\n","            return F.relu(x)\n","        elif self.activation == 'tanh':\n","            return F.tanh(x)\n","        elif self.activation == 'sigmoid':\n","            return F.sigmoid(x)\n","        else:\n","            raise ValueError(f'Invalid activation: {self.activation}')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# TODO activation for now only relu\n","class FeatureBasedMTLModel(nn.Module):\n","    def __init__(self, hidden_layers_shared=[50,30], hidden_layers_outputs=[[10],[10],[10],[10],[10]], activation='relu', dropout=0.5):\n","        super(FeatureBasedMTLModel, self).__init__()\n","\n","        self.activation = activation\n","        self.dropout = dropout\n","\n","        # shared layers\n","        self.shared_layers = nn.ModuleList()\n","        input_dim = 30\n","        for hidden_layer in hidden_layers_shared:\n","            self.shared_layers.append(nn.Linear(input_dim, hidden_layer))\n","            self.shared_layers.append(nn.Dropout(self.dropout))  # Add dropout layer\n","            input_dim = hidden_layer\n","\n","        # output layers\n","        self.output_a2 = self.create_output_layers(hidden_layers_outputs[0], input_dim, 2)\n","        self.output_a3 = self.create_output_layers(hidden_layers_outputs[1], input_dim, 2)\n","        self.output_a4 = self.create_output_layers(hidden_layers_outputs[2], input_dim, 3)\n","        self.output_a12 = self.create_output_layers(hidden_layers_outputs[3], input_dim, 6)\n","        self.output_a21 = self.create_output_layers(hidden_layers_outputs[4], input_dim, 2)\n","\n","\n","    def create_output_layers(self, hidden_layers, input_dim, output_dim):\n","        layers = nn.ModuleList()\n","        for hidden_layer in hidden_layers:\n","            layers.append(nn.Linear(input_dim, hidden_layer))\n","            layers.append(nn.Dropout(self.dropout)) \n","            input_dim = hidden_layer\n","        layers.append(nn.Linear(input_dim, output_dim))\n","        return layers\n","\n","    def forward(self, x: torch.Tensor, task_id: str):\n","        # shared layers\n","        for layer in self.shared_layers:\n","            if self.activation == 'relu':\n","                x = F.relu(layer(x))\n","            elif self.activation == 'tanh':\n","                x = F.tanh(layer(x))\n","            elif self.activation == 'sigmoid':\n","                x = F.sigmoid(layer(x))\n","            else:\n","                raise ValueError(f'Invalid activation: {self.activation}')\n","\n","        \n","        # output layers\n","        if task_id == 'a2':\n","            return self.forward_output_layers(x, self.output_a2)\n","        elif task_id == 'a3':\n","            return self.forward_output_layers(x, self.output_a3)\n","        elif task_id == 'a4':\n","            return self.forward_output_layers(x, self.output_a4)\n","        elif task_id == 'a12':\n","            return self.forward_output_layers(x, self.output_a12)\n","        elif task_id == 'a21':\n","            return self.forward_output_layers(x, self.output_a21)\n","        else:\n","            raise ValueError(f'Invalid task_id: {task_id}')\n","        \n","\n","    def forward_output_layers(self, x, layers):\n","        for layer in layers[:-1]:\n","            if self.activation == 'relu':\n","                x = F.relu(layer(x))\n","            elif self.activation == 'tanh':\n","                x = F.tanh(layer(x))\n","            elif self.activation == 'sigmoid':\n","                x = F.sigmoid(layer(x))\n","            else:\n","                raise ValueError(f'Invalid activation: {self.activation}')\n","            \n","        x = layers[-1](x)\n","        return F.softmax(x, dim=1)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:17.291825Z","iopub.status.busy":"2024-07-10T10:30:17.290934Z","iopub.status.idle":"2024-07-10T10:30:17.298631Z","shell.execute_reply":"2024-07-10T10:30:17.297583Z","shell.execute_reply.started":"2024-07-10T10:30:17.291782Z"},"trusted":true},"outputs":[],"source":["\n","def create_batches(X_train, y_train, batch_size):\n","    X_train_batches = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n","    y_train_batches = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n","\n","    for dataset in helper.dataset_list:\n","\n","        for i in range(0, floor(len(X_train[dataset])/batch_size)):\n","                X_train_batches[dataset][str(i)] =  X_train[dataset].iloc[i* batch_size:i*batch_size+batch_size]\n","                y_train_batches[dataset][str(i)] =  y_train[dataset].iloc[i* batch_size:i* batch_size+batch_size]\n","\n","    return X_train_batches, y_train_batches\n","\n","# batchsize in this case refers to the size of batch per dataset\n","def train(model, X_train, y_train, num_epochs, batch_size, learning_rate, loss_weighting_a12):\n","        \n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(model.parameters())\n","\n","        max_number_of_batches = max(len(X_train[dataset]) // batch_size for dataset in helper.dataset_list)\n","\n","        X_train, y_train = create_batches(X_train, y_train, batch_size)\n","\n","        for epoch in range(num_epochs):\n","                optimizer.zero_grad()\n","\n","                losses_per_epoch = {'a2': 0, 'a3': 0, 'a4': 0, 'a12': 0, 'a21': 0}\n","\n","                for batch_number in range(0, max_number_of_batches):\n","\n","                        for dataset in helper.dataset_list:\n","\n","                                if len(X_train[dataset]) > batch_number:\n","\n","                                        X_train_tensor = torch.tensor(X_train[dataset][str(batch_number)].values, dtype=torch.float32)\n","                                        X_train_tensor.to(device)\n","                                        y_train_tensor= torch.tensor(y_train[dataset][str(batch_number)].values, dtype=torch.float32)\n","                                        y_train_tensor.to(device)\n","                                        outputs = model(X_train_tensor, task_id = dataset)\n","\n","                                        \n","                                        loss = criterion(outputs, y_train_tensor)\n","\n","                                        if dataset == 'a12':\n","                                                losses_per_epoch[dataset] += loss_weighting_a12\n","                                        else:\n","                                                losses_per_epoch[dataset] += loss\n","\n","                for dataset in helper.dataset_list:\n","                        losses_per_epoch[dataset] /= len(X_train[dataset])\n","\n","                # for now sum of average loss per dataset\n","                loss = sum(losses_per_epoch.values())\n","                loss.backward()\n","                optimizer.step()\n","\n","        return model\n","\n","\n","def mcc(fn, fp, tn, tp):\n","    return (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n","\n","def evaluate(model, X_test, y_test):\n","\n","    eval_dict = {'a2': {}, 'a3': {}, 'a4': {}, 'a12': {}, 'a21': {}}\n","    num_classes = {'a2': 2, 'a3': 2, 'a4': 3, 'a12': 6, 'a21': 2}\n","    for dataset in helper.dataset_list:\n","        X_test_tensor = torch.tensor(X_test[dataset].values, dtype=torch.float32)\n","        X_test_tensor.to(device)\n","        #y_test_tensor = torch.tensor(y_test[dataset].values, dtype=torch.float32)\n","        y_pred_tensor = model(X_test_tensor, task_id = dataset)\n","        y_pred = y_pred_tensor.detach().numpy()\n","        y_pred = np.argmax(y_pred, axis=1)\n","\n","        y_test_np = y_test[dataset].to_numpy()\n","        y_test_np = np.argmax(y_test_np, axis=1)\n","\n","\n","        # Calculate classification metrics using one-hot encoded targets\n","        eval_dict[dataset]['accuracy'] = accuracy_score(y_test_np, y_pred)\n","        eval_dict[dataset]['micro_f1'] = f1_score(y_test_np, y_pred, average='micro')\n","        eval_dict[dataset]['macro_f1'] = f1_score(y_test_np, y_pred, average='macro')\n","\n","        eval_dict[dataset]['mcc'] = matthews_corrcoef(y_test_np, y_pred)\n","\n","    return eval_dict"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:17.753453Z","iopub.status.busy":"2024-07-10T10:30:17.753116Z","iopub.status.idle":"2024-07-10T10:30:17.765149Z","shell.execute_reply":"2024-07-10T10:30:17.764141Z","shell.execute_reply.started":"2024-07-10T10:30:17.753425Z"},"trusted":true},"outputs":[],"source":["def evaluate_model_on_dataset_one_split(split_index, config, CNN=False, normalized=False):\n","    \n","\n","    X_train, X_test, y_train, y_test =  helper.get_joined_train_test_folds(split_index, normalized)\n","\n","    if CNN:\n","        model = FeatureBasedMTLModelCNN(activation=config['activation'], hidden_layers_shared=config['shared_hidden_layers'], hidden_layers_outputs=[\n","            config['a2_output_hidden_layers'], config['a3_output_hidden_layers'], config['a4_output_hidden_layers'], config['a12_output_hidden_layers'], config['a21_output_hidden_layers']\n","        ], dropout=config['dropout_rate'])\n","    else:\n","        model = FeatureBasedMTLModel(activation=config['activation'], hidden_layers_shared=config['shared_hidden_layers'], hidden_layers_outputs=[\n","            config['a2_output_hidden_layers'], config['a3_output_hidden_layers'], config['a4_output_hidden_layers'], config['a12_output_hidden_layers'], config['a21_output_hidden_layers']\n","        ], dropout=config['dropout_rate'])\n","\n","    # Train the model and collect performance data\n","    model = train(model, X_train, y_train, num_epochs=config['epochs'], batch_size=config['batch_size'], learning_rate=config['learning_rate'], loss_weighting_a12=config['a12_loss_weighting'])\n","    # Evaluate the model and collect performance data\n","    eval_dict = evaluate(model, X_test, y_test)\n","\n","    return eval_dict\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#evaluate_model_on_dataset_one_split(1, {\n","#    'activation': 'sigmoid',\n","#    'shared_hidden_layers': [50, 30],\n","#    'a2_output_hidden_layers': [10],\n","#    'a3_output_hidden_layers': [10],\n","#    'a4_output_hidden_layers': [10],\n","#    'a12_output_hidden_layers': [10],\n","#    'a21_output_hidden_layers': [10],\n","#    'epochs': 10,\n","#    'batch_size': 32,\n","#    'learning_rate': 0.001\n","#})"]},{"cell_type":"markdown","metadata":{},"source":["## 5-Fold-Cross Validation"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df_data_spike_1_split = {\n","    'a2': pd.DataFrame(),\n","    'a3': pd.DataFrame(),\n","    'a4': pd.DataFrame(),\n","    'a12': pd.DataFrame(),\n","    'a21': pd.DataFrame()\n","}\n","\n","df_data_spike_full_split = pd.DataFrame()\n","\n","data_spike_exec_1_split_dict = dict()\n","data_spike_exec_full_split_dict = {\n","    'a2': {},\n","    'a3': {},\n","    'a4': {},\n","    'a12': {},\n","    'a21': {}\n","}\n","\n","\n","best_params_list_getting = []\n","\n","def custom_trial_dirname(trial):\n","    return f\"trial_{trial.trial_id}\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:21.118433Z","iopub.status.busy":"2024-07-10T10:30:21.118035Z","iopub.status.idle":"2024-07-10T11:21:07.299373Z","shell.execute_reply":"2024-07-10T11:21:07.298387Z","shell.execute_reply.started":"2024-07-10T10:30:21.118400Z"},"trusted":true},"outputs":[],"source":["def train_and_evaluate(config, CNN = False, normalized=False):\n","    \n","    global data_spike_exec_1_split_dict\n","    global data_spike_exec_full_split_dict\n","    global df_data_spike_1_split\n","    global results_dir\n","\n","    overall_result = {\n","    \"a2\": {\n","        \"accuracy_scores\": [],\n","        \"f1_macro_scores\": [],\n","        \"f1_micro_scores\": [],\n","        \"mcc_scores\": []\n","    },\n","    \"a3\": {\n","        \"accuracy_scores\": [],\n","        \"f1_macro_scores\": [],\n","        \"f1_micro_scores\": [],\n","        \"mcc_scores\": []\n","    },\n","    \"a4\": { \n","        \"accuracy_scores\": [],\n","        \"f1_macro_scores\": [],\n","        \"f1_micro_scores\": [],\n","        \"mcc_scores\": []\n","    },\n","    \"a12\": {\n","        \"accuracy_scores\": [],\n","        \"f1_macro_scores\": [],\n","        \"f1_micro_scores\": [],\n","        \"mcc_scores\": []\n","    },\n","    \"a21\": {\n","        \"accuracy_scores\": [],\n","        \"f1_macro_scores\": [],\n","        \"f1_micro_scores\": [],\n","        \"mcc_scores\": []\n","    }\n","    }\n","\n","    accuracy_scores = []\n","    f1_macro_scores = []\n","    f1_micro_scores = []\n","    mcc_scores = []\n","\n","    data_spike_exec_1_split_dict = {}\n","    for dataset in overall_result.keys():\n","        data_spike_exec_1_split_dict[dataset] = pd.DataFrame()\n","\n","    #session_id_for_df = session.get_trial_id()\n","    #print(type(session_id_for_df))\n","    \n","    # save individual results for each dataset\n","    for i in range(5):\n","        eval_dict = evaluate_model_on_dataset_one_split(i, config, CNN=CNN, normalized=normalized)\n","        # fill all nan values in eval_dict with 0\n","        for dataset in eval_dict.keys():\n","            for key in eval_dict[dataset].keys():\n","                if np.isnan(eval_dict[dataset][key]):\n","                    eval_dict[dataset][key] = 0\n","        \n","        for dataset in overall_result.keys():\n","\n","            overall_result[dataset]['accuracy_scores'].append(eval_dict[dataset]['accuracy'])\n","            overall_result[dataset]['f1_macro_scores'].append(eval_dict[dataset]['macro_f1'])\n","            overall_result[dataset]['f1_micro_scores'].append(eval_dict[dataset]['micro_f1'])\n","            overall_result[dataset]['mcc_scores'].append(eval_dict[dataset]['mcc'])\n","\n","            accuracy_scores.append(eval_dict[dataset]['accuracy'])\n","            f1_macro_scores.append(eval_dict[dataset]['macro_f1'])\n","            f1_micro_scores.append(eval_dict[dataset]['micro_f1'])\n","            mcc_scores.append(eval_dict[dataset]['mcc'])\n","            \n","            new_entry= pd.DataFrame({\n","            #data_spike_exec_1_split_dict[dataset][dataset + \"_\" + str(i+1) + \"_\" + session_id_for_df] = {\n","                #\"name\": [session_id_for_df],\n","                \"accuracy\": [eval_dict[dataset]['accuracy']],\n","                \"macro f1\": [eval_dict[dataset]['macro_f1']],\n","                \"micro_f1\": [eval_dict[dataset]['micro_f1']],\n","                \"mcc\": [eval_dict[dataset]['mcc']],\n","                \"config\": [str(config)],\n","                \"config_activation\": [str(config[\"activation\"])],\n","                \"config_learning_rate\": [str(config[\"learning_rate\"])],\n","                \"config_number_shared_hidden_layers\": [str(len(config[\"shared_hidden_layers\"]))],\n","                \"config_shared_hidden_layers\": [str(config[\"shared_hidden_layers\"])],\n","                \"config_number_a2_hidden_layers\": [str(len(config[\"a2_output_hidden_layers\"]))],\n","                \"config_a2_hidden_layers\": [str(config[\"a2_output_hidden_layers\"])],\n","                \"config_number_a3_hidden_layers\": [str(len(config[\"a3_output_hidden_layers\"]))],\n","                \"config_a3_hidden_layers\": [str(config[\"a3_output_hidden_layers\"])],\n","                \"config_number_a4_hidden_layers\": [str(len(config[\"a4_output_hidden_layers\"]))],\n","                \"config_a4_hidden_layers\": [str(config[\"a4_output_hidden_layers\"])],\n","                \"config_number_a12_hidden_layers\": [str(len(config[\"a12_output_hidden_layers\"]))],\n","                \"config_a12_hidden_layers\": [str(config[\"a12_output_hidden_layers\"])],\n","                \"config_number_a21_hidden_layers\": [str(len(config[\"a21_output_hidden_layers\"]))],\n","                \"config_a21_hidden_layers\": [str(config[\"a21_output_hidden_layers\"])],\n","                \"config_epochs\": [str(config[\"epochs\"])],\n","                \"config_batch_size\": [str(config[\"batch_size\"])],\n","                \"config_dropout_rate\": [str(config[\"dropout_rate\"])],\n","                \"config_a12_loss_weighting\": [str(config[\"a12_loss_weighting\"])],\n","                \"config_is_cnn\": [str(CNN)],\n","                \"config_normalized\": [str(normalized)]\n","            })\n","\n","            if i == 0:\n","                df_data_spike_1_split[dataset] = new_entry\n","            else:\n","                df_data_spike_1_split[dataset]= pd.concat([df_data_spike_1_split[dataset], new_entry], ignore_index=True, axis=0)\n","\n","\n","    \n","    for dataset in overall_result.keys():\n","\n","        #data_spike_exec_full_split_dict[dataset][dataset + \"_\" + session_id_for_df] = {\n","        new_data_spike_exec_full_split_dict = pd.DataFrame({\n","                #\"name\" : [session_id_for_df],\n","                \"config_is_cnn\": [str(CNN)],\n","                \"config_normalized\": [str(normalized)],\n","                \"config_activation\": [str(config[\"activation\"])],\n","                \"config_learning_rate\": [str(config[\"learning_rate\"])],\n","                \"config_number_shared_hidden_layers\": [str(len(config[\"shared_hidden_layers\"]))],\n","                \"config_shared_hidden_layers\": [str(config[\"shared_hidden_layers\"])],\n","                \"config_number_a2_hidden_layers\": [str(len(config[\"a2_output_hidden_layers\"]))],\n","                \"config_a2_hidden_layers\": [str(config[\"a2_output_hidden_layers\"])],\n","                \"config_number_a3_hidden_layers\": [str(len(config[\"a3_output_hidden_layers\"]))],\n","                \"config_a3_hidden_layers\": [str(config[\"a3_output_hidden_layers\"])],\n","                \"config_number_a4_hidden_layers\": [str(len(config[\"a4_output_hidden_layers\"]))],\n","                \"config_a4_hidden_layers\": [str(config[\"a4_output_hidden_layers\"])],\n","                \"config_number_a12_hidden_layers\": [str(len(config[\"a12_output_hidden_layers\"]))],\n","                \"config_a12_hidden_layers\": [str(config[\"a12_output_hidden_layers\"])],\n","                \"config_number_a21_hidden_layers\": [str(len(config[\"a21_output_hidden_layers\"]))],\n","                \"config_a21_hidden_layers\": [str(config[\"a21_output_hidden_layers\"])],\n","                \"config_epochs\": [str(config[\"epochs\"])],\n","                \"config_batch_size\": [str(config[\"batch_size\"])],\n","                \"config_dropout_rate\": [str(config[\"dropout_rate\"])],\n","                \"config_a12_loss_weighting\": [str(config[\"a12_loss_weighting\"])],\n","                \n","\n","                \"mean_accuracy\": [np.mean(overall_result[dataset]['accuracy_scores'])],\n","                \"std_accuracy\": [np.std(overall_result[dataset]['accuracy_scores'])],\n","                \"min_accuracy\": [min(overall_result[dataset]['accuracy_scores'])],\n","                \"max_accuracy\": [max(overall_result[dataset]['accuracy_scores'])],\n","\n","                \"mean_f1_macro\": [np.mean(overall_result[dataset]['f1_macro_scores'])],\n","                \"min_f1_macro\": [min(overall_result[dataset]['f1_macro_scores'])],\n","                \"max_f1_macro\": [max(overall_result[dataset]['f1_macro_scores'])],\n","                \"std_f1_macro\": [np.std(overall_result[dataset]['f1_macro_scores'])],\n","\n","                \"mean_f1_micro\": [np.mean(overall_result[dataset]['f1_micro_scores'])],\n","                \"min_f1_micro\": [min(overall_result[dataset]['f1_micro_scores'])],\n","                \"max_f1_micro\": [max(overall_result[dataset]['f1_micro_scores'])],\n","                \"std_f1_micro\": [np.std(overall_result[dataset]['f1_micro_scores'])],\n","\n","                \"mean_mcc\": [np.mean(overall_result[dataset]['mcc_scores'])],\n","                \"std_mcc\": [np.std(overall_result[dataset]['mcc_scores'])],\n","                \"min_mcc\": [min(overall_result[dataset]['mcc_scores'])],\n","                \"max_mcc\": [max(overall_result[dataset]['mcc_scores'])],\n","\n","                \"config\": [str(config)]\n","                \n","        })\n","\n","        full_split_path = os.path.join(helper.results_dir, f'Feature_based/exp_24_08_2024/{dataset}.pkl')\n","        if os.path.exists(full_split_path):\n","            df_existing_full = pd.read_pickle(full_split_path)\n","            df_data_spike_full_split = pd.concat([df_existing_full, new_data_spike_exec_full_split_dict], ignore_index=True, axis=0)\n","        else:\n","            print(f\"No existing full split data found at {full_split_path}, creating new file.\")\n","            df_data_spike_full_split = new_data_spike_exec_full_split_dict\n","        \n","        # Save the updated full split data to file\n","        df_data_spike_full_split.to_pickle(full_split_path)\n","\n","\n","        # Save the 1 split data using the full path\n","        split_path = os.path.join(helper.results_dir, f'Feature_based/exp_24_08_2024/{dataset}_single_split.pkl')\n","        if os.path.exists(split_path):\n","            df_existing_1_spike = pd.read_pickle(split_path)\n","            df_data_spike_1_split[dataset] = pd.concat([df_data_spike_1_split[dataset], df_existing_1_spike], ignore_index=True, axis=0)\n","        else:\n","            print(f\"No existing 1 split data found at {split_path}, creating new file\")\n","\n","        # Save the updated 1 split data to file\n","        df_data_spike_1_split[dataset].to_pickle(split_path)\n","\n","    if np.min(accuracy_scores) == 0:\n","        print(f\"Zero accuracy detected in config: {config}\")\n","        print(f\"Accuracy scores: {accuracy_scores}\")\n","\n","    #session.report({\n","    #    \"mean_accuracy_a12\": np.mean(overall_result['a12']['accuracy_scores']),\n","    #    \"min_mean_accuracy\": np.min(accuracy_scores),\n","    #    \"max_mean_accuracy\": np.max(accuracy_scores),\n","    #    \"mean_mean_accuracy\": np.mean(accuracy_scores),\n","    #    \"mean_mean_f1_macro\": np.mean(f1_macro_scores),\n","    #    \"mean_mean_f1_micro\": np.mean(f1_micro_scores),\n","    #    \"mean_mean_mcc\": np.mean(mcc_scores)\n","    #})"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def generate_hidden_layers_config(min_layers=1, max_layers=5, min_nodes=10, max_nodes=50, step=10):\n","    possible_layers = []\n","\n","    for num_layers in range(min_layers, max_layers + 1):\n","\n","        shared_layers = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_layers))\n","\n","        max_output_layers = max_layers - num_layers\n","        \n","        output_layers = []\n","        for num_output_layers in range(0, max_output_layers + 1):\n","\n","            output_layers = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_output_layers))\n","\n","        possible_layers.append({'shared': shared_layers, 'output': {'a2': output_layers, 'a3': output_layers, 'a4': output_layers, 'a12': output_layers, 'a21': output_layers}})\n","\n","    return possible_layers"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def five_fold_cross_validation(num_layers_shared, num_layers_individual, weight_a12, cnn=False, normalized=False):\n","\n","    global best_params_list_getting\n","\n","    #shared_hidden_layers_options = list(itertools.product(range(10, 50 + 1, 20), repeat=num_layers_shared))\n","    #individual_hidden_layers_options = list(itertools.product(range(10, 50 + 1, 20), repeat=num_layers_individual))\n","\n","    shared_hidden_layers_options = [30 for i in range(0, num_layers_shared)]\n","    #individual_hidden_layers_options = [10 for i in range(0, num_layers_individual)]\n","    individual_hidden_layers_options = [10,5]\n","    #config = {\n","    #    \"activation\": tune.choice([\"relu\"]),\n","    #    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n","    #    \"batch_size\": tune.choice([32, 64, 128]),\n","    #    \"shared_hidden_layers\": tune.choice(shared_hidden_layers_options),\n","    #    \"a2_output_hidden_layers\": tune.choice(individual_hidden_layers_options),\n","    #    \"a3_output_hidden_layers\": tune.choice(individual_hidden_layers_options),\n","    #    \"a4_output_hidden_layers\": tune.choice(individual_hidden_layers_options),\n","    #    \"a12_output_hidden_layers\": tune.choice(individual_hidden_layers_options),\n","    #    \"a21_output_hidden_layers\": tune.choice(individual_hidden_layers_options),\n","    #    \"epochs\": tune.choice([40, 50, 60]),\n","    #    \"dropout_rate\": tune.uniform(0.2, 0.5)\n","    #}\n","\n","\n","# {'activation': 'relu', 'learning_rate': 0.0010403297388100502, 'batch_size': 32, 'shared_hidden_layers': (40,), 'a2_output_hidden_layers': (50, 30, 10, 10), 'a3_output_hidden_layers': (20, 30, 10, 20), \n","# #'a4_output_hidden_layers': (40, 30, 40, 20), 'a12_output_hidden_layers': (40, 50, 30, 50), 'a21_output_hidden_layers': (20, 20, 20, 20), 'epochs': 50, 'dropout_rate': 0.3364741105922103} with a mean accuracy of 0.6904200512645706\n","\n","    if cnn:\n","        config = {\n","            \"activation\": \"tanh\",\n","            \"learning_rate\": 0.0003717867434919446,\n","            \"batch_size\": 32,\n","            \"shared_hidden_layers\": [40,30,30],\n","            \"a2_output_hidden_layers\": [30, 10, 20, 10],\n","            \"a3_output_hidden_layers\": [40, 10, 50, 20],\n","            \"a4_output_hidden_layers\": [10, 40, 30, 20],\n","            \"a12_output_hidden_layers\": [50, 30, 50, 30],\n","            \"a21_output_hidden_layers\": [30, 50, 30, 20],\n","            \"epochs\": 50,\n","            \"dropout_rate\": 0.46133106454695794,\n","            \"a12_loss_weighting\": weight_a12\n","        }\n","    else:\n","        config = {\n","                \"activation\": \"relu\",\n","                \"learning_rate\": 0.0005,\n","                \"batch_size\": 128,\n","                \"shared_hidden_layers\": [40,30,30],\n","                \"a2_output_hidden_layers\": [30,10],\n","                \"a3_output_hidden_layers\": [30,10],\n","                \"a4_output_hidden_layers\": [30,10],\n","                \"a12_output_hidden_layers\": [30,30],\n","                \"a21_output_hidden_layers\": [30,30],\n","                \"epochs\": 60,\n","                \"dropout_rate\": 0.25,\n","                \"a12_loss_weighting\": weight_a12\n","        }\n","\n","    train_and_evaluate(config, CNN=cnn, normalized=normalized)\n","    \n","    #scheduler = ASHAScheduler(\n","    #    metric=\"mean_accuracy_a12\",\n","    #    mode=\"max\",\n","    #    max_t=10,\n","    #    grace_period=1,\n","    #    reduction_factor=2\n","    #)\n","    \n","    #search_alg = OptunaSearch(metric=\"mean_accuracy_a12\", mode=\"max\")\n","    \n","    #analysis = tune.run(\n","    #    tune.with_parameters(train_and_evaluate),\n","    #    resources_per_trial={\"cpu\": 12, \"gpu\": 1, \"accelerator_type:RTX\": 1},\n","    #    config=config,\n","    #    scheduler=scheduler,\n","    #    search_alg=search_alg,\n","    #    num_samples=1,\n","    #    verbose=1,\n","    #    storage_path=helper.ray_results_dir,\n","    #    trial_dirname_creator=custom_trial_dirname\n","    #)\n","\n","    #best_config_data_ray_tune = analysis.get_best_config(metric=\"mean_accuracy_a12\", mode=\"max\")\n","    #print(\"Best hyperparameters found were: \", best_config_data_ray_tune)\n","    #best_params_list_getting.append(best_config_data_ray_tune)\n","    \n","    #return analysis"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cnn\n","1\n","2\n","4\n","8\n","16\n","32\n","fnn\n","1\n","2\n","4\n","8\n","16\n","32\n"]}],"source":["print(\"cnn\")\n","for i in [1, 2, 4, 8, 16, 32]:\n","    print(i)\n","    analysis = five_fold_cross_validation(0, 0, i, False, False)\n","    analysis = five_fold_cross_validation(0, 0, i, False, True)\n","\n","print(\"fnn\")\n","for i in [1, 2, 4, 8, 16, 32]:\n","    print(i)\n","    analysis = five_fold_cross_validation(0, 0, i, True, False)\n","    analysis = five_fold_cross_validation(0, 0, i, True, True)\n","#for i in range(0,6):\n","#    print(i)\n","#    for j in range(0,6):\n","#        analysis = five_fold_cross_validation(i, j)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df_dict_A2 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A2.pkl\")\n","df_dict_A3 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A3.pkl\")\n","df_dict_A4 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A4.pkl\")\n","df_dict_A12 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A12.pkl\")\n","df_dict_A21 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A21.pkl\")\n","\n","df_dict = pd.concat([df_dict_A2, df_dict_A3, df_dict_A4, df_dict_A12, df_dict_A21])\n","\n","df_dict = df_dict.loc[:,~df_dict.columns.duplicated()].copy()\n","df_dict = df_dict.drop_duplicates()\n","\n","df_dict.to_csv('outputs/Feature_based/exp_24_08_2024/_one_split_metrics_system.csv')"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T11:21:36.163810Z","iopub.status.busy":"2024-07-10T11:21:36.161935Z","iopub.status.idle":"2024-07-10T11:21:36.173402Z","shell.execute_reply":"2024-07-10T11:21:36.172464Z","shell.execute_reply.started":"2024-07-10T11:21:36.163737Z"},"trusted":true},"outputs":[],"source":["df_dict_full_A2 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A2.pkl\")\n","df_dict_full_A3 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A3.pkl\")\n","df_dict_full_A4 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A4.pkl\")\n","df_dict_full_A12 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A12.pkl\")\n","df_dict_full_A21 = pd.read_pickle(\"results/Feature_based/exp_24_08_2024/A21.pkl\")\n","\n","# remove any columns with \"config!\" in the name\n","df_dict_A3 = df_dict_full_A3.loc[:,~df_dict_full_A3.columns.str.contains('config')].copy()\n","df_dict_A4 = df_dict_full_A4.loc[:,~df_dict_full_A4.columns.str.contains('config')].copy()\n","df_dict_A12 = df_dict_full_A12.loc[:,~df_dict_full_A12.columns.str.contains('config')].copy()\n","df_dict_A21 = df_dict_full_A21.loc[:,~df_dict_full_A21.columns.str.contains('config')].copy()\n","\n","# for a2 add the suffix '_a2' to all following columns: mean_accuracy\tstd_accuracy\tmin_accuracy\tmax_accuracy\tmean_f1_macro\tmin_f1_macro\tmax_f1_macro\tstd_f1_macro\tmean_f1_micro\tmin_f1_micro\tmax_f1_micro\tstd_f1_micro\tmean_mcc\tstd_mcc\tmin_mcc\tmax_mcc\n","#df_dict_A2.columns = [str(col) + '_a2' for col in ['mean_accuracy', 'std_accuracy', 'min_accuracy', 'max_accuracy', 'mean_f1_macro', 'min_f1_macro', 'max_f1_macro', 'std_f1_macro', 'mean_f1_micro', 'min_f1_micro', 'max_f1_micro', 'std_f1_micro', 'mean_mcc', 'std_mcc', 'min_mcc', 'max_mcc']]\n","\n","#df_full_dict = df_dict_A2.join(df_dict_A3.set_index('name'), on='name', rsuffix='_a3')\n","#df_full_dict = df_full_dict.join(df_dict_A4.set_index('name'), on='name', rsuffix='_a4')\n","#df_full_dict = df_full_dict.join(df_dict_A12.set_index('name'), on='name', rsuffix='_a12')\n","#df_full_dict = df_full_dict.join(df_dict_A21.set_index('name'), on='name', rsuffix='_a21')\n","\n","df_full_dict = df_dict_A2.join(df_dict_A3, rsuffix='_a3')\n","df_full_dict = df_full_dict.join(df_dict_A4, rsuffix='_a4')\n","df_full_dict = df_full_dict.join(df_dict_A12, rsuffix='_a12')\n","df_full_dict = df_full_dict.join(df_dict_A21, rsuffix='_a21')\n","\n","# calculate the mean mean accuracy \n","df_full_dict['mean_mean_accuracy'] = (df_full_dict['mean_accuracy'] + df_full_dict['mean_accuracy_a3'] + df_full_dict['mean_accuracy_a4'] + df_full_dict['mean_accuracy_a12'] + df_full_dict['mean_accuracy_a21']) / 5\n","\n","\n","df_full_dict = df_full_dict.loc[:, ~df_full_dict.columns.duplicated()].copy()\n","df_full_dict = df_full_dict.drop_duplicates()\n","#df_full_dict[\"loss_criterion\"] = \"if dataset == 'a12': losses_per_epoch[dataset] += loss * 32 else: losses_per_epoch[dataset] += loss for dataset in helper.dataset_list: losses_per_epoch[dataset] /= len(X_train[dataset]) loss = sum(losses_per_epoch.values())\"\n","\n","df_full_dict.to_csv(\"outputs/Feature_based/exp_24_08_2024/full_split.csv\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["df_dict_full_A2.to_csv(\"outputs/Feature_based/exp_24_08_2024/A2.csv\")\n","df_dict_full_A3.to_csv(\"outputs/Feature_based/exp_24_08_2024/A3.csv\")\n","df_dict_full_A4.to_csv(\"outputs/Feature_based/exp_24_08_2024/A4.csv\")\n","df_dict_full_A12.to_csv(\"outputs/Feature_based/exp_24_08_2024/A12.csv\")\n","df_dict_full_A21.to_csv(\"outputs/Feature_based/exp_24_08_2024/A21.csv\")\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The best config is {'activation': 'tanh', 'learning_rate': 0.0003717867434919446, 'batch_size': 32, 'shared_hidden_layers': [40, 30, 30], 'a2_output_hidden_layers': [30, 10, 20, 10], 'a3_output_hidden_layers': [40, 10, 50, 20], 'a4_output_hidden_layers': [10, 40, 30, 20], 'a12_output_hidden_layers': [50, 30, 50, 30], 'a21_output_hidden_layers': [30, 50, 30, 20], 'epochs': 50, 'dropout_rate': 0.46133106454695794, 'a12_loss_weighting': 32} with a mean accuracy for a12 of 0.1811771163861777\n","a2\n","a3\n","a4\n","a12\n","a21\n","    mean_accuracy_a21  std_accuracy_a21  mean_f1_macro_a21  std_f1_macro_a21  \\\n","63           0.731932          0.034853           0.694052          0.042490   \n","64           0.653962          0.041777           0.590549          0.047475   \n","\n","    mean_f1_micro_a21  std_f1_micro_a21  mean_mcc_a21  std_mcc_a21  \n","63           0.731932          0.034853      0.431967     0.059913  \n","64           0.653962          0.041777      0.274015     0.069578  \n"]}],"source":["# print the configs and results for the hyperparemters wiht the highest mean accuracy\n","# read df_full_dict and print columns with configs in best_params_list_getting\n","df_full_dict = pd.read_csv(\"outputs/Feature_based/exp_24_08_2024/full_split.csv\")\n","\n","config_acc_dict = {}\n","#for good_param in best_params_list_getting:\n","#    # get the row witht the highest mean mean accuracy\n","#    mean_accuracy_a12 = df_full_dict.loc[df_full_dict['config'] == str(good_param)]['mean_accuracy_a12']\n","#    config_acc_dict[str(good_param)] = mean_accuracy_a12\n","#    print(f\"Mean accuracy for a12 config {good_param} is {mean_accuracy_a12}\")\n","\n","# get the best config\n","# TODO get config where a12 has the highest mean accuracy\n","\n","for i in range(0, len(df_full_dict)):\n","    mean_accuracy_a12 = df_full_dict.loc[i]['mean_accuracy_a12']\n","    config_acc_dict[df_full_dict.loc[i]['config']] = mean_accuracy_a12\n","\n","best_config = max(config_acc_dict, key=config_acc_dict.get)\n","print(f\"The best config is {best_config} with a mean accuracy for a12 of {config_acc_dict[best_config]}\")\n","\n","#print aggregate results for the best config\n","print(\"a2\")\n","metrics = ['mean_accuracy', 'std_accuracy', 'mean_f1_macro', 'std_f1_macro', 'mean_f1_micro', 'std_f1_micro', 'mean_mcc', 'std_mcc']\n","print(\"a3\")\n","metrics = ['mean_accuracy_a3', 'std_accuracy_a3', 'mean_f1_macro_a3', 'std_f1_macro_a3', 'mean_f1_micro_a3', 'std_f1_micro_a3', 'mean_mcc_a3', 'std_mcc_a3']\n","print(\"a4\")\n","metrics = [ 'mean_accuracy_a4', 'std_accuracy_a4', 'mean_f1_macro_a4', 'std_f1_macro_a4', 'mean_f1_micro_a4', 'std_f1_micro_a4', 'mean_mcc_a4', 'std_mcc_a4']\n","print(\"a12\")\n","metrics = ['mean_accuracy_a12', 'std_accuracy_a12', 'mean_f1_macro_a12', 'std_f1_macro_a12', 'mean_f1_micro_a12', 'std_f1_micro_a12', 'mean_mcc_a12', 'std_mcc_a12']\n","print(\"a21\")\n","metrics = ['mean_accuracy_a21', 'std_accuracy_a21', 'mean_f1_macro_a21', 'std_f1_macro_a21', 'mean_f1_micro_a21', 'std_f1_micro_a21', 'mean_mcc_a21', 'std_mcc_a21']\n","print(df_full_dict[metrics].loc[df_full_dict['config'] == best_config])\n","    \n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5354302,"sourceId":8905505,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
