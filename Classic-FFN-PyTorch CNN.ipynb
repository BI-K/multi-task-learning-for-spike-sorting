{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Prep"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:03.334868Z","iopub.status.busy":"2024-07-10T10:30:03.334549Z","iopub.status.idle":"2024-07-10T10:30:13.163038Z","shell.execute_reply":"2024-07-10T10:30:13.161739Z","shell.execute_reply.started":"2024-07-10T10:30:03.334833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-11 15:25:23,271\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"]},{"name":"stdout","output_type":"stream","text":["Using device: mps\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import f1_score, matthews_corrcoef\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import psutil\n","import time\n","import logging\n","from datetime import datetime\n","import random\n","import os\n","import shutil\n","import itertools\n","\n","import ray\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.air import session\n","from ray.tune.integration.keras import TuneReportCallback\n","from ray.tune.search.optuna import OptunaSearch\n","from ray.tune.tuner import Tuner, TuneConfig\n","from ray.train import RunConfig\n","from ray.tune import Trainable\n","\n","import helper\n","\n","# Initialize Ray\n","ray.init(ignore_reinit_error=True)\n","\n","# Check if MPS (Apple Silicon) or CUDA (Nvidia) GPU is available\n","if torch.backends.mps.is_available():\n","    device = torch.device('mps')\n","elif torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Existing Data Files Check "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def clear_folder(folder_path):\n","    for item in os.listdir(folder_path):\n","        item_path = os.path.join(folder_path, item)\n","        if item == '.gitignore':\n","            continue\n","        if os.path.isfile(item_path):\n","            os.remove(item_path)\n","        elif os.path.isdir(item_path):\n","            shutil.rmtree(item_path)\n","\n","clear_folder(\"ray_results\")\n","clear_folder(\"outputs\")\n","clear_folder(\"results\")"]},{"cell_type":"markdown","metadata":{},"source":["## Simple FNN"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class Classic_FFN(nn.Module):\n","    def __init__(self, num_classes, hidden_layers=[30, 15], dropout_rate=0.5, activation='relu'):\n","        super(Classic_FFN, self).__init__()\n","\n","        self.activation = activation\n","\n","        self.shared_layers = nn.ModuleList()\n","        self.conv_layers = nn.ModuleList()\n","\n","        self.conv_layers.append(nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1))\n","        self.conv_layers.append(nn.BatchNorm1d(32))\n","        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n","        self.conv_layers.append(nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1))\n","        self.conv_layers.append(nn.BatchNorm1d(64))\n","        self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n","\n","        input_dim = 64 * (30 // 4)\n","\n","        for hidden_layer in hidden_layers:\n","            self.shared_layers.append(nn.Linear(input_dim, hidden_layer))\n","            self.shared_layers.append(nn.Dropout(dropout_rate))\n","            input_dim = hidden_layer\n","\n","        self.output_layer = nn.Linear(input_dim, num_classes)\n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        x = x.unsqueeze(1) # Add channel dimension\n","        for conv in self.conv_layers:\n","            if isinstance(conv, nn.Conv1d):\n","                x = conv(x)\n","                x = self.apply_activation(x)\n","            elif isinstance(conv, nn.BatchNorm1d):\n","                x = conv(x)\n","            elif isinstance(conv, nn.MaxPool1d):\n","                x = conv(x)\n","\n","        x = x.view(x.size(0), -1) # Flatten the output for the fully connected layers\n","\n","        for layer in self.shared_layers:\n","            if isinstance(layer, nn.Linear):\n","                x = layer(x)\n","                x = self.apply_activation(x)\n","            elif isinstance(layer, nn.Dropout):\n","                x = layer(x)\n","            \n","        x = self.output_layer(x)\n","        return x\n","\n","    def apply_activation(self, x):\n","        if self.activation == 'relu':\n","            return F.relu(x)\n","        elif self.activation == 'tanh':\n","            return torch.tanh(x)\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","        else:\n","            raise ValueError(f'Invalid activation: {self.activation}')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_model(num_classes, hidden_layers=[30,15], dropout_rate=0.5, activation='relu'):\n","    model = Classic_FFN(num_classes=num_classes, hidden_layers=hidden_layers, dropout_rate=dropout_rate, activation=activation)\n","    model.to(device)  # Move the model to the selected device\n","    return model\n","\n","def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n","    # Convert data to PyTorch tensors and move to the selected device\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long).to(device)  # Convert one-hot to class indices\n","\n","    # Create a DataLoader for the training data\n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    \n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    \n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        for X_batch, y_batch in train_loader:\n","            optimizer.zero_grad()  # Clear the gradients\n","            outputs = model(X_batch)  # Forward pass\n","            loss = criterion(outputs, y_batch)  # Calculate loss\n","            loss.backward()  # Backward pass\n","            optimizer.step()  # Update weights\n","    \n","    return model\n","\n","def evaluate(model, X_test, y_test):\n","    # Convert data to PyTorch tensors and move to the selected device\n","    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n","    \n","    # Set model to evaluation mode\n","    model.eval()\n","    \n","    # Disable gradient calculation for inference\n","    with torch.no_grad():\n","        outputs = model(X_test_tensor)  # Forward pass\n","        y_pred_classes = torch.argmax(outputs, dim=1).cpu().numpy()  # Get predicted classes and move to CPU\n","        y_test_classes = torch.argmax(y_test_tensor, dim=1).cpu().numpy()  # Get true classes and move to CPU\n","        \n","        # Calculate accuracy\n","        accuracy = (y_pred_classes == y_test_classes).mean()\n","        \n","        # Calculate F1 scores and MCC\n","        f1_macro = f1_score(y_test_classes, y_pred_classes, average='macro')\n","        f1_micro = f1_score(y_test_classes, y_pred_classes, average='micro')\n","        mcc = matthews_corrcoef(y_test_classes, y_pred_classes)\n","    \n","    return accuracy, f1_macro, f1_micro, mcc"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def evaluate_model_on_dataset_one_split(config, split, split_index):\n","    test_set = split[split_index]\n","    indices = [0, 1, 2, 3, 4]\n","    indices.remove(split_index)\n","    train_splits = [split[i] for i in indices]\n","    train_set = pd.concat(train_splits, axis=0)\n","\n","    classes = train_set['track'].unique()\n","    num_classes = len(classes)\n","    #print(\"number of classes: \" + str(num_classes))\n","\n","    one_hot_columns = train_set['track'].unique()\n","    one_hot = pd.get_dummies(train_set['track'])\n","    train_set = train_set.drop('track', axis=1)\n","    train_set = train_set.join(one_hot).astype(float)\n","\n","    one_hot_columns = test_set['track'].unique()\n","    one_hot = pd.get_dummies(test_set['track'])\n","    test_set = test_set.drop('track', axis=1)\n","    test_set = test_set.join(one_hot).astype(float)\n","\n","    X_train = train_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_train = train_set[one_hot_columns].values.reshape(-1, num_classes)\n","    X_test = test_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_test = test_set[one_hot_columns].values.reshape(-1, num_classes)\n","\n","    model = create_model(num_classes, activation=config['activation'], hidden_layers=config['hidden_layers'], dropout_rate=config['dropout_rate'])\n","\n","    # Train the model and collect performance data\n","    model= train(model=model, X_train=X_train, y_train=y_train, num_epochs=config['epochs'], batch_size=config['batch_size'], learning_rate=config['learning_rate'])\n","    # Evaluate the model and collect performance data\n","    accuracy, f1_macro, f1_micro, mcc = evaluate(model=model, X_test=X_test, y_test=y_test)\n","\n","\n","    return accuracy, f1_macro, f1_micro, mcc, num_classes"]},{"cell_type":"markdown","metadata":{},"source":["## 5-Fold-Cross Validation"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df_data_spike_1_split = pd.DataFrame()\n","df_data_spike_full_split = pd.DataFrame()\n","\n","data_spike_exec_1_split_dict = dict()\n","data_spike_exec_full_split_dict = dict()\n","\n","\n","best_params_list_getting = []\n","\n","def custom_trial_dirname(trial):\n","    return f\"trial_{trial.trial_id}\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def train_and_evaluate(config, splits, splits_name):\n","    \n","    global data_spike_exec_1_split_dict\n","    global data_spike_exec_full_split_dict\n","    global df_data_spike_1_split\n","    global results_dir\n","    \n","    min_accuracy = 1\n","    max_accuracy = 0\n","    avg_accuracy = 0\n","    min_f1_macro = 1\n","    max_f1_macro = 0\n","    avg_f1_macro = 0\n","    min_f1_micro = 1\n","    max_f1_micro = 0\n","    avg_f1_micro = 0\n","    min_mcc = 1\n","    max_mcc = -1\n","    avg_mcc = 0\n","\n","    num_classes = 0\n","    \n","    accuracies = []\n","    f1_macro_scores = []\n","    f1_micro_scores = []\n","    mcc_scores = []\n","\n","    session_id_for_df = session.get_trial_id()\n","    \n","    for i in range(5):\n","        accuracy, f1_macro, f1_micro, mcc, num_classes = evaluate_model_on_dataset_one_split(config, splits, i)\n","        accuracies.append(accuracy)\n","        f1_macro_scores.append(f1_macro)\n","        f1_micro_scores.append(f1_micro)\n","        mcc_scores.append(mcc)\n","        \n","        avg_accuracy += accuracy\n","        avg_f1_macro += f1_macro\n","        avg_f1_micro += f1_micro\n","        min_accuracy = min(min_accuracy, accuracy)\n","        max_accuracy = max(max_accuracy, accuracy)\n","        min_f1_macro = min(min_f1_macro, f1_macro)\n","        max_f1_macro = max(max_f1_macro, f1_macro)\n","        min_f1_micro = min(min_f1_micro, f1_micro)\n","        max_f1_micro = max(max_f1_micro, f1_micro)\n","        avg_mcc += mcc\n","        min_mcc = min(min_mcc, mcc)\n","        max_mcc = max(max_mcc, mcc)\n","\n","\n","        \n","        data_spike_exec_1_split_dict[splits_name + \"_\" + str(i+1) + \"_\" + session_id_for_df] = {\n","            \"number of classes\": num_classes,\n","            \"accuracy\": accuracy,\n","            \"macro f1\": f1_macro,\n","            \"micro_f1\": f1_micro,\n","            \"mcc\": mcc,\n","            \"config\": str(config)\n","        }\n","        \n","       \n","    temp_df = pd.DataFrame.from_dict(data_spike_exec_1_split_dict, orient='index')\n","    df_data_spike_1_split = pd.concat([df_data_spike_1_split, temp_df], axis=0)\n","\n","        # df_data_spike_1_split = pd.concat([df_data_spike_1_split, pd.DataFrame.from_dict(data_spike_exec_1_split_dict)], axis=1)\n","\n","        \n","    avg_accuracy /= 5\n","    avg_f1_macro /= 5\n","    avg_f1_micro /= 5\n","    avg_mcc /= 5\n","    \n","    data_spike_exec_full_split_dict[splits_name + \"_\" + session_id_for_df] = {\n","        \"number of classes\": num_classes,\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"mean_f1_micro\": np.mean(f1_micro_scores),\n","        \"min_mcc\": min_mcc,\n","        \"max_mcc\": max_mcc,\n","        \"mean_mcc\": np.mean(mcc_scores),\n","        \"std_accuracy\": np.std(accuracies),\n","        \"std_f1_macro\": np.std(f1_macro_scores),\n","        \"std_f1_micro\": np.std(f1_micro_scores),\n","        \"std_mcc\": np.std(mcc_scores),\n","        \"config\": str(config)\n","    }\n","\n","    # print(f\"Mean accuracy of the model {splits_name + '_' + session_id_for_df}: {avg_accuracy}\")\n","\n","    df_data_spike_full_split = pd.DataFrame.from_dict(data_spike_exec_full_split_dict, orient='index')\n","\n","    # Load existing data from file if it exists and append new data\n","    full_split_path = os.path.join(helper.results_dir, f'output_full_{splits_name}.pkl')\n","    if os.path.exists(full_split_path):\n","        df_existing_full = pd.read_pickle(full_split_path)\n","        df_data_spike_full_split = pd.concat([df_existing_full, df_data_spike_full_split], axis=0)\n","    else:\n","        print(f\"No existing full split data found at {full_split_path}, creating new file.\")\n","    \n","    # Save the updated full split data to file\n","    df_data_spike_full_split.to_pickle(full_split_path)\n","\n","    # Save the 1 split data using the full path\n","    split_path = os.path.join(helper.results_dir, f'output_{splits_name}.pkl')\n","    if os.path.exists(split_path):\n","        df_existing_1_spike = pd.read_pickle(split_path)\n","        df_data_spike_1_split = pd.concat([df_existing_1_spike, df_data_spike_1_split], axis=0)\n","    else:\n","        print(f\"No existing 1 split data found at {split_path}, creating new file\")\n","\n","    # Save the updated 1 split data to file\n","    df_data_spike_1_split.to_pickle(split_path)\n","\n","    time.sleep(5)\n","    \n","    session.report({\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"mean_f1_micro\": np.mean(f1_micro_scores)\n","    })"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def generate_hidden_layers_config(min_layers=2, max_layers=5, min_nodes=30, max_nodes=90, step=5):\n","    possible_layers = []\n","    for num_layers in range(min_layers, max_layers + 1):\n","        layer_configurations = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_layers))\n","        possible_layers.extend(layer_configurations)\n","    return possible_layers\n","\n","def five_fold_cross_validation(splits, splits_name):\n","\n","    global best_params_list_getting\n","\n","    hidden_layers_options = generate_hidden_layers_config()\n","\n","    config = {\n","        \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\"]),\n","        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n","        \"batch_size\": tune.choice([32, 64, 128]),\n","        \"hidden_layers\": tune.choice(hidden_layers_options),\n","        \"epochs\": tune.choice([10, 20, 30, 40, 50]),\n","        \"dropout_rate\": tune.uniform(0.2, 0.5)\n","    }\n","    \n","    scheduler = ASHAScheduler(\n","        metric=\"mean_accuracy\",\n","        mode=\"max\",\n","        max_t=10,\n","        grace_period=1,\n","        reduction_factor=2\n","    )\n","    \n","    search_alg = OptunaSearch(metric=\"mean_accuracy\", mode=\"max\")\n","    \n","    analysis = tune.run(\n","        tune.with_parameters(train_and_evaluate, splits=splits, splits_name=splits_name),\n","        resources_per_trial={\"cpu\": 10, \"gpu\": 0, \"accelerator_type:RTX\": 0},\n","        config=config,\n","        scheduler=scheduler,\n","        search_alg=search_alg,\n","        num_samples=32,\n","        verbose=1,\n","        storage_path=helper.ray_results_dir,\n","        trial_dirname_creator=custom_trial_dirname\n","    )\n","\n","    best_config_data_ray_tune = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n","    print(\"Best hyperparameters found were: \", best_config_data_ray_tune)\n","    best_params_list_getting.append(best_config_data_ray_tune)\n","    \n","    return analysis"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-11 15:36:54</td></tr>\n","<tr><td>Running for: </td><td>00:11:16.40        </td></tr>\n","<tr><td>Memory:      </td><td>11.8/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=15<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9245413637937001<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_c8f37cfa</td><td>TERMINATED</td><td>127.0.0.1:25644</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.386146</td><td style=\"text-align: right;\">      20</td><td>(70, 85, 45, 80, 40)</td><td style=\"text-align: right;\">    0.000475089</td><td style=\"text-align: right;\">0.945985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.2631 </td><td style=\"text-align: right;\">      0.898148</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.897718</td></tr>\n","<tr><td>train_and_evaluate_babcb040</td><td>TERMINATED</td><td>127.0.0.1:25700</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.370302</td><td style=\"text-align: right;\">      50</td><td>(30, 50, 55, 80, 60)</td><td style=\"text-align: right;\">    0.00406359 </td><td style=\"text-align: right;\">0.917982</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.4287 </td><td style=\"text-align: right;\">      0.878505</td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.877822</td></tr>\n","<tr><td>train_and_evaluate_88f68813</td><td>TERMINATED</td><td>127.0.0.1:25763</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.265711</td><td style=\"text-align: right;\">      40</td><td>(60, 80, 45, 70, 85)</td><td style=\"text-align: right;\">    0.00128    </td><td style=\"text-align: right;\">0.925459</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.9579 </td><td style=\"text-align: right;\">      0.87963 </td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.879371</td></tr>\n","<tr><td>train_and_evaluate_322ea99c</td><td>TERMINATED</td><td>127.0.0.1:25819</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.356004</td><td style=\"text-align: right;\">      10</td><td>(40, 90, 65, 30, 60)</td><td style=\"text-align: right;\">    0.00557794 </td><td style=\"text-align: right;\">0.66414 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.77181</td><td style=\"text-align: right;\">      0.53271 </td><td style=\"text-align: right;\">      0.775701</td><td style=\"text-align: right;\">      0.408709</td></tr>\n","<tr><td>train_and_evaluate_ac8220f3</td><td>TERMINATED</td><td>127.0.0.1:25853</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.364958</td><td style=\"text-align: right;\">      50</td><td>(85, 70, 80, 70)    </td><td style=\"text-align: right;\">    0.000311364</td><td style=\"text-align: right;\">0.931031</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.0272 </td><td style=\"text-align: right;\">      0.898148</td><td style=\"text-align: right;\">      0.953271</td><td style=\"text-align: right;\">      0.897929</td></tr>\n","<tr><td>train_and_evaluate_e68e1810</td><td>TERMINATED</td><td>127.0.0.1:25943</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.328347</td><td style=\"text-align: right;\">      40</td><td>(70, 85, 65, 90, 50)</td><td style=\"text-align: right;\">    0.000144562</td><td style=\"text-align: right;\">0.919886</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        31.8538 </td><td style=\"text-align: right;\">      0.861111</td><td style=\"text-align: right;\">      0.953271</td><td style=\"text-align: right;\">      0.861099</td></tr>\n","<tr><td>train_and_evaluate_642cfde0</td><td>TERMINATED</td><td>127.0.0.1:26037</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.447546</td><td style=\"text-align: right;\">      10</td><td>(45, 90, 50, 30, 60)</td><td style=\"text-align: right;\">    0.00287442 </td><td style=\"text-align: right;\">0.904881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.8305 </td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.925234</td><td style=\"text-align: right;\">      0.887051</td></tr>\n","<tr><td>train_and_evaluate_4dce1433</td><td>TERMINATED</td><td>127.0.0.1:26088</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.284703</td><td style=\"text-align: right;\">      10</td><td>(30, 35, 30, 35, 75)</td><td style=\"text-align: right;\">    0.00922983 </td><td style=\"text-align: right;\">0.923624</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.62855</td><td style=\"text-align: right;\">      0.861111</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.859069</td></tr>\n","<tr><td>train_and_evaluate_299d4255</td><td>TERMINATED</td><td>127.0.0.1:26127</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.47607 </td><td style=\"text-align: right;\">      40</td><td>(40, 45, 55, 75, 70)</td><td style=\"text-align: right;\">    0.000567754</td><td style=\"text-align: right;\">0.712582</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.1606 </td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.943925</td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_9f0cede7</td><td>TERMINATED</td><td>127.0.0.1:26170</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.357885</td><td style=\"text-align: right;\">      20</td><td>(80, 60, 30, 55)    </td><td style=\"text-align: right;\">    0.000109625</td><td style=\"text-align: right;\">0.819263</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.3076 </td><td style=\"text-align: right;\">      0.694444</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.692308</td></tr>\n","<tr><td>train_and_evaluate_13cbd6db</td><td>TERMINATED</td><td>127.0.0.1:26215</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.462552</td><td style=\"text-align: right;\">      10</td><td>(85, 75, 90, 65, 40)</td><td style=\"text-align: right;\">    0.0004348  </td><td style=\"text-align: right;\">0.845067</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.91883</td><td style=\"text-align: right;\">      0.495327</td><td style=\"text-align: right;\">      0.990654</td><td style=\"text-align: right;\">      0.33125 </td></tr>\n","<tr><td>train_and_evaluate_bfbb7d7f</td><td>TERMINATED</td><td>127.0.0.1:26248</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.210636</td><td style=\"text-align: right;\">      20</td><td>(75, 35, 70, 75, 80)</td><td style=\"text-align: right;\">    0.00131639 </td><td style=\"text-align: right;\">0.92731 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.1637 </td><td style=\"text-align: right;\">      0.888889</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.887928</td></tr>\n","<tr><td>train_and_evaluate_9929bac1</td><td>TERMINATED</td><td>127.0.0.1:26296</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.414659</td><td style=\"text-align: right;\">      20</td><td>(50, 35, 80, 85, 40)</td><td style=\"text-align: right;\">    0.000334167</td><td style=\"text-align: right;\">0.936622</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.0815 </td><td style=\"text-align: right;\">      0.907407</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.906897</td></tr>\n","<tr><td>train_and_evaluate_786a5ccc</td><td>TERMINATED</td><td>127.0.0.1:26353</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.407255</td><td style=\"text-align: right;\">      30</td><td>(30, 35, 45, 30)    </td><td style=\"text-align: right;\">    0.000277417</td><td style=\"text-align: right;\">0.901177</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.3136 </td><td style=\"text-align: right;\">      0.850467</td><td style=\"text-align: right;\">      0.943925</td><td style=\"text-align: right;\">      0.849825</td></tr>\n","<tr><td>train_and_evaluate_e1bfe1bc</td><td>TERMINATED</td><td>127.0.0.1:26421</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.418947</td><td style=\"text-align: right;\">      30</td><td>(70, 40, 90, 40, 70)</td><td style=\"text-align: right;\">    0.000242071</td><td style=\"text-align: right;\">0.942264</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        25.4316 </td><td style=\"text-align: right;\">      0.888889</td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.888545</td></tr>\n","<tr><td>train_and_evaluate_89d676d4</td><td>TERMINATED</td><td>127.0.0.1:26484</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.417958</td><td style=\"text-align: right;\">      20</td><td>(75, 70, 65, 55, 55)</td><td style=\"text-align: right;\">    0.000814077</td><td style=\"text-align: right;\">0.921755</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.0427 </td><td style=\"text-align: right;\">      0.861111</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.859069</td></tr>\n","<tr><td>train_and_evaluate_67c03ce0</td><td>TERMINATED</td><td>127.0.0.1:26554</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.419606</td><td style=\"text-align: right;\">      30</td><td>(85, 55, 80, 75, 40)</td><td style=\"text-align: right;\">    0.000727862</td><td style=\"text-align: right;\">0.932883</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.6575 </td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.89716 </td></tr>\n","<tr><td>train_and_evaluate_527500c4</td><td>TERMINATED</td><td>127.0.0.1:26612</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.494127</td><td style=\"text-align: right;\">      30</td><td>(90, 90, 65, 55, 40)</td><td style=\"text-align: right;\">    0.000181536</td><td style=\"text-align: right;\">0.949706</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.5821 </td><td style=\"text-align: right;\">      0.907407</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.907121</td></tr>\n","<tr><td>train_and_evaluate_88a69fed</td><td>TERMINATED</td><td>127.0.0.1:26664</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.313109</td><td style=\"text-align: right;\">      30</td><td>(40, 55, 90, 65, 50)</td><td style=\"text-align: right;\">    0.000224215</td><td style=\"text-align: right;\">0.931014</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        25.4284 </td><td style=\"text-align: right;\">      0.907407</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.906897</td></tr>\n","<tr><td>train_and_evaluate_117d5c5a</td><td>TERMINATED</td><td>127.0.0.1:26734</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.498274</td><td style=\"text-align: right;\">      30</td><td>(35, 70, 75, 45, 65)</td><td style=\"text-align: right;\">    0.00017419 </td><td style=\"text-align: right;\">0.927328</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.3318 </td><td style=\"text-align: right;\">      0.87963 </td><td style=\"text-align: right;\">      0.953271</td><td style=\"text-align: right;\">      0.878788</td></tr>\n","<tr><td>train_and_evaluate_187d98ec</td><td>TERMINATED</td><td>127.0.0.1:26787</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.498757</td><td style=\"text-align: right;\">      20</td><td>(75, 30, 60, 55, 60)</td><td style=\"text-align: right;\">    0.00016315 </td><td style=\"text-align: right;\">0.916078</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.3169 </td><td style=\"text-align: right;\">      0.88785 </td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.887368</td></tr>\n","<tr><td>train_and_evaluate_ec816bf0</td><td>TERMINATED</td><td>127.0.0.1:26816</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.49402 </td><td style=\"text-align: right;\">      20</td><td>(30, 75, 30, 90, 65)</td><td style=\"text-align: right;\">    0.00233392 </td><td style=\"text-align: right;\">0.908584</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.3573 </td><td style=\"text-align: right;\">      0.82243 </td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.819337</td></tr>\n","<tr><td>train_and_evaluate_f406c975</td><td>TERMINATED</td><td>127.0.0.1:26849</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.384643</td><td style=\"text-align: right;\">      30</td><td>(70, 45, 90, 35, 75)</td><td style=\"text-align: right;\">    0.000101828</td><td style=\"text-align: right;\">0.931031</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.8393 </td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.896618</td></tr>\n","<tr><td>train_and_evaluate_4b995d14</td><td>TERMINATED</td><td>127.0.0.1:26908</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.386227</td><td style=\"text-align: right;\">      30</td><td>(65, 70, 85, 50, 90)</td><td style=\"text-align: right;\">    0.000544902</td><td style=\"text-align: right;\">0.927293</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.3829 </td><td style=\"text-align: right;\">      0.898148</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.897929</td></tr>\n","<tr><td>train_and_evaluate_885ec373</td><td>TERMINATED</td><td>127.0.0.1:26963</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.441134</td><td style=\"text-align: right;\">      30</td><td>(65, 50, 70, 65, 65)</td><td style=\"text-align: right;\">    0.000409497</td><td style=\"text-align: right;\">0.938525</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.1072 </td><td style=\"text-align: right;\">      0.888889</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.888276</td></tr>\n","<tr><td>train_and_evaluate_85a77752</td><td>TERMINATED</td><td>127.0.0.1:27032</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.440971</td><td style=\"text-align: right;\">      30</td><td>(90, 30, 60, 40, 30)</td><td style=\"text-align: right;\">    0.000405591</td><td style=\"text-align: right;\">0.944116</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7448 </td><td style=\"text-align: right;\">      0.898148</td><td style=\"text-align: right;\">      0.981308</td><td style=\"text-align: right;\">      0.897718</td></tr>\n","<tr><td>train_and_evaluate_f403c936</td><td>TERMINATED</td><td>127.0.0.1:27092</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.449843</td><td style=\"text-align: right;\">      30</td><td>(65, 65, 30, 70, 30)</td><td style=\"text-align: right;\">    0.000216785</td><td style=\"text-align: right;\">0.94036 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        26.4155 </td><td style=\"text-align: right;\">      0.907407</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.907376</td></tr>\n","<tr><td>train_and_evaluate_009daa63</td><td>TERMINATED</td><td>127.0.0.1:27165</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.449229</td><td style=\"text-align: right;\">      50</td><td>(80, 70, 55, 60, 30)</td><td style=\"text-align: right;\">    0.000977478</td><td style=\"text-align: right;\">0.919886</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.2717 </td><td style=\"text-align: right;\">      0.861111</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.860813</td></tr>\n","<tr><td>train_and_evaluate_aaf817ab</td><td>TERMINATED</td><td>127.0.0.1:27217</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.471854</td><td style=\"text-align: right;\">      50</td><td>(50, 65, 65, 30, 40)</td><td style=\"text-align: right;\">    0.000940721</td><td style=\"text-align: right;\">0.929162</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.2404 </td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.971963</td><td style=\"text-align: right;\">      0.896618</td></tr>\n","<tr><td>train_and_evaluate_90cf0112</td><td>TERMINATED</td><td>127.0.0.1:27271</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.469025</td><td style=\"text-align: right;\">      30</td><td>(30, 45, 75, 35, 85)</td><td style=\"text-align: right;\">    0.000575634</td><td style=\"text-align: right;\">0.904967</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.4263 </td><td style=\"text-align: right;\">      0.842593</td><td style=\"text-align: right;\">      0.953271</td><td style=\"text-align: right;\">      0.841929</td></tr>\n","<tr><td>train_and_evaluate_4bb4ad6c</td><td>TERMINATED</td><td>127.0.0.1:27323</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.39033 </td><td style=\"text-align: right;\">      30</td><td>(30, 50, 50, 30, 80)</td><td style=\"text-align: right;\">    0.000614586</td><td style=\"text-align: right;\">0.923555</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.4173 </td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.962617</td><td style=\"text-align: right;\">      0.897196</td></tr>\n","<tr><td>train_and_evaluate_281cbaeb</td><td>TERMINATED</td><td>127.0.0.1:27347</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.383624</td><td style=\"text-align: right;\">      20</td><td>(50, 70, 55, 70, 60)</td><td style=\"text-align: right;\">    0.00172039 </td><td style=\"text-align: right;\">0.912305</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.3882 </td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.925234</td><td style=\"text-align: right;\">      0.897196</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=25644)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A2.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=25644)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A2.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-11 15:25:58,854\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 85, 45, 80, 40)}\n","2024-08-11 15:26:26,217\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 50, 55, 80, 60)}\n","2024-08-11 15:26:49,419\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 80, 45, 70, 85)}\n","2024-08-11 15:27:01,469\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 90, 65, 30, 60)}\n","2024-08-11 15:27:41,488\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 70, 80, 70)}\n","2024-08-11 15:28:16,332\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 85, 65, 90, 50)}\n","2024-08-11 15:28:32,371\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 90, 50, 30, 60)}\n","2024-08-11 15:28:45,204\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 35, 30, 35, 75)}\n","2024-08-11 15:29:03,722\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 45, 55, 75, 70)}\n","2024-08-11 15:29:17,949\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 60, 30, 55)}\n","2024-08-11 15:29:31,552\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 90, 65, 40)}\n","2024-08-11 15:29:47,854\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 35, 70, 75, 80)}\n","2024-08-11 15:30:11,102\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 35, 80, 85, 40)}\n","2024-08-11 15:30:38,281\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 35, 45, 30)}\n","2024-08-11 15:31:07,287\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 40, 90, 40, 70)}\n","2024-08-11 15:31:30,139\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 70, 65, 55, 55)}\n","2024-08-11 15:31:50,864\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 55, 80, 75, 40)}\n","2024-08-11 15:32:11,512\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 90, 65, 55, 40)}\n","2024-08-11 15:32:40,380\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 55, 90, 65, 50)}\n","2024-08-11 15:33:01,701\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 70, 75, 45, 65)}\n","2024-08-11 15:33:18,709\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 30, 60, 55, 60)}\n","2024-08-11 15:33:35,937\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 75, 30, 90, 65)}\n","2024-08-11 15:33:56,097\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 45, 90, 35, 75)}\n","2024-08-11 15:34:17,747\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 70, 85, 50, 90)}\n","2024-08-11 15:34:34,551\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 50, 70, 65, 65)}\n","2024-08-11 15:34:52,444\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 30, 60, 40, 30)}\n","2024-08-11 15:35:22,985\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 65, 30, 70, 30)}\n","2024-08-11 15:35:43,865\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 70, 55, 60, 30)}\n","2024-08-11 15:36:04,399\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 65, 65, 30, 40)}\n","2024-08-11 15:36:21,569\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 75, 35, 85)}\n","2024-08-11 15:36:37,822\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 50, 50, 30, 80)}\n","2024-08-11 15:36:54,833\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 70, 55, 70, 60)}\n","2024-08-11 15:36:54,960\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-11_15-25-35' in 0.1248s.\n","2024-08-11 15:36:54,967\tINFO tune.py:1041 -- Total run time: 680.51 seconds (676.27 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.00018153627532969336, 'batch_size': 64, 'hidden_layers': (90, 90, 65, 55, 40), 'epochs': 30, 'dropout_rate': 0.4941268198413076}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a2_splits, \"A2\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-11 15:46:44</td></tr>\n","<tr><td>Running for: </td><td>00:08:40.35        </td></tr>\n","<tr><td>Memory:      </td><td>11.7/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=14<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9003105590062113<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_96966d85</td><td>TERMINATED</td><td>127.0.0.1:27542</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.484764</td><td style=\"text-align: right;\">      30</td><td>(65, 50, 50, 50, 60)</td><td style=\"text-align: right;\">    0.000173104</td><td style=\"text-align: right;\">0.901739</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.9572 </td><td style=\"text-align: right;\">      0.884058</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.883838</td></tr>\n","<tr><td>train_and_evaluate_50f18203</td><td>TERMINATED</td><td>127.0.0.1:27613</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.314612</td><td style=\"text-align: right;\">      50</td><td>(75, 75, 85, 90, 80)</td><td style=\"text-align: right;\">    0.000432857</td><td style=\"text-align: right;\">0.869896</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.4819 </td><td style=\"text-align: right;\">      0.855072</td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.854798</td></tr>\n","<tr><td>train_and_evaluate_6b11b74e</td><td>TERMINATED</td><td>127.0.0.1:27656</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.284729</td><td style=\"text-align: right;\">      30</td><td>(85, 75, 35, 35, 50)</td><td style=\"text-align: right;\">    0.000238421</td><td style=\"text-align: right;\">0.497101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.4896 </td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_b42d5367</td><td>TERMINATED</td><td>127.0.0.1:27682</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.309728</td><td style=\"text-align: right;\">      50</td><td>(70, 90, 55, 35)    </td><td style=\"text-align: right;\">    0.00181374 </td><td style=\"text-align: right;\">0.893126</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        24.099  </td><td style=\"text-align: right;\">      0.869565</td><td style=\"text-align: right;\">      0.913043</td><td style=\"text-align: right;\">      0.869456</td></tr>\n","<tr><td>train_and_evaluate_538b6a51</td><td>TERMINATED</td><td>127.0.0.1:27729</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.268505</td><td style=\"text-align: right;\">      50</td><td>(70, 90, 65, 60, 45)</td><td style=\"text-align: right;\">    0.000474446</td><td style=\"text-align: right;\">0.866874</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        26.1983 </td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.928571</td><td style=\"text-align: right;\">      0.810159</td></tr>\n","<tr><td>train_and_evaluate_f4918c0c</td><td>TERMINATED</td><td>127.0.0.1:27776</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.464299</td><td style=\"text-align: right;\">      20</td><td>(55, 55, 85, 45, 60)</td><td style=\"text-align: right;\">    0.00011015 </td><td style=\"text-align: right;\">0.656066</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.43999</td><td style=\"text-align: right;\">      0.57971 </td><td style=\"text-align: right;\">      0.724638</td><td style=\"text-align: right;\">      0.521864</td></tr>\n","<tr><td>train_and_evaluate_9d1159ce</td><td>TERMINATED</td><td>127.0.0.1:27801</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.379875</td><td style=\"text-align: right;\">      30</td><td>(45, 85, 55, 90, 50)</td><td style=\"text-align: right;\">    0.000791936</td><td style=\"text-align: right;\">0.91619 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.6449 </td><td style=\"text-align: right;\">      0.913043</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.912584</td></tr>\n","<tr><td>train_and_evaluate_c502041b</td><td>TERMINATED</td><td>127.0.0.1:27829</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.237654</td><td style=\"text-align: right;\">      30</td><td>(55, 60, 55, 65, 30)</td><td style=\"text-align: right;\">    0.000781158</td><td style=\"text-align: right;\">0.701822</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.1784 </td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.871429</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_fb92d106</td><td>TERMINATED</td><td>127.0.0.1:27878</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.346008</td><td style=\"text-align: right;\">      10</td><td>(75, 75, 35, 85, 50)</td><td style=\"text-align: right;\">    0.00217906 </td><td style=\"text-align: right;\">0.904638</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.4611 </td><td style=\"text-align: right;\">      0.884058</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.884034</td></tr>\n","<tr><td>train_and_evaluate_4143a349</td><td>TERMINATED</td><td>127.0.0.1:27927</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.386488</td><td style=\"text-align: right;\">      40</td><td>(50, 35, 65, 80, 90)</td><td style=\"text-align: right;\">    0.000144336</td><td style=\"text-align: right;\">0.5     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.7044 </td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_c11a193f</td><td>TERMINATED</td><td>127.0.0.1:27965</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.310794</td><td style=\"text-align: right;\">      20</td><td>(85, 65, 55, 35, 65)</td><td style=\"text-align: right;\">    0.00160425 </td><td style=\"text-align: right;\">0.904638</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.1772 </td><td style=\"text-align: right;\">      0.884058</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.883838</td></tr>\n","<tr><td>train_and_evaluate_fcce14ab</td><td>TERMINATED</td><td>127.0.0.1:27996</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.419838</td><td style=\"text-align: right;\">      20</td><td>(50, 90, 65, 50, 45)</td><td style=\"text-align: right;\">    0.00975662 </td><td style=\"text-align: right;\">0.913375</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.1214 </td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.88534 </td></tr>\n","<tr><td>train_and_evaluate_083204a0</td><td>TERMINATED</td><td>127.0.0.1:28020</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.382693</td><td style=\"text-align: right;\">      20</td><td>(65, 35, 45, 50, 70)</td><td style=\"text-align: right;\">    0.00639751 </td><td style=\"text-align: right;\">0.898841</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.1363 </td><td style=\"text-align: right;\">      0.869565</td><td style=\"text-align: right;\">      0.913043</td><td style=\"text-align: right;\">      0.869456</td></tr>\n","<tr><td>train_and_evaluate_41a67217</td><td>TERMINATED</td><td>127.0.0.1:28045</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.426033</td><td style=\"text-align: right;\">      20</td><td>(65, 50, 70, 80, 65)</td><td style=\"text-align: right;\">    0.00592495 </td><td style=\"text-align: right;\">0.924845</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.1166 </td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.898551</td></tr>\n","<tr><td>train_and_evaluate_dc24b41b</td><td>TERMINATED</td><td>127.0.0.1:28073</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.426016</td><td style=\"text-align: right;\">      10</td><td>(55, 70, 65, 30, 90)</td><td style=\"text-align: right;\">    0.00899447 </td><td style=\"text-align: right;\">0.913251</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.92547</td><td style=\"text-align: right;\">      0.884058</td><td style=\"text-align: right;\">      0.956522</td><td style=\"text-align: right;\">      0.883446</td></tr>\n","<tr><td>train_and_evaluate_c46f72ad</td><td>TERMINATED</td><td>127.0.0.1:28103</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.435456</td><td style=\"text-align: right;\">      10</td><td>(60, 45, 90, 50, 75)</td><td style=\"text-align: right;\">    0.00365464 </td><td style=\"text-align: right;\">0.919213</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.85737</td><td style=\"text-align: right;\">      0.871429</td><td style=\"text-align: right;\">      0.956522</td><td style=\"text-align: right;\">      0.870769</td></tr>\n","<tr><td>train_and_evaluate_2a8be546</td><td>TERMINATED</td><td>127.0.0.1:28133</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.43167 </td><td style=\"text-align: right;\">      40</td><td>(60, 55, 85, 75, 65)</td><td style=\"text-align: right;\">    0.00336917 </td><td style=\"text-align: right;\">0.907619</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.6742 </td><td style=\"text-align: right;\">      0.871429</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.869267</td></tr>\n","<tr><td>train_and_evaluate_57776127</td><td>TERMINATED</td><td>127.0.0.1:28165</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.442792</td><td style=\"text-align: right;\">      10</td><td>(45, 35, 30, 50, 40)</td><td style=\"text-align: right;\">    0.00352888 </td><td style=\"text-align: right;\">0.898882</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.83147</td><td style=\"text-align: right;\">      0.884058</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.884034</td></tr>\n","<tr><td>train_and_evaluate_d77ce84b</td><td>TERMINATED</td><td>127.0.0.1:28186</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.462285</td><td style=\"text-align: right;\">      10</td><td>(65, 85, 75, 85, 75)</td><td style=\"text-align: right;\">    0.00414297 </td><td style=\"text-align: right;\">0.904679</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.61117</td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.88534 </td></tr>\n","<tr><td>train_and_evaluate_48a26c7f</td><td>TERMINATED</td><td>127.0.0.1:28204</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.498347</td><td style=\"text-align: right;\">      10</td><td>(60, 30, 90, 80, 90)</td><td style=\"text-align: right;\">    0.00486543 </td><td style=\"text-align: right;\">0.52294 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.62428</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.585714</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_a07e0acf</td><td>TERMINATED</td><td>127.0.0.1:28228</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.493348</td><td style=\"text-align: right;\">      20</td><td>(85, 35, 40, 90, 35)</td><td style=\"text-align: right;\">    0.00592818 </td><td style=\"text-align: right;\">0.846915</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.0572 </td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.913043</td><td style=\"text-align: right;\">      0.807652</td></tr>\n","<tr><td>train_and_evaluate_5dfc9803</td><td>TERMINATED</td><td>127.0.0.1:28279</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.403992</td><td style=\"text-align: right;\">      20</td><td>(90, 30, 45, 90, 55)</td><td style=\"text-align: right;\">    0.00269999 </td><td style=\"text-align: right;\">0.881532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.4294 </td><td style=\"text-align: right;\">      0.84058 </td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.838407</td></tr>\n","<tr><td>train_and_evaluate_9eb3ee07</td><td>TERMINATED</td><td>127.0.0.1:28312</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.371859</td><td style=\"text-align: right;\">      30</td><td>(90, 35, 35, 40, 80)</td><td style=\"text-align: right;\">    0.00117316 </td><td style=\"text-align: right;\">0.904638</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5061 </td><td style=\"text-align: right;\">      0.855072</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.853565</td></tr>\n","<tr><td>train_and_evaluate_3b70fa06</td><td>TERMINATED</td><td>127.0.0.1:28340</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.359712</td><td style=\"text-align: right;\">      30</td><td>(45, 90, 50, 30, 35)</td><td style=\"text-align: right;\">    0.00106897 </td><td style=\"text-align: right;\">0.890311</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.4742 </td><td style=\"text-align: right;\">      0.842857</td><td style=\"text-align: right;\">      0.942029</td><td style=\"text-align: right;\">      0.840216</td></tr>\n","<tr><td>train_and_evaluate_8a74eb7b</td><td>TERMINATED</td><td>127.0.0.1:28369</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.347174</td><td style=\"text-align: right;\">      30</td><td>(55, 30, 70, 55, 65)</td><td style=\"text-align: right;\">    0.000757383</td><td style=\"text-align: right;\">0.913333</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.3718 </td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.898209</td></tr>\n","<tr><td>train_and_evaluate_5361d637</td><td>TERMINATED</td><td>127.0.0.1:28396</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.341578</td><td style=\"text-align: right;\">      40</td><td>(55, 50, 85, 35, 70)</td><td style=\"text-align: right;\">    0.000588216</td><td style=\"text-align: right;\">0.907536</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.763  </td><td style=\"text-align: right;\">      0.869565</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.869456</td></tr>\n","<tr><td>train_and_evaluate_0aa0cd4a</td><td>TERMINATED</td><td>127.0.0.1:28426</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.403024</td><td style=\"text-align: right;\">      40</td><td>(55, 90, 90, 70, 85)</td><td style=\"text-align: right;\">    0.000539571</td><td style=\"text-align: right;\">0.907578</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.6277 </td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.884868</td></tr>\n","<tr><td>train_and_evaluate_0202b28a</td><td>TERMINATED</td><td>127.0.0.1:28459</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.456116</td><td style=\"text-align: right;\">      10</td><td>(55, 90, 65, 80, 55)</td><td style=\"text-align: right;\">    0.000381968</td><td style=\"text-align: right;\">0.72207 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.80579</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.885714</td><td style=\"text-align: right;\">      0.336538</td></tr>\n","<tr><td>train_and_evaluate_eff0db11</td><td>TERMINATED</td><td>127.0.0.1:28480</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.454644</td><td style=\"text-align: right;\">      10</td><td>(70, 45, 35, 90, 90)</td><td style=\"text-align: right;\">    0.00132737 </td><td style=\"text-align: right;\">0.91913 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.74182</td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.971014</td><td style=\"text-align: right;\">      0.898209</td></tr>\n","<tr><td>train_and_evaluate_8c599ec0</td><td>TERMINATED</td><td>127.0.0.1:28501</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.405042</td><td style=\"text-align: right;\">      10</td><td>(65, 90, 45, 40, 45)</td><td style=\"text-align: right;\">    0.00140366 </td><td style=\"text-align: right;\">0.5     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.90854</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_a64c7b8f</td><td>TERMINATED</td><td>127.0.0.1:28519</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.474756</td><td style=\"text-align: right;\">      10</td><td>(30, 30, 80, 50, 80)</td><td style=\"text-align: right;\">    0.00143718 </td><td style=\"text-align: right;\">0.5     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.87949</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.507246</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_96349dd3</td><td>TERMINATED</td><td>127.0.0.1:28537</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.479697</td><td style=\"text-align: right;\">      10</td><td>(85, 75, 40, 80, 75)</td><td style=\"text-align: right;\">    0.00255156 </td><td style=\"text-align: right;\">0.924928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.3161 </td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.956522</td><td style=\"text-align: right;\">      0.898551</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=27542)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A3.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=27542)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A3.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-11 15:38:29,305\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 50, 50, 50, 60)}\n","2024-08-11 15:38:47,887\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 75, 85, 90, 80)}\n","2024-08-11 15:39:05,176\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 35, 35, 50)}\n","2024-08-11 15:39:32,369\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 90, 55, 35)}\n","2024-08-11 15:40:02,539\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 90, 65, 60, 45)}\n","2024-08-11 15:40:15,734\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 55, 85, 45, 60)}\n","2024-08-11 15:40:33,013\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 85, 55, 90, 50)}\n","2024-08-11 15:40:47,529\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 60, 55, 65, 30)}\n","2024-08-11 15:41:01,979\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 75, 35, 85, 50)}\n","2024-08-11 15:41:18,272\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 35, 65, 80, 90)}\n","2024-08-11 15:41:37,209\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 65, 55, 35, 65)}\n","2024-08-11 15:41:51,940\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 90, 65, 50, 45)}\n","2024-08-11 15:42:06,870\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 35, 45, 50, 70)}\n","2024-08-11 15:42:21,809\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 50, 70, 80, 65)}\n","2024-08-11 15:42:34,652\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 70, 65, 30, 90)}\n","2024-08-11 15:42:47,686\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 45, 90, 50, 75)}\n","2024-08-11 15:43:07,291\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 55, 85, 75, 65)}\n","2024-08-11 15:43:19,672\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 35, 30, 50, 40)}\n","2024-08-11 15:43:32,332\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 85, 75, 85, 75)}\n","2024-08-11 15:43:44,185\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 30, 90, 80, 90)}\n","2024-08-11 15:43:58,657\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 35, 40, 90, 35)}\n","2024-08-11 15:44:14,596\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 30, 45, 90, 55)}\n","2024-08-11 15:44:31,374\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 35, 35, 40, 80)}\n","2024-08-11 15:44:48,302\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 90, 50, 30, 35)}\n","2024-08-11 15:45:05,140\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 30, 70, 55, 65)}\n","2024-08-11 15:45:24,506\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 50, 85, 35, 70)}\n","2024-08-11 15:45:43,509\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 90, 90, 70, 85)}\n","2024-08-11 15:45:55,810\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 90, 65, 80, 55)}\n","2024-08-11 15:46:08,612\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 45, 35, 90, 90)}\n","2024-08-11 15:46:19,624\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 90, 45, 40, 45)}\n","2024-08-11 15:46:30,628\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 30, 80, 50, 80)}\n","2024-08-11 15:46:44,115\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 40, 80, 75)}\n","2024-08-11 15:46:44,236\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-11_15-38-03' in 0.1193s.\n","2024-08-11 15:46:44,243\tINFO tune.py:1041 -- Total run time: 522.39 seconds (520.23 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.00255156012554144, 'batch_size': 32, 'hidden_layers': (85, 75, 40, 80, 75), 'epochs': 10, 'dropout_rate': 0.4796967270268306}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a3_splits, \"A3\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-11 16:12:03</td></tr>\n","<tr><td>Running for: </td><td>00:25:16.80        </td></tr>\n","<tr><td>Memory:      </td><td>11.6/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=10<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.6859314656230222<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_9b2d3396</td><td>TERMINATED</td><td>127.0.0.1:28566</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.410779</td><td style=\"text-align: right;\">      50</td><td>(85, 70, 65, 45, 70)</td><td style=\"text-align: right;\">    0.000231647</td><td style=\"text-align: right;\">0.697899</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.5798</td><td style=\"text-align: right;\">      0.629442</td><td style=\"text-align: right;\">      0.743003</td><td style=\"text-align: right;\">      0.623863</td></tr>\n","<tr><td>train_and_evaluate_26df366b</td><td>TERMINATED</td><td>127.0.0.1:28729</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.321661</td><td style=\"text-align: right;\">      50</td><td>(85, 50, 40, 50)    </td><td style=\"text-align: right;\">    0.000491122</td><td style=\"text-align: right;\">0.710099</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         38.1263</td><td style=\"text-align: right;\">      0.654822</td><td style=\"text-align: right;\">      0.748092</td><td style=\"text-align: right;\">      0.646804</td></tr>\n","<tr><td>train_and_evaluate_f4ddb59d</td><td>TERMINATED</td><td>127.0.0.1:28817</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.454449</td><td style=\"text-align: right;\">      10</td><td>(80, 75, 65, 55, 50)</td><td style=\"text-align: right;\">    0.000476328</td><td style=\"text-align: right;\">0.654148</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.6746</td><td style=\"text-align: right;\">      0.598985</td><td style=\"text-align: right;\">      0.704835</td><td style=\"text-align: right;\">      0.59633 </td></tr>\n","<tr><td>train_and_evaluate_307b090f</td><td>TERMINATED</td><td>127.0.0.1:28850</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.375117</td><td style=\"text-align: right;\">      10</td><td>(35, 55, 90, 40, 30)</td><td style=\"text-align: right;\">    0.000248086</td><td style=\"text-align: right;\">0.595691</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.0798</td><td style=\"text-align: right;\">      0.467005</td><td style=\"text-align: right;\">      0.676845</td><td style=\"text-align: right;\">      0.394355</td></tr>\n","<tr><td>train_and_evaluate_e357259f</td><td>TERMINATED</td><td>127.0.0.1:28888</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.290919</td><td style=\"text-align: right;\">      30</td><td>(85, 35, 85, 45, 45)</td><td style=\"text-align: right;\">    0.000299527</td><td style=\"text-align: right;\">0.580393</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.254 </td><td style=\"text-align: right;\">      0.527919</td><td style=\"text-align: right;\">      0.608142</td><td style=\"text-align: right;\">      0.468235</td></tr>\n","<tr><td>train_and_evaluate_9d5323a1</td><td>TERMINATED</td><td>127.0.0.1:29028</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.408665</td><td style=\"text-align: right;\">      50</td><td>(30, 75, 35, 80, 65)</td><td style=\"text-align: right;\">    0.000135284</td><td style=\"text-align: right;\">0.65161 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        115.201 </td><td style=\"text-align: right;\">      0.586294</td><td style=\"text-align: right;\">      0.684478</td><td style=\"text-align: right;\">      0.57644 </td></tr>\n","<tr><td>train_and_evaluate_69f98f8e</td><td>TERMINATED</td><td>127.0.0.1:29204</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.498652</td><td style=\"text-align: right;\">      30</td><td>(80, 30, 40, 35, 35)</td><td style=\"text-align: right;\">    0.000334171</td><td style=\"text-align: right;\">0.638884</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         41.9436</td><td style=\"text-align: right;\">      0.593909</td><td style=\"text-align: right;\">      0.671756</td><td style=\"text-align: right;\">      0.555153</td></tr>\n","<tr><td>train_and_evaluate_55ae5c59</td><td>TERMINATED</td><td>127.0.0.1:29306</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.448578</td><td style=\"text-align: right;\">      40</td><td>(75, 65, 70, 30, 75)</td><td style=\"text-align: right;\">    0.000328173</td><td style=\"text-align: right;\">0.582414</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.5289</td><td style=\"text-align: right;\">      0.555838</td><td style=\"text-align: right;\">      0.600509</td><td style=\"text-align: right;\">      0.454831</td></tr>\n","<tr><td>train_and_evaluate_632d9fe0</td><td>TERMINATED</td><td>127.0.0.1:29485</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.336196</td><td style=\"text-align: right;\">      20</td><td>(55, 35, 90, 75, 85)</td><td style=\"text-align: right;\">    0.000284677</td><td style=\"text-align: right;\">0.577358</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.7362</td><td style=\"text-align: right;\">      0.492386</td><td style=\"text-align: right;\">      0.615776</td><td style=\"text-align: right;\">      0.463398</td></tr>\n","<tr><td>train_and_evaluate_52039d27</td><td>TERMINATED</td><td>127.0.0.1:29536</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.264362</td><td style=\"text-align: right;\">      20</td><td>(85, 75, 70, 55, 90)</td><td style=\"text-align: right;\">    0.000246696</td><td style=\"text-align: right;\">0.572759</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.8428</td><td style=\"text-align: right;\">      0.527919</td><td style=\"text-align: right;\">      0.597964</td><td style=\"text-align: right;\">      0.432284</td></tr>\n","<tr><td>train_and_evaluate_63e5b5a7</td><td>TERMINATED</td><td>127.0.0.1:29590</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.273421</td><td style=\"text-align: right;\">      20</td><td>(45, 50, 45, 80, 55)</td><td style=\"text-align: right;\">    0.000139484</td><td style=\"text-align: right;\">0.632286</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.5253</td><td style=\"text-align: right;\">      0.558376</td><td style=\"text-align: right;\">      0.681934</td><td style=\"text-align: right;\">      0.556159</td></tr>\n","<tr><td>train_and_evaluate_06c83782</td><td>TERMINATED</td><td>127.0.0.1:29645</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.224599</td><td style=\"text-align: right;\">      50</td><td>(55, 65, 65, 85, 60)</td><td style=\"text-align: right;\">    0.00314277 </td><td style=\"text-align: right;\">0.69534 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         40.008 </td><td style=\"text-align: right;\">      0.651399</td><td style=\"text-align: right;\">      0.753181</td><td style=\"text-align: right;\">      0.651498</td></tr>\n","<tr><td>train_and_evaluate_4041396a</td><td>TERMINATED</td><td>127.0.0.1:29708</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.338854</td><td style=\"text-align: right;\">      50</td><td>(85, 75, 50, 55, 75)</td><td style=\"text-align: right;\">    0.00250412 </td><td style=\"text-align: right;\">0.695862</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         39.9342</td><td style=\"text-align: right;\">      0.63198 </td><td style=\"text-align: right;\">      0.725191</td><td style=\"text-align: right;\">      0.635483</td></tr>\n","<tr><td>train_and_evaluate_b7853902</td><td>TERMINATED</td><td>127.0.0.1:29774</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.339884</td><td style=\"text-align: right;\">      50</td><td>(75, 85, 40, 30, 35)</td><td style=\"text-align: right;\">    0.00132705 </td><td style=\"text-align: right;\">0.686695</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         38.137 </td><td style=\"text-align: right;\">      0.64467 </td><td style=\"text-align: right;\">      0.715013</td><td style=\"text-align: right;\">      0.643824</td></tr>\n","<tr><td>train_and_evaluate_c7900e11</td><td>TERMINATED</td><td>127.0.0.1:29838</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.377154</td><td style=\"text-align: right;\">      50</td><td>(40, 80, 55, 90, 60)</td><td style=\"text-align: right;\">    0.000992749</td><td style=\"text-align: right;\">0.692298</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         39.9422</td><td style=\"text-align: right;\">      0.634518</td><td style=\"text-align: right;\">      0.727735</td><td style=\"text-align: right;\">      0.616471</td></tr>\n","<tr><td>train_and_evaluate_6ea5b5dc</td><td>TERMINATED</td><td>127.0.0.1:29906</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.389798</td><td style=\"text-align: right;\">      50</td><td>(70, 40, 40, 75, 85)</td><td style=\"text-align: right;\">    0.000781047</td><td style=\"text-align: right;\">0.698391</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         40.2763</td><td style=\"text-align: right;\">      0.662437</td><td style=\"text-align: right;\">      0.740458</td><td style=\"text-align: right;\">      0.661221</td></tr>\n","<tr><td>train_and_evaluate_3e989dba</td><td>TERMINATED</td><td>127.0.0.1:29994</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.417571</td><td style=\"text-align: right;\">      40</td><td>(55, 45, 50, 55, 70)</td><td style=\"text-align: right;\">    0.00864528 </td><td style=\"text-align: right;\">0.659228</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.6357</td><td style=\"text-align: right;\">      0.616751</td><td style=\"text-align: right;\">      0.717557</td><td style=\"text-align: right;\">      0.549254</td></tr>\n","<tr><td>train_and_evaluate_894ddc1e</td><td>TERMINATED</td><td>127.0.0.1:30061</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.204621</td><td style=\"text-align: right;\">      40</td><td>(45, 65, 90, 30, 45)</td><td style=\"text-align: right;\">    0.00768859 </td><td style=\"text-align: right;\">0.705523</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.2373</td><td style=\"text-align: right;\">      0.647208</td><td style=\"text-align: right;\">      0.743003</td><td style=\"text-align: right;\">      0.631818</td></tr>\n","<tr><td>train_and_evaluate_849eab07</td><td>TERMINATED</td><td>127.0.0.1:30118</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.297941</td><td style=\"text-align: right;\">      50</td><td>(60, 70, 90, 40, 60)</td><td style=\"text-align: right;\">    0.000760604</td><td style=\"text-align: right;\">0.685168</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         40.0279</td><td style=\"text-align: right;\">      0.64467 </td><td style=\"text-align: right;\">      0.712468</td><td style=\"text-align: right;\">      0.633878</td></tr>\n","<tr><td>train_and_evaluate_f6e4ae82</td><td>TERMINATED</td><td>127.0.0.1:30183</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.204339</td><td style=\"text-align: right;\">      40</td><td>(45, 70, 80, 35, 35)</td><td style=\"text-align: right;\">    0.00944183 </td><td style=\"text-align: right;\">0.678056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.9786</td><td style=\"text-align: right;\">      0.619289</td><td style=\"text-align: right;\">      0.715013</td><td style=\"text-align: right;\">      0.600129</td></tr>\n","<tr><td>train_and_evaluate_41c2533b</td><td>TERMINATED</td><td>127.0.0.1:30260</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.20448 </td><td style=\"text-align: right;\">      40</td><td>(80, 70, 70, 65, 35)</td><td style=\"text-align: right;\">    0.00836363 </td><td style=\"text-align: right;\">0.678555</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.3674</td><td style=\"text-align: right;\">      0.639594</td><td style=\"text-align: right;\">      0.73028 </td><td style=\"text-align: right;\">      0.592004</td></tr>\n","<tr><td>train_and_evaluate_eb1bde45</td><td>TERMINATED</td><td>127.0.0.1:30317</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.241235</td><td style=\"text-align: right;\">      40</td><td>(35, 90, 50, 40, 45)</td><td style=\"text-align: right;\">    0.00399125 </td><td style=\"text-align: right;\">0.71163 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.4339</td><td style=\"text-align: right;\">      0.647208</td><td style=\"text-align: right;\">      0.753181</td><td style=\"text-align: right;\">      0.638725</td></tr>\n","<tr><td>train_and_evaluate_2a04e5a9</td><td>TERMINATED</td><td>127.0.0.1:30371</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.249807</td><td style=\"text-align: right;\">      40</td><td>(90, 50, 60, 80, 30)</td><td style=\"text-align: right;\">    0.00167166 </td><td style=\"text-align: right;\">0.694323</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.4103</td><td style=\"text-align: right;\">      0.654822</td><td style=\"text-align: right;\">      0.715013</td><td style=\"text-align: right;\">      0.651064</td></tr>\n","<tr><td>train_and_evaluate_eae083b7</td><td>TERMINATED</td><td>127.0.0.1:30431</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.242672</td><td style=\"text-align: right;\">      40</td><td>(40, 35, 30, 35, 80)</td><td style=\"text-align: right;\">    0.0048004  </td><td style=\"text-align: right;\">0.705013</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         32.9839</td><td style=\"text-align: right;\">      0.649746</td><td style=\"text-align: right;\">      0.770992</td><td style=\"text-align: right;\">      0.646495</td></tr>\n","<tr><td>train_and_evaluate_fd11e9e5</td><td>TERMINATED</td><td>127.0.0.1:30486</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.236639</td><td style=\"text-align: right;\">      40</td><td>(65, 70, 75, 65, 40)</td><td style=\"text-align: right;\">    0.00492021 </td><td style=\"text-align: right;\">0.683649</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         34.5382</td><td style=\"text-align: right;\">      0.629442</td><td style=\"text-align: right;\">      0.727735</td><td style=\"text-align: right;\">      0.621129</td></tr>\n","<tr><td>train_and_evaluate_7bb0e950</td><td>TERMINATED</td><td>127.0.0.1:30589</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.30708 </td><td style=\"text-align: right;\">      40</td><td>(70, 35, 60, 60, 60)</td><td style=\"text-align: right;\">    0.00393341 </td><td style=\"text-align: right;\">0.688736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         31.8342</td><td style=\"text-align: right;\">      0.634518</td><td style=\"text-align: right;\">      0.704835</td><td style=\"text-align: right;\">      0.611329</td></tr>\n","<tr><td>train_and_evaluate_5e66896b</td><td>TERMINATED</td><td>127.0.0.1:30644</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.313281</td><td style=\"text-align: right;\">      40</td><td>(65, 70, 30, 50, 50)</td><td style=\"text-align: right;\">    0.00520135 </td><td style=\"text-align: right;\">0.688738</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.2943</td><td style=\"text-align: right;\">      0.629442</td><td style=\"text-align: right;\">      0.715013</td><td style=\"text-align: right;\">      0.606733</td></tr>\n","<tr><td>train_and_evaluate_b059a58e</td><td>TERMINATED</td><td>127.0.0.1:30701</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.319822</td><td style=\"text-align: right;\">      10</td><td>(75, 45, 85, 55, 85)</td><td style=\"text-align: right;\">    0.0020827  </td><td style=\"text-align: right;\">0.662285</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         28.285 </td><td style=\"text-align: right;\">      0.600509</td><td style=\"text-align: right;\">      0.707379</td><td style=\"text-align: right;\">      0.506251</td></tr>\n","<tr><td>train_and_evaluate_0d993711</td><td>TERMINATED</td><td>127.0.0.1:30749</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.222969</td><td style=\"text-align: right;\">      30</td><td>(30, 70, 40, 80, 90)</td><td style=\"text-align: right;\">    0.00208087 </td><td style=\"text-align: right;\">0.706537</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.0428</td><td style=\"text-align: right;\">      0.654822</td><td style=\"text-align: right;\">      0.740458</td><td style=\"text-align: right;\">      0.637235</td></tr>\n","<tr><td>train_and_evaluate_40a015a2</td><td>TERMINATED</td><td>127.0.0.1:30860</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.276512</td><td style=\"text-align: right;\">      30</td><td>(50, 50, 90, 85, 75)</td><td style=\"text-align: right;\">    0.00647149 </td><td style=\"text-align: right;\">0.688741</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         27.1767</td><td style=\"text-align: right;\">      0.624365</td><td style=\"text-align: right;\">      0.722646</td><td style=\"text-align: right;\">      0.608443</td></tr>\n","<tr><td>train_and_evaluate_21409efd</td><td>TERMINATED</td><td>127.0.0.1:30933</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.276758</td><td style=\"text-align: right;\">      30</td><td>(65, 60, 65, 30, 45)</td><td style=\"text-align: right;\">    0.000453546</td><td style=\"text-align: right;\">0.704503</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.502 </td><td style=\"text-align: right;\">      0.652284</td><td style=\"text-align: right;\">      0.735369</td><td style=\"text-align: right;\">      0.646306</td></tr>\n","<tr><td>train_and_evaluate_068b4f2b</td><td>TERMINATED</td><td>127.0.0.1:31038</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.222352</td><td style=\"text-align: right;\">      30</td><td>(90, 85, 90, 45, 70)</td><td style=\"text-align: right;\">    0.000540358</td><td style=\"text-align: right;\">0.599207</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.305 </td><td style=\"text-align: right;\">      0.558376</td><td style=\"text-align: right;\">      0.661578</td><td style=\"text-align: right;\">      0.47861 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=28566)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A4.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=28566)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A4.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-11 15:47:59,432\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 70, 65, 45, 70)}\n","2024-08-11 15:48:41,502\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 50, 40, 50)}\n","2024-08-11 15:49:02,794\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 75, 65, 55, 50)}\n","2024-08-11 15:49:23,966\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 55, 90, 40, 30)}\n","2024-08-11 15:50:38,129\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 35, 85, 45, 45)}\n","2024-08-11 15:52:37,564\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 75, 35, 80, 65)}\n","2024-08-11 15:53:23,453\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 30, 40, 35, 35)}\n","2024-08-11 15:55:03,237\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 65, 70, 30, 75)}\n","2024-08-11 15:55:36,249\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 35, 90, 75, 85)}\n","2024-08-11 15:56:09,336\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 70, 55, 90)}\n","2024-08-11 15:56:43,054\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 50, 45, 80, 55)}\n","2024-08-11 15:57:26,852\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 65, 65, 85, 60)}\n","2024-08-11 15:58:10,652\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 50, 55, 75)}\n","2024-08-11 15:58:52,900\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 85, 40, 30, 35)}\n","2024-08-11 15:59:36,792\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 80, 55, 90, 60)}\n","2024-08-11 16:00:21,258\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 40, 40, 75, 85)}\n","2024-08-11 16:00:58,588\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 45, 50, 55, 70)}\n","2024-08-11 16:01:35,123\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 65, 90, 30, 45)}\n","2024-08-11 16:02:19,060\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 70, 90, 40, 60)}\n","2024-08-11 16:02:57,011\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 70, 80, 35, 35)}\n","2024-08-11 16:03:34,419\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 70, 70, 65, 35)}\n","2024-08-11 16:04:11,482\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 90, 50, 40, 45)}\n","2024-08-11 16:04:48,490\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 50, 60, 80, 30)}\n","2024-08-11 16:05:25,110\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 30, 35, 80)}\n","2024-08-11 16:06:03,680\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 70, 75, 65, 40)}\n","2024-08-11 16:06:39,137\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 35, 60, 60, 60)}\n","2024-08-11 16:07:16,633\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 70, 30, 50, 50)}\n","2024-08-11 16:07:48,571\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 45, 85, 55, 85)}\n","2024-08-11 16:09:04,194\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 70, 40, 80, 90)}\n","2024-08-11 16:09:35,582\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 50, 90, 85, 75)}\n","2024-08-11 16:10:47,845\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 60, 65, 30, 45)}\n","2024-08-11 16:12:02,956\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 85, 90, 45, 70)}\n","2024-08-11 16:12:03,082\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-11_15-46-45' in 0.1237s.\n","2024-08-11 16:12:03,088\tINFO tune.py:1041 -- Total run time: 1518.78 seconds (1516.67 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.003991246077136659, 'batch_size': 128, 'hidden_layers': (35, 90, 50, 40, 45), 'epochs': 40, 'dropout_rate': 0.2412349299577167}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a4_splits, \"A4\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-11 16:44:43</td></tr>\n","<tr><td>Running for: </td><td>00:32:38.07        </td></tr>\n","<tr><td>Memory:      </td><td>11.7/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=16<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.2875366510021288<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_68216dea</td><td>TERMINATED</td><td>127.0.0.1:31163</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.218793</td><td style=\"text-align: right;\">      20</td><td>(70, 70, 40, 80, 50)</td><td style=\"text-align: right;\">    0.000111358</td><td style=\"text-align: right;\">0.309538</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         52.7195</td><td style=\"text-align: right;\">      0.27907 </td><td style=\"text-align: right;\">      0.360104</td><td style=\"text-align: right;\">     0.260333 </td></tr>\n","<tr><td>train_and_evaluate_5b2d6334</td><td>TERMINATED</td><td>127.0.0.1:31305</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.272707</td><td style=\"text-align: right;\">      20</td><td>(70, 65, 55, 45, 70)</td><td style=\"text-align: right;\">    0.000781905</td><td style=\"text-align: right;\">0.317287</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         54.9252</td><td style=\"text-align: right;\">      0.282383</td><td style=\"text-align: right;\">      0.336788</td><td style=\"text-align: right;\">     0.272012 </td></tr>\n","<tr><td>train_and_evaluate_65a21aee</td><td>TERMINATED</td><td>127.0.0.1:31439</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.45018 </td><td style=\"text-align: right;\">      10</td><td>(50, 75, 60, 30, 30)</td><td style=\"text-align: right;\">    0.00149841 </td><td style=\"text-align: right;\">0.256206</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.5489</td><td style=\"text-align: right;\">      0.219638</td><td style=\"text-align: right;\">      0.302326</td><td style=\"text-align: right;\">     0.126909 </td></tr>\n","<tr><td>train_and_evaluate_1e2f13fd</td><td>TERMINATED</td><td>127.0.0.1:31489</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.322403</td><td style=\"text-align: right;\">      40</td><td>(75, 90, 85, 60, 45)</td><td style=\"text-align: right;\">    0.000620446</td><td style=\"text-align: right;\">0.295018</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.1543</td><td style=\"text-align: right;\">      0.253886</td><td style=\"text-align: right;\">      0.323834</td><td style=\"text-align: right;\">     0.23585  </td></tr>\n","<tr><td>train_and_evaluate_c0536757</td><td>TERMINATED</td><td>127.0.0.1:31649</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.253595</td><td style=\"text-align: right;\">      30</td><td>(60, 85, 85, 65, 90)</td><td style=\"text-align: right;\">    0.000603   </td><td style=\"text-align: right;\">0.230862</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         26.755 </td><td style=\"text-align: right;\">      0.196382</td><td style=\"text-align: right;\">      0.272021</td><td style=\"text-align: right;\">     0.102357 </td></tr>\n","<tr><td>train_and_evaluate_72a7cbdc</td><td>TERMINATED</td><td>127.0.0.1:31698</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.44846 </td><td style=\"text-align: right;\">      20</td><td>(55, 30, 55, 30, 30)</td><td style=\"text-align: right;\">    0.000489886</td><td style=\"text-align: right;\">0.257234</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.0721</td><td style=\"text-align: right;\">      0.238342</td><td style=\"text-align: right;\">      0.29199 </td><td style=\"text-align: right;\">     0.143659 </td></tr>\n","<tr><td>train_and_evaluate_9d607b59</td><td>TERMINATED</td><td>127.0.0.1:31750</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.404192</td><td style=\"text-align: right;\">      30</td><td>(50, 30, 90, 55, 35)</td><td style=\"text-align: right;\">    0.000489296</td><td style=\"text-align: right;\">0.283133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.2651</td><td style=\"text-align: right;\">      0.251295</td><td style=\"text-align: right;\">      0.321244</td><td style=\"text-align: right;\">     0.237131 </td></tr>\n","<tr><td>train_and_evaluate_9655de85</td><td>TERMINATED</td><td>127.0.0.1:31867</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.340042</td><td style=\"text-align: right;\">      10</td><td>(35, 85, 75, 55, 50)</td><td style=\"text-align: right;\">    0.00372203 </td><td style=\"text-align: right;\">0.29194 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.2751</td><td style=\"text-align: right;\">      0.263566</td><td style=\"text-align: right;\">      0.339378</td><td style=\"text-align: right;\">     0.202148 </td></tr>\n","<tr><td>train_and_evaluate_57f09617</td><td>TERMINATED</td><td>127.0.0.1:31903</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.431568</td><td style=\"text-align: right;\">      20</td><td>(55, 65, 35, 55, 50)</td><td style=\"text-align: right;\">    0.00416328 </td><td style=\"text-align: right;\">0.274835</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.7534</td><td style=\"text-align: right;\">      0.202073</td><td style=\"text-align: right;\">      0.336788</td><td style=\"text-align: right;\">     0.152118 </td></tr>\n","<tr><td>train_and_evaluate_2c77a4ff</td><td>TERMINATED</td><td>127.0.0.1:31954</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.212696</td><td style=\"text-align: right;\">      20</td><td>(60, 60, 85, 35, 80)</td><td style=\"text-align: right;\">    0.0011401  </td><td style=\"text-align: right;\">0.304844</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         19.3777</td><td style=\"text-align: right;\">      0.253886</td><td style=\"text-align: right;\">      0.346253</td><td style=\"text-align: right;\">     0.234801 </td></tr>\n","<tr><td>train_and_evaluate_b381106d</td><td>TERMINATED</td><td>127.0.0.1:31987</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.223869</td><td style=\"text-align: right;\">      10</td><td>(50, 45, 55, 40, 65)</td><td style=\"text-align: right;\">    0.000275712</td><td style=\"text-align: right;\">0.26553 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.374 </td><td style=\"text-align: right;\">      0.246114</td><td style=\"text-align: right;\">      0.300518</td><td style=\"text-align: right;\">     0.173894 </td></tr>\n","<tr><td>train_and_evaluate_daa49850</td><td>TERMINATED</td><td>127.0.0.1:32017</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.285769</td><td style=\"text-align: right;\">      50</td><td>(80, 90, 85, 80, 75)</td><td style=\"text-align: right;\">    0.000163034</td><td style=\"text-align: right;\">0.239658</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        113.823 </td><td style=\"text-align: right;\">      0.209845</td><td style=\"text-align: right;\">      0.292746</td><td style=\"text-align: right;\">     0.126451 </td></tr>\n","<tr><td>train_and_evaluate_03268dc2</td><td>TERMINATED</td><td>127.0.0.1:32190</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.284169</td><td style=\"text-align: right;\">      50</td><td>(80, 65, 50, 50, 30)</td><td style=\"text-align: right;\">    0.000153825</td><td style=\"text-align: right;\">0.244312</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.4   </td><td style=\"text-align: right;\">      0.199482</td><td style=\"text-align: right;\">      0.277202</td><td style=\"text-align: right;\">     0.100465 </td></tr>\n","<tr><td>train_and_evaluate_9da901bb</td><td>TERMINATED</td><td>127.0.0.1:32402</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.274654</td><td style=\"text-align: right;\">      20</td><td>(40, 35, 65, 35, 90)</td><td style=\"text-align: right;\">    0.00010207 </td><td style=\"text-align: right;\">0.274814</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.0567</td><td style=\"text-align: right;\">      0.243523</td><td style=\"text-align: right;\">      0.317829</td><td style=\"text-align: right;\">     0.185137 </td></tr>\n","<tr><td>train_and_evaluate_25549b7a</td><td>TERMINATED</td><td>127.0.0.1:32487</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.203573</td><td style=\"text-align: right;\">      20</td><td>(85, 75, 50, 50)    </td><td style=\"text-align: right;\">    0.00210885 </td><td style=\"text-align: right;\">0.302774</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         43.1406</td><td style=\"text-align: right;\">      0.240933</td><td style=\"text-align: right;\">      0.325581</td><td style=\"text-align: right;\">     0.225793 </td></tr>\n","<tr><td>train_and_evaluate_81871533</td><td>TERMINATED</td><td>127.0.0.1:32558</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.201558</td><td style=\"text-align: right;\">      20</td><td>(50, 30, 90, 60, 30)</td><td style=\"text-align: right;\">    0.00220319 </td><td style=\"text-align: right;\">0.310553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.9373</td><td style=\"text-align: right;\">      0.284238</td><td style=\"text-align: right;\">      0.348837</td><td style=\"text-align: right;\">     0.258782 </td></tr>\n","<tr><td>train_and_evaluate_5dbf6bda</td><td>TERMINATED</td><td>127.0.0.1:32681</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.495526</td><td style=\"text-align: right;\">      40</td><td>(40, 35, 65, 55, 85)</td><td style=\"text-align: right;\">    0.00866238 </td><td style=\"text-align: right;\">0.190986</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.7053</td><td style=\"text-align: right;\">      0.165375</td><td style=\"text-align: right;\">      0.232558</td><td style=\"text-align: right;\">     0.0473023</td></tr>\n","<tr><td>train_and_evaluate_cf274344</td><td>TERMINATED</td><td>127.0.0.1:32859</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.383446</td><td style=\"text-align: right;\">      40</td><td>(35, 85, 50, 30, 65)</td><td style=\"text-align: right;\">    0.00239069 </td><td style=\"text-align: right;\">0.311576</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.5641</td><td style=\"text-align: right;\">      0.290155</td><td style=\"text-align: right;\">      0.330749</td><td style=\"text-align: right;\">     0.274997 </td></tr>\n","<tr><td>train_and_evaluate_d0f60304</td><td>TERMINATED</td><td>127.0.0.1:33003</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.375645</td><td style=\"text-align: right;\">      20</td><td>(80, 75, 40, 40, 45)</td><td style=\"text-align: right;\">    0.00226126 </td><td style=\"text-align: right;\">0.302783</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.6651</td><td style=\"text-align: right;\">      0.282383</td><td style=\"text-align: right;\">      0.320413</td><td style=\"text-align: right;\">     0.253916 </td></tr>\n","<tr><td>train_and_evaluate_7ad080fa</td><td>TERMINATED</td><td>127.0.0.1:33117</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.391236</td><td style=\"text-align: right;\">      40</td><td>(75, 85, 60, 45, 45)</td><td style=\"text-align: right;\">    0.00433841 </td><td style=\"text-align: right;\">0.28312 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.4814</td><td style=\"text-align: right;\">      0.259067</td><td style=\"text-align: right;\">      0.294574</td><td style=\"text-align: right;\">     0.211104 </td></tr>\n","<tr><td>train_and_evaluate_8c9da834</td><td>TERMINATED</td><td>127.0.0.1:33293</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.316909</td><td style=\"text-align: right;\">      40</td><td>(85, 90, 60, 45, 45)</td><td style=\"text-align: right;\">    0.00836816 </td><td style=\"text-align: right;\">0.264982</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         52.8983</td><td style=\"text-align: right;\">      0.15544 </td><td style=\"text-align: right;\">      0.313472</td><td style=\"text-align: right;\">     0.116017 </td></tr>\n","<tr><td>train_and_evaluate_d42346d0</td><td>TERMINATED</td><td>127.0.0.1:33378</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.350422</td><td style=\"text-align: right;\">      40</td><td>(50, 50, 90, 50, 75)</td><td style=\"text-align: right;\">    0.00961853 </td><td style=\"text-align: right;\">0.205457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         52.5536</td><td style=\"text-align: right;\">      0.176166</td><td style=\"text-align: right;\">      0.237726</td><td style=\"text-align: right;\">     0.113063 </td></tr>\n","<tr><td>train_and_evaluate_f3bfbf44</td><td>TERMINATED</td><td>127.0.0.1:33467</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.251278</td><td style=\"text-align: right;\">      40</td><td>(70, 50, 35, 50, 85)</td><td style=\"text-align: right;\">    0.0009424  </td><td style=\"text-align: right;\">0.297081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.2967</td><td style=\"text-align: right;\">      0.248705</td><td style=\"text-align: right;\">      0.328165</td><td style=\"text-align: right;\">     0.248049 </td></tr>\n","<tr><td>train_and_evaluate_f52eac57</td><td>TERMINATED</td><td>127.0.0.1:33608</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.247138</td><td style=\"text-align: right;\">      20</td><td>(30, 60, 35, 60)    </td><td style=\"text-align: right;\">    0.00204351 </td><td style=\"text-align: right;\">0.30019 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         45.0142</td><td style=\"text-align: right;\">      0.261658</td><td style=\"text-align: right;\">      0.338501</td><td style=\"text-align: right;\">     0.241551 </td></tr>\n","<tr><td>train_and_evaluate_de8d9290</td><td>TERMINATED</td><td>127.0.0.1:33700</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.246539</td><td style=\"text-align: right;\">      20</td><td>(50, 35, 80, 70, 55)</td><td style=\"text-align: right;\">    0.00201137 </td><td style=\"text-align: right;\">0.300212</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         47.6249</td><td style=\"text-align: right;\">      0.279793</td><td style=\"text-align: right;\">      0.323834</td><td style=\"text-align: right;\">     0.248703 </td></tr>\n","<tr><td>train_and_evaluate_07fd19ff</td><td>TERMINATED</td><td>127.0.0.1:33780</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.312168</td><td style=\"text-align: right;\">      50</td><td>(70, 60, 35, 35, 75)</td><td style=\"text-align: right;\">    0.00287885 </td><td style=\"text-align: right;\">0.254144</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.705 </td><td style=\"text-align: right;\">      0.237726</td><td style=\"text-align: right;\">      0.287565</td><td style=\"text-align: right;\">     0.171396 </td></tr>\n","<tr><td>train_and_evaluate_b7a08722</td><td>TERMINATED</td><td>127.0.0.1:33947</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.304251</td><td style=\"text-align: right;\">      30</td><td>(45, 60, 40, 75, 90)</td><td style=\"text-align: right;\">    0.00307326 </td><td style=\"text-align: right;\">0.257251</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.3998</td><td style=\"text-align: right;\">      0.233161</td><td style=\"text-align: right;\">      0.287565</td><td style=\"text-align: right;\">     0.162668 </td></tr>\n","<tr><td>train_and_evaluate_54dc37f5</td><td>TERMINATED</td><td>127.0.0.1:34059</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.36001 </td><td style=\"text-align: right;\">      30</td><td>(75, 90, 40, 30, 90)</td><td style=\"text-align: right;\">    0.000975203</td><td style=\"text-align: right;\">0.318298</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.8541</td><td style=\"text-align: right;\">      0.266839</td><td style=\"text-align: right;\">      0.351421</td><td style=\"text-align: right;\">     0.245337 </td></tr>\n","<tr><td>train_and_evaluate_3d03bd8d</td><td>TERMINATED</td><td>127.0.0.1:34171</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.367907</td><td style=\"text-align: right;\">      20</td><td>(40, 35, 90, 85)    </td><td style=\"text-align: right;\">    0.000947554</td><td style=\"text-align: right;\">0.297606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         45.7715</td><td style=\"text-align: right;\">      0.253886</td><td style=\"text-align: right;\">      0.320413</td><td style=\"text-align: right;\">     0.251071 </td></tr>\n","<tr><td>train_and_evaluate_90372af2</td><td>TERMINATED</td><td>127.0.0.1:34256</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.360314</td><td style=\"text-align: right;\">      30</td><td>(40, 80, 55, 55, 45)</td><td style=\"text-align: right;\">    0.00095781 </td><td style=\"text-align: right;\">0.297609</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.1459</td><td style=\"text-align: right;\">      0.261658</td><td style=\"text-align: right;\">      0.330749</td><td style=\"text-align: right;\">     0.259124 </td></tr>\n","<tr><td>train_and_evaluate_9dbd4e23</td><td>TERMINATED</td><td>127.0.0.1:34371</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.401863</td><td style=\"text-align: right;\">      30</td><td>(75, 35, 45, 90)    </td><td style=\"text-align: right;\">    0.00032251 </td><td style=\"text-align: right;\">0.292434</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         25.0041</td><td style=\"text-align: right;\">      0.251295</td><td style=\"text-align: right;\">      0.318653</td><td style=\"text-align: right;\">     0.222507 </td></tr>\n","<tr><td>train_and_evaluate_7d677121</td><td>TERMINATED</td><td>127.0.0.1:34418</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.40858 </td><td style=\"text-align: right;\">      30</td><td>(45, 40, 45, 40, 85)</td><td style=\"text-align: right;\">    0.000327902</td><td style=\"text-align: right;\">0.275875</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         26.2265</td><td style=\"text-align: right;\">      0.259067</td><td style=\"text-align: right;\">      0.289406</td><td style=\"text-align: right;\">     0.215303 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=31163)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A12.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=31163)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A12.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-11 16:13:03,160\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 70, 40, 80, 50)}\n","2024-08-11 16:14:01,730\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 65, 55, 45, 70)}\n","2024-08-11 16:14:19,339\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 75, 60, 30, 30)}\n","2024-08-11 16:15:56,614\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 90, 85, 60, 45)}\n","2024-08-11 16:16:27,513\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 85, 85, 65, 90)}\n","2024-08-11 16:17:00,904\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 30, 55, 30, 30)}\n","2024-08-11 16:18:15,733\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 30, 90, 55, 35)}\n","2024-08-11 16:18:38,190\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 85, 75, 55, 50)}\n","2024-08-11 16:19:11,636\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 65, 35, 55, 50)}\n","2024-08-11 16:19:35,327\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 60, 85, 35, 80)}\n","2024-08-11 16:19:52,211\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 45, 55, 40, 65)}\n","2024-08-11 16:21:49,826\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 90, 85, 80, 75)}\n","2024-08-11 16:23:48,621\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 65, 50, 50, 30)}\n","2024-08-11 16:24:41,447\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 65, 35, 90)}\n","2024-08-11 16:25:28,481\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 75, 50, 50)}\n","2024-08-11 16:26:22,391\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 30, 90, 60, 30)}\n","2024-08-11 16:28:00,194\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 65, 55, 85)}\n","2024-08-11 16:29:36,285\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 85, 50, 30, 65)}\n","2024-08-11 16:30:29,417\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 75, 40, 40, 45)}\n","2024-08-11 16:32:06,517\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 85, 60, 45, 45)}\n","2024-08-11 16:33:03,831\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 90, 60, 45, 45)}\n","2024-08-11 16:34:00,386\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 50, 90, 50, 75)}\n","2024-08-11 16:35:36,203\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 50, 35, 50, 85)}\n","2024-08-11 16:36:25,007\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 60, 35, 60)}\n","2024-08-11 16:37:16,606\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 35, 80, 70, 55)}\n","2024-08-11 16:39:09,866\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 60, 35, 35, 75)}\n","2024-08-11 16:40:24,693\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 60, 40, 75, 90)}\n","2024-08-11 16:41:39,364\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 90, 40, 30, 90)}\n","2024-08-11 16:42:29,330\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 35, 90, 85)}\n","2024-08-11 16:43:43,808\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 80, 55, 55, 45)}\n","2024-08-11 16:44:12,757\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 35, 45, 90)}\n","2024-08-11 16:44:42,994\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 40, 45, 40, 85)}\n","2024-08-11 16:44:43,235\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-11_16-12-04' in 0.1266s.\n","2024-08-11 16:44:43,242\tINFO tune.py:1041 -- Total run time: 1960.08 seconds (1957.94 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.0009752032860232802, 'batch_size': 32, 'hidden_layers': (75, 90, 40, 30, 90), 'epochs': 30, 'dropout_rate': 0.36000954648695677}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a12_splits, \"A12\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-11 17:00:00</td></tr>\n","<tr><td>Running for: </td><td>00:15:14.77        </td></tr>\n","<tr><td>Memory:      </td><td>11.7/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=14<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.7100449236298292<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_22621218</td><td>TERMINATED</td><td>127.0.0.1:34469</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.222726</td><td style=\"text-align: right;\">      10</td><td>(40, 70, 40, 80, 70)</td><td style=\"text-align: right;\">    0.000590307</td><td style=\"text-align: right;\">0.667206</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.65057</td><td style=\"text-align: right;\">      0.619048</td><td style=\"text-align: right;\">      0.716981</td><td style=\"text-align: right;\">      0.577295</td></tr>\n","<tr><td>train_and_evaluate_e2b8f74b</td><td>TERMINATED</td><td>127.0.0.1:34490</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.43581 </td><td style=\"text-align: right;\">      50</td><td>(40, 90, 60, 65, 90)</td><td style=\"text-align: right;\">    0.00596532 </td><td style=\"text-align: right;\">0.608589</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        22.6503 </td><td style=\"text-align: right;\">      0.490566</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_f36f8fe2</td><td>TERMINATED</td><td>127.0.0.1:34530</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.263188</td><td style=\"text-align: right;\">      30</td><td>(30, 75, 80, 75, 90)</td><td style=\"text-align: right;\">    0.00147135 </td><td style=\"text-align: right;\">0.720539</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.1059 </td><td style=\"text-align: right;\">      0.685714</td><td style=\"text-align: right;\">      0.780952</td><td style=\"text-align: right;\">      0.671284</td></tr>\n","<tr><td>train_and_evaluate_6f3b5199</td><td>TERMINATED</td><td>127.0.0.1:34563</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.380552</td><td style=\"text-align: right;\">      50</td><td>(85, 85, 80, 80, 45)</td><td style=\"text-align: right;\">    0.000748961</td><td style=\"text-align: right;\">0.716604</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.4393 </td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.783019</td><td style=\"text-align: right;\">      0.627622</td></tr>\n","<tr><td>train_and_evaluate_9a40801f</td><td>TERMINATED</td><td>127.0.0.1:34633</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.263001</td><td style=\"text-align: right;\">      40</td><td>(50, 60, 30, 60, 80)</td><td style=\"text-align: right;\">    0.00147537 </td><td style=\"text-align: right;\">0.71292 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        30.6045 </td><td style=\"text-align: right;\">      0.67619 </td><td style=\"text-align: right;\">      0.8     </td><td style=\"text-align: right;\">      0.65987 </td></tr>\n","<tr><td>train_and_evaluate_3d91fbab</td><td>TERMINATED</td><td>127.0.0.1:34684</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.323234</td><td style=\"text-align: right;\">      30</td><td>(65, 65, 30, 80)    </td><td style=\"text-align: right;\">    0.000501718</td><td style=\"text-align: right;\">0.705355</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.9367 </td><td style=\"text-align: right;\">      0.638095</td><td style=\"text-align: right;\">      0.742857</td><td style=\"text-align: right;\">      0.635417</td></tr>\n","<tr><td>train_and_evaluate_2dccf331</td><td>TERMINATED</td><td>127.0.0.1:34709</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.444578</td><td style=\"text-align: right;\">      30</td><td>(80, 85, 35, 35, 40)</td><td style=\"text-align: right;\">    0.000195843</td><td style=\"text-align: right;\">0.686307</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.1387 </td><td style=\"text-align: right;\">      0.647619</td><td style=\"text-align: right;\">      0.72381 </td><td style=\"text-align: right;\">      0.587496</td></tr>\n","<tr><td>train_and_evaluate_331fbfcb</td><td>TERMINATED</td><td>127.0.0.1:34742</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.24549 </td><td style=\"text-align: right;\">      20</td><td>(60, 60, 45, 30, 50)</td><td style=\"text-align: right;\">    0.000152514</td><td style=\"text-align: right;\">0.692058</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5673 </td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.771429</td><td style=\"text-align: right;\">      0.632537</td></tr>\n","<tr><td>train_and_evaluate_9a4954e8</td><td>TERMINATED</td><td>127.0.0.1:34780</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.484509</td><td style=\"text-align: right;\">      50</td><td>(40, 50, 40, 50, 75)</td><td style=\"text-align: right;\">    0.000428808</td><td style=\"text-align: right;\">0.726199</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.2091 </td><td style=\"text-align: right;\">      0.67619 </td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.676161</td></tr>\n","<tr><td>train_and_evaluate_9face699</td><td>TERMINATED</td><td>127.0.0.1:34841</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.296144</td><td style=\"text-align: right;\">      30</td><td>(40, 30, 75, 70, 55)</td><td style=\"text-align: right;\">    0.000345762</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.6994 </td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_07133cde</td><td>TERMINATED</td><td>127.0.0.1:34868</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.35841 </td><td style=\"text-align: right;\">      30</td><td>(65, 30, 70, 55, 70)</td><td style=\"text-align: right;\">    0.00555746 </td><td style=\"text-align: right;\">0.709111</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.3449 </td><td style=\"text-align: right;\">      0.67619 </td><td style=\"text-align: right;\">      0.752381</td><td style=\"text-align: right;\">      0.653263</td></tr>\n","<tr><td>train_and_evaluate_cc42afba</td><td>TERMINATED</td><td>127.0.0.1:34893</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.494299</td><td style=\"text-align: right;\">      50</td><td>(50, 35, 55, 55, 85)</td><td style=\"text-align: right;\">    0.00558716 </td><td style=\"text-align: right;\">0.663288</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        35.5902 </td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.773585</td><td style=\"text-align: right;\">      0.513889</td></tr>\n","<tr><td>train_and_evaluate_694f38e9</td><td>TERMINATED</td><td>127.0.0.1:34955</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.499084</td><td style=\"text-align: right;\">      50</td><td>(65, 45, 35, 65)    </td><td style=\"text-align: right;\">    0.00172906 </td><td style=\"text-align: right;\">0.724295</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.0315 </td><td style=\"text-align: right;\">      0.609524</td><td style=\"text-align: right;\">      0.771429</td><td style=\"text-align: right;\">      0.600241</td></tr>\n","<tr><td>train_and_evaluate_67d476d6</td><td>TERMINATED</td><td>127.0.0.1:35012</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.201307</td><td style=\"text-align: right;\">      10</td><td>(40, 65, 55, 70, 75)</td><td style=\"text-align: right;\">    0.00189057 </td><td style=\"text-align: right;\">0.710979</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.82526</td><td style=\"text-align: right;\">      0.657143</td><td style=\"text-align: right;\">      0.780952</td><td style=\"text-align: right;\">      0.632867</td></tr>\n","<tr><td>train_and_evaluate_c17c61c5</td><td>TERMINATED</td><td>127.0.0.1:35033</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.498312</td><td style=\"text-align: right;\">      50</td><td>(90, 40, 45, 50, 45)</td><td style=\"text-align: right;\">    0.00230034 </td><td style=\"text-align: right;\">0.720521</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        35.3148 </td><td style=\"text-align: right;\">      0.67619 </td><td style=\"text-align: right;\">      0.790476</td><td style=\"text-align: right;\">      0.662698</td></tr>\n","<tr><td>train_and_evaluate_6fd4ce7e</td><td>TERMINATED</td><td>127.0.0.1:35096</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.496649</td><td style=\"text-align: right;\">      50</td><td>(30, 60, 55, 30, 70)</td><td style=\"text-align: right;\">    0.0028298  </td><td style=\"text-align: right;\">0.71301 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.8001 </td><td style=\"text-align: right;\">      0.657143</td><td style=\"text-align: right;\">      0.771429</td><td style=\"text-align: right;\">      0.608534</td></tr>\n","<tr><td>train_and_evaluate_6aa0c339</td><td>TERMINATED</td><td>127.0.0.1:35156</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.443226</td><td style=\"text-align: right;\">      50</td><td>(90, 35, 55, 40, 65)</td><td style=\"text-align: right;\">    0.000276466</td><td style=\"text-align: right;\">0.733819</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        37.1201 </td><td style=\"text-align: right;\">      0.72381 </td><td style=\"text-align: right;\">      0.752381</td><td style=\"text-align: right;\">      0.717244</td></tr>\n","<tr><td>train_and_evaluate_fc443e8c</td><td>TERMINATED</td><td>127.0.0.1:35217</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.440892</td><td style=\"text-align: right;\">      20</td><td>(55, 50, 65, 45)    </td><td style=\"text-align: right;\">    0.000281053</td><td style=\"text-align: right;\">0.667152</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.9999 </td><td style=\"text-align: right;\">      0.580952</td><td style=\"text-align: right;\">      0.752381</td><td style=\"text-align: right;\">      0.36747 </td></tr>\n","<tr><td>train_and_evaluate_732e8438</td><td>TERMINATED</td><td>127.0.0.1:35250</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.426349</td><td style=\"text-align: right;\">      40</td><td>(30, 45, 70, 35, 70)</td><td style=\"text-align: right;\">    0.000258669</td><td style=\"text-align: right;\">0.720557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        30.4874 </td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.771429</td><td style=\"text-align: right;\">      0.660633</td></tr>\n","<tr><td>train_and_evaluate_6f97330c</td><td>TERMINATED</td><td>127.0.0.1:35303</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.39071 </td><td style=\"text-align: right;\">      40</td><td>(60, 80, 60, 55, 80)</td><td style=\"text-align: right;\">    0.000100511</td><td style=\"text-align: right;\">0.689955</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        30.6763 </td><td style=\"text-align: right;\">      0.552381</td><td style=\"text-align: right;\">      0.773585</td><td style=\"text-align: right;\">      0.355828</td></tr>\n","<tr><td>train_and_evaluate_ff965344</td><td>TERMINATED</td><td>127.0.0.1:35354</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.410489</td><td style=\"text-align: right;\">      50</td><td>(55, 65, 50, 60, 75)</td><td style=\"text-align: right;\">    0.000960305</td><td style=\"text-align: right;\">0.718598</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        37.0987 </td><td style=\"text-align: right;\">      0.638095</td><td style=\"text-align: right;\">      0.780952</td><td style=\"text-align: right;\">      0.616346</td></tr>\n","<tr><td>train_and_evaluate_c4fb9620</td><td>TERMINATED</td><td>127.0.0.1:35417</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.462795</td><td style=\"text-align: right;\">      50</td><td>(80, 90, 90, 70, 40)</td><td style=\"text-align: right;\">    0.00090664 </td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.7208 </td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_803c4f50</td><td>TERMINATED</td><td>127.0.0.1:35453</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.471251</td><td style=\"text-align: right;\">      50</td><td>(45, 90, 30, 75, 35)</td><td style=\"text-align: right;\">    0.000449356</td><td style=\"text-align: right;\">0.737682</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        37.1607 </td><td style=\"text-align: right;\">      0.695238</td><td style=\"text-align: right;\">      0.790476</td><td style=\"text-align: right;\">      0.693878</td></tr>\n","<tr><td>train_and_evaluate_2d5b005b</td><td>TERMINATED</td><td>127.0.0.1:35518</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.469448</td><td style=\"text-align: right;\">      50</td><td>(45, 70, 45, 90, 75)</td><td style=\"text-align: right;\">    0.000414628</td><td style=\"text-align: right;\">0.730009</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        35.6228 </td><td style=\"text-align: right;\">      0.685714</td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.678243</td></tr>\n","<tr><td>train_and_evaluate_8d426873</td><td>TERMINATED</td><td>127.0.0.1:35576</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.452277</td><td style=\"text-align: right;\">      50</td><td>(75, 50, 75, 65, 70)</td><td style=\"text-align: right;\">    0.000410358</td><td style=\"text-align: right;\">0.743324</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        37.4531 </td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.693647</td></tr>\n","<tr><td>train_and_evaluate_a9a28f88</td><td>TERMINATED</td><td>127.0.0.1:35673</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.463342</td><td style=\"text-align: right;\">      50</td><td>(35, 60, 35, 55, 45)</td><td style=\"text-align: right;\">    0.000163912</td><td style=\"text-align: right;\">0.642749</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        38.8204 </td><td style=\"text-align: right;\">      0.52381 </td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.357576</td></tr>\n","<tr><td>train_and_evaluate_d735bd5d</td><td>TERMINATED</td><td>127.0.0.1:35778</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.402165</td><td style=\"text-align: right;\">      50</td><td>(75, 90, 75, 90, 70)</td><td style=\"text-align: right;\">    0.000159659</td><td style=\"text-align: right;\">0.714861</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        38.3746 </td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.665575</td></tr>\n","<tr><td>train_and_evaluate_b9ad3bbb</td><td>TERMINATED</td><td>127.0.0.1:35873</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.405735</td><td style=\"text-align: right;\">      10</td><td>(60, 30, 50, 35, 80)</td><td style=\"text-align: right;\">    0.00068202 </td><td style=\"text-align: right;\">0.614178</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.0453 </td><td style=\"text-align: right;\">      0.556604</td><td style=\"text-align: right;\">      0.752381</td><td style=\"text-align: right;\">      0.357576</td></tr>\n","<tr><td>train_and_evaluate_4198f2cd</td><td>TERMINATED</td><td>127.0.0.1:35912</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.356723</td><td style=\"text-align: right;\">      20</td><td>(50, 90, 65, 40, 70)</td><td style=\"text-align: right;\">    0.000616634</td><td style=\"text-align: right;\">0.712848</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.2165 </td><td style=\"text-align: right;\">      0.619048</td><td style=\"text-align: right;\">      0.761905</td><td style=\"text-align: right;\">      0.619013</td></tr>\n","<tr><td>train_and_evaluate_2a6e3ee3</td><td>TERMINATED</td><td>127.0.0.1:35972</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.343791</td><td style=\"text-align: right;\">      20</td><td>(30, 35, 85, 45, 80)</td><td style=\"text-align: right;\">    0.000248675</td><td style=\"text-align: right;\">0.608446</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.7799 </td><td style=\"text-align: right;\">      0.514286</td><td style=\"text-align: right;\">      0.714286</td><td style=\"text-align: right;\">      0.398922</td></tr>\n","<tr><td>train_and_evaluate_a3899d82</td><td>TERMINATED</td><td>127.0.0.1:36008</td><td>tanh        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.45699 </td><td style=\"text-align: right;\">      10</td><td>(30, 75, 35, 60, 50)</td><td style=\"text-align: right;\">    0.000247997</td><td style=\"text-align: right;\">0.587511</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.96414</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.742857</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_cda4e09d</td><td>TERMINATED</td><td>127.0.0.1:36045</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.456041</td><td style=\"text-align: right;\">      10</td><td>(35, 30, 90, 30)    </td><td style=\"text-align: right;\">    0.000339324</td><td style=\"text-align: right;\">0.604582</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.37041</td><td style=\"text-align: right;\">      0.542857</td><td style=\"text-align: right;\">      0.704762</td><td style=\"text-align: right;\">      0.44    </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=34469)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A21.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=34469)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A21.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-11 16:44:59,789\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 70, 40, 80, 70)}\n","2024-08-11 16:45:26,176\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 90, 60, 65, 90)}\n","2024-08-11 16:45:46,096\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 75, 80, 75, 90)}\n","2024-08-11 16:46:26,286\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 85, 80, 80, 45)}\n","2024-08-11 16:47:00,590\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 60, 30, 60, 80)}\n","2024-08-11 16:47:15,875\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 65, 30, 80)}\n","2024-08-11 16:47:35,937\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 85, 35, 35, 40)}\n","2024-08-11 16:47:58,582\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 60, 45, 30, 50)}\n","2024-08-11 16:48:38,251\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 50, 40, 50, 75)}\n","2024-08-11 16:48:54,743\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 30, 75, 70, 55)}\n","2024-08-11 16:49:10,296\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 30, 70, 55, 70)}\n","2024-08-11 16:49:49,549\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 35, 55, 55, 85)}\n","2024-08-11 16:50:27,188\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (65, 45, 35, 65)}\n","2024-08-11 16:50:41,062\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 65, 55, 70, 75)}\n","2024-08-11 16:51:20,377\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 40, 45, 50, 45)}\n","2024-08-11 16:52:01,149\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 60, 55, 30, 70)}\n","2024-08-11 16:52:42,511\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 35, 55, 40, 65)}\n","2024-08-11 16:53:03,292\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 50, 65, 45)}\n","2024-08-11 16:53:37,659\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 45, 70, 35, 70)}\n","2024-08-11 16:54:12,100\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 80, 60, 55, 80)}\n","2024-08-11 16:54:52,529\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 65, 50, 60, 75)}\n","2024-08-11 16:55:13,194\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (80, 90, 90, 70, 40)}\n","2024-08-11 16:55:53,655\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 90, 30, 75, 35)}\n","2024-08-11 16:56:33,143\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 70, 45, 90, 75)}\n","2024-08-11 16:57:14,165\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 50, 75, 65, 70)}\n","2024-08-11 16:57:56,756\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 60, 35, 55, 45)}\n","2024-08-11 16:58:39,234\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 90, 75, 90, 70)}\n","2024-08-11 16:58:55,791\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 30, 50, 35, 80)}\n","2024-08-11 16:59:19,125\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 90, 65, 40, 70)}\n","2024-08-11 16:59:33,700\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 35, 85, 45, 80)}\n","2024-08-11 16:59:46,795\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (30, 75, 35, 60, 50)}\n","2024-08-11 16:59:59,911\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 30, 90, 30)}\n","2024-08-11 17:00:00,037\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-11_16-44-44' in 0.1240s.\n","2024-08-11 17:00:00,044\tINFO tune.py:1041 -- Total run time: 916.74 seconds (914.65 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.0004103576794828613, 'batch_size': 32, 'hidden_layers': (75, 50, 75, 65, 70), 'epochs': 50, 'dropout_rate': 0.45227663901390397}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a21_splits, \"A21\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["df_dict_A2 = pd.read_pickle(\"results/output_A2.pkl\")\n","df_dict_A3 = pd.read_pickle(\"results/output_A3.pkl\")\n","df_dict_A4 = pd.read_pickle(\"results/output_A4.pkl\")\n","df_dict_A12 = pd.read_pickle(\"results/output_A12.pkl\")\n","df_dict_A21 = pd.read_pickle(\"results/output_A21.pkl\")\n","\n","df_dict = pd.concat([df_dict_A2, df_dict_A3, df_dict_A4, df_dict_A12, df_dict_A21])\n","\n","df_dict = df_dict.loc[:,~df_dict.columns.duplicated()].copy()\n","df_dict = df_dict.reset_index().rename(columns={'index': 'name'})\n","df_dict = df_dict.drop_duplicates()\n","\n","df_dict.to_csv('outputs/Classic_FFN_one_split_metrics_system.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df_dict_full_A2 = pd.read_pickle(\"results/output_full_A2.pkl\")\n","df_dict_full_A3 = pd.read_pickle(\"results/output_full_A3.pkl\")\n","df_dict_full_A4 = pd.read_pickle(\"results/output_full_A4.pkl\")\n","df_dict_full_A12 = pd.read_pickle(\"results/output_full_A12.pkl\")\n","df_dict_full_A21 = pd.read_pickle(\"results/output_full_A21.pkl\")\n","\n","df_full_dict = pd.concat([df_dict_full_A2, df_dict_full_A3, df_dict_full_A4, df_dict_full_A12, df_dict_full_A21])\n","\n","df_full_dict = df_full_dict.loc[:, ~df_full_dict.columns.duplicated()].copy()\n","df_full_dict = df_full_dict.reset_index().rename(columns={'index': 'name'})\n","df_full_dict = df_full_dict.drop_duplicates()\n","\n","df_full_dict.to_csv(\"outputs/Classic_FFN_full_split_metrics_system.csv\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_dict_full_A2.to_csv(\"outputs/Classic_FFN_full_split_metrics_A2.csv\")\n","df_dict_full_A3.to_csv(\"outputs/Classic_FFN_full_split_metrics_A3.csv\")\n","df_dict_full_A4.to_csv(\"outputs/Classic_FFN_full_split_metrics_A4.csv\")\n","df_dict_full_A12.to_csv(\"outputs/Classic_FFN_full_split_metrics_A12.csv\")\n","df_dict_full_A21.to_csv(\"outputs/Classic_FFN_full_split_metrics_A21.csv\")"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.00018153627532969336, 'batch_size': 64, 'hidden_layers': (90, 90, 65, 55, 40), 'epochs': 30, 'dropout_rate': 0.4941268198413076} is 0.949705780546902\n","Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.00255156012554144, 'batch_size': 32, 'hidden_layers': (85, 75, 40, 80, 75), 'epochs': 10, 'dropout_rate': 0.4796967270268306} is 0.924927536231884\n","Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.003991246077136659, 'batch_size': 128, 'hidden_layers': (35, 90, 50, 40, 45), 'epochs': 40, 'dropout_rate': 0.2412349299577167} is 0.7116299195308766\n","Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.0009752032860232802, 'batch_size': 32, 'hidden_layers': (75, 90, 40, 30, 90), 'epochs': 30, 'dropout_rate': 0.36000954648695677} is 0.3182980546518322\n","Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.0004103576794828613, 'batch_size': 32, 'hidden_layers': (75, 50, 75, 65, 70), 'epochs': 50, 'dropout_rate': 0.45227663901390397} is 0.7433243486073675\n","\n","\n","The best config is {'activation': 'relu', 'learning_rate': 0.00018153627532969336, 'batch_size': 64, 'hidden_layers': (90, 90, 65, 55, 40), 'epochs': 30, 'dropout_rate': 0.4941268198413076} with a mean accuracy of 0.949705780546902\n"]}],"source":["# print the configs and results for the hyperparemters wiht the highest mean accuracy\n","# read df_full_dict and print columns with configs in best_params_list_getting\n","df_full_dict = pd.read_csv(\"outputs/Classic_FFN_full_split_metrics_system.csv\")\n","df_full_dict.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","config_acc_dict = {}\n","for good_param in best_params_list_getting:\n","    # get the row witht the highest mean mean accuracy\n","    mean_accuracy = df_full_dict.loc[df_full_dict['config'] == str(good_param)]['mean_accuracy'].mean()\n","    config_acc_dict[str(good_param)] = mean_accuracy\n","    print(f\"Mean accuracy for config {good_param} is {mean_accuracy}\")\n","\n","# get the best config\n","best_config = max(config_acc_dict, key=config_acc_dict.get)\n","print(f\"\\n\\nThe best config is {best_config} with a mean accuracy of {config_acc_dict[best_config]}\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>number of classes</th>\n","      <th>min_accuracy</th>\n","      <th>max_accuracy</th>\n","      <th>mean_accuracy</th>\n","      <th>min_f1_macro</th>\n","      <th>max_f1_macro</th>\n","      <th>mean_f1_macro</th>\n","      <th>min_f1_micro</th>\n","      <th>max_f1_micro</th>\n","      <th>mean_f1_micro</th>\n","      <th>min_mcc</th>\n","      <th>max_mcc</th>\n","      <th>mean_mcc</th>\n","      <th>std_accuracy</th>\n","      <th>std_f1_macro</th>\n","      <th>std_f1_micro</th>\n","      <th>std_mcc</th>\n","      <th>config</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A2_527500c4</td>\n","      <td>2</td>\n","      <td>0.907407</td>\n","      <td>0.971963</td>\n","      <td>0.949706</td>\n","      <td>0.907121</td>\n","      <td>0.971953</td>\n","      <td>0.949639</td>\n","      <td>0.907407</td>\n","      <td>0.971963</td>\n","      <td>0.949706</td>\n","      <td>0.819892</td>\n","      <td>0.945436</td>\n","      <td>0.901048</td>\n","      <td>0.022256</td>\n","      <td>0.022362</td>\n","      <td>0.022256</td>\n","      <td>0.042945</td>\n","      <td>{'activation': 'relu', 'learning_rate': 0.0001...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A3_96349dd3</td>\n","      <td>2</td>\n","      <td>0.898551</td>\n","      <td>0.956522</td>\n","      <td>0.924928</td>\n","      <td>0.898551</td>\n","      <td>0.956522</td>\n","      <td>0.924858</td>\n","      <td>0.898551</td>\n","      <td>0.956522</td>\n","      <td>0.924928</td>\n","      <td>0.797479</td>\n","      <td>0.916397</td>\n","      <td>0.851265</td>\n","      <td>0.026286</td>\n","      <td>0.026287</td>\n","      <td>0.026286</td>\n","      <td>0.052860</td>\n","      <td>{'activation': 'relu', 'learning_rate': 0.0025...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A4_eb1bde45</td>\n","      <td>3</td>\n","      <td>0.647208</td>\n","      <td>0.753181</td>\n","      <td>0.711630</td>\n","      <td>0.638725</td>\n","      <td>0.752376</td>\n","      <td>0.708532</td>\n","      <td>0.647208</td>\n","      <td>0.753181</td>\n","      <td>0.711630</td>\n","      <td>0.490295</td>\n","      <td>0.631115</td>\n","      <td>0.572734</td>\n","      <td>0.039143</td>\n","      <td>0.041674</td>\n","      <td>0.039143</td>\n","      <td>0.052306</td>\n","      <td>{'activation': 'relu', 'learning_rate': 0.0039...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A12_54dc37f5</td>\n","      <td>6</td>\n","      <td>0.266839</td>\n","      <td>0.351421</td>\n","      <td>0.318298</td>\n","      <td>0.245337</td>\n","      <td>0.325706</td>\n","      <td>0.302332</td>\n","      <td>0.266839</td>\n","      <td>0.351421</td>\n","      <td>0.318298</td>\n","      <td>0.121942</td>\n","      <td>0.225464</td>\n","      <td>0.184049</td>\n","      <td>0.028498</td>\n","      <td>0.029590</td>\n","      <td>0.028498</td>\n","      <td>0.034524</td>\n","      <td>{'activation': 'tanh', 'learning_rate': 0.0009...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A21_8d426873</td>\n","      <td>2</td>\n","      <td>0.704762</td>\n","      <td>0.761905</td>\n","      <td>0.743324</td>\n","      <td>0.693647</td>\n","      <td>0.761125</td>\n","      <td>0.726623</td>\n","      <td>0.704762</td>\n","      <td>0.761905</td>\n","      <td>0.743324</td>\n","      <td>0.398184</td>\n","      <td>0.525728</td>\n","      <td>0.468759</td>\n","      <td>0.020220</td>\n","      <td>0.025642</td>\n","      <td>0.020220</td>\n","      <td>0.047783</td>\n","      <td>{'activation': 'relu', 'learning_rate': 0.0004...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           name  number of classes  min_accuracy  max_accuracy  mean_accuracy  \\\n","0   A2_527500c4                  2      0.907407      0.971963       0.949706   \n","1   A3_96349dd3                  2      0.898551      0.956522       0.924928   \n","2   A4_eb1bde45                  3      0.647208      0.753181       0.711630   \n","3  A12_54dc37f5                  6      0.266839      0.351421       0.318298   \n","4  A21_8d426873                  2      0.704762      0.761905       0.743324   \n","\n","   min_f1_macro  max_f1_macro  mean_f1_macro  min_f1_micro  max_f1_micro  \\\n","0      0.907121      0.971953       0.949639      0.907407      0.971963   \n","1      0.898551      0.956522       0.924858      0.898551      0.956522   \n","2      0.638725      0.752376       0.708532      0.647208      0.753181   \n","3      0.245337      0.325706       0.302332      0.266839      0.351421   \n","4      0.693647      0.761125       0.726623      0.704762      0.761905   \n","\n","   mean_f1_micro   min_mcc   max_mcc  mean_mcc  std_accuracy  std_f1_macro  \\\n","0       0.949706  0.819892  0.945436  0.901048      0.022256      0.022362   \n","1       0.924928  0.797479  0.916397  0.851265      0.026286      0.026287   \n","2       0.711630  0.490295  0.631115  0.572734      0.039143      0.041674   \n","3       0.318298  0.121942  0.225464  0.184049      0.028498      0.029590   \n","4       0.743324  0.398184  0.525728  0.468759      0.020220      0.025642   \n","\n","   std_f1_micro   std_mcc                                             config  \n","0      0.022256  0.042945  {'activation': 'relu', 'learning_rate': 0.0001...  \n","1      0.026286  0.052860  {'activation': 'relu', 'learning_rate': 0.0025...  \n","2      0.039143  0.052306  {'activation': 'relu', 'learning_rate': 0.0039...  \n","3      0.028498  0.034524  {'activation': 'tanh', 'learning_rate': 0.0009...  \n","4      0.020220  0.047783  {'activation': 'relu', 'learning_rate': 0.0004...  "]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["best_performers = df_full_dict.loc[df_full_dict.groupby(df_full_dict['name'].str.split('_').str[0])['mean_accuracy'].idxmax()]\n","best_performers = best_performers.sort_index().reset_index(drop=True)\n","\n","best_performers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5354302,"sourceId":8905505,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
