{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Data Files Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import clear_folder\n",
    "\n",
    "clear_folder(\"ray_results\")\n",
    "clear_folder(\"outputs\")\n",
    "clear_folder(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated stratified nested Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code performs the repeated stratified nested cross validation according to Krstajic et al. (2014).\n",
    "- 10 repitions\n",
    "- 5 outer loops\n",
    "- 4 inner loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean, stdev\n",
    "import logging\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def run_nested_cv(\n",
    "    data_path,\n",
    "    output_path,\n",
    "    outer_folds=5,\n",
    "    inner_folds=4,\n",
    "    repeats=10,\n",
    "    param_grid={\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "):\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    # Data import\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = df.drop(columns=['track'])\n",
    "    y = df['track']\n",
    "\n",
    "    # Result container\n",
    "    nested_accuracies = []\n",
    "    best_params_per_fold = []\n",
    "    inner_cv_scores_per_fold = []\n",
    "\n",
    "    # Outer Loop (repitions)\n",
    "    for rep in range(repeats):\n",
    "        logger.info(f\"Repetition {rep + 1}/{repeats}\")\n",
    "        outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=rep)\n",
    "\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "            logger.info(f\"  Outer Fold {fold_idx + 1}/{outer_folds}\")\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=rep)\n",
    "            grid_search = GridSearchCV(\n",
    "                SVC(),\n",
    "                param_grid,\n",
    "                cv=inner_cv,\n",
    "                scoring='accuracy',\n",
    "                return_train_score=False,\n",
    "                n_jobs=1  # n_jobs sets the number of CPU cores used in parallel; -1 means using all available cores\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            nested_accuracies.append(acc)\n",
    "\n",
    "            logger.info(f\"    Best params: {grid_search.best_params_}\")\n",
    "            logger.info(f\"    Nested CV accuracy: {acc:.4f}\")\n",
    "\n",
    "            best_params_per_fold.append(grid_search.best_params_)\n",
    "\n",
    "            inner_scores = grid_search.cv_results_['mean_test_score']\n",
    "            inner_cv_scores_per_fold.append({\n",
    "                'mean': float(np.mean(inner_scores)),\n",
    "                'std': float(np.std(inner_scores)),\n",
    "                'min': float(np.min(inner_scores)),\n",
    "                'max': float(np.max(inner_scores))\n",
    "            })\n",
    "\n",
    "    # Summary\n",
    "    summary = {\n",
    "        'mean_nested_accuracy': mean(nested_accuracies),\n",
    "        'sd_nested_accuracy': stdev(nested_accuracies),\n",
    "        'min_nested_accuracy': min(nested_accuracies),\n",
    "        'max_nested_accuracy': max(nested_accuracies),\n",
    "        'nested_accuracies': nested_accuracies,\n",
    "        'best_params_per_fold': best_params_per_fold,\n",
    "        'inner_cv_scores_per_fold': inner_cv_scores_per_fold\n",
    "    }\n",
    "\n",
    "    # Create results folder\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Save results\n",
    "    df_inner = pd.DataFrame(summary['inner_cv_scores_per_fold'])\n",
    "    df_inner.to_csv(os.path.join(output_path, \"inner_cv_scores_per_fold.csv\"), index=False)\n",
    "\n",
    "    df_summary = pd.DataFrame([{\n",
    "        'mean_nested_accuracy': summary['mean_nested_accuracy'],\n",
    "        'sd_nested_accuracy': summary['sd_nested_accuracy'],\n",
    "        'min_nested_accuracy': summary['min_nested_accuracy'],\n",
    "        'max_nested_accuracy': summary['max_nested_accuracy']\n",
    "    }])\n",
    "    df_summary.to_csv(os.path.join(output_path, \"nested_cv_accuracy_summary.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nNested CV Accuracy (gesamt):\")\n",
    "    print(f\"Mean: {summary['mean_nested_accuracy']:.4f}, \"\n",
    "          f\"SD: {summary['sd_nested_accuracy']:.4f}, \"\n",
    "          f\"Min: {summary['min_nested_accuracy']:.4f}, \"\n",
    "          f\"Max: {summary['max_nested_accuracy']:.4f}\")\n",
    "\n",
    "    # Visualisation\n",
    "    param_combos = [f\"C={p['C']}, kernel={p['kernel']}, gamma={p['gamma']}\" for p in best_params_per_fold]\n",
    "    combo_counts = Counter(param_combos)\n",
    "    df_combos = pd.DataFrame(combo_counts.items(), columns=['Parameter Combination', 'Frequency'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(y='Parameter Combination', x='Frequency', data=df_combos.sort_values('Frequency', ascending=False))\n",
    "    plt.title('Frequency of Best Hyperparameter Combinations')\n",
    "    plt.xlabel('Number of Occurrences')\n",
    "    plt.ylabel('Hyperparameter Combination')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"best_hyperparameter_combinations.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Basic path for data and results\n",
    "base_data_path = os.path.join(\"data\", \"normalized\")\n",
    "base_output_path = os.path.join(\"results\")\n",
    "\n",
    "# List of data sets\n",
    "datasets = [\"A2\", \"A3\", \"A4\", \"A12\", \"A21\"] # select the data sets to be analyzed\n",
    "\n",
    "# Loop over all data sets\n",
    "for name in datasets:\n",
    "    data_path = os.path.join(base_data_path, f\"{name}.csv\")\n",
    "    output_path = os.path.join(base_output_path, f\"results_{name}\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    run_nested_cv(\n",
    "        data_path=data_path,\n",
    "        output_path=output_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Up Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model configurations that were most frequently the best model per data set in the repeated stratified nested cross validation:\n",
    "\n",
    "- **Data set A2** (2 classes/tracks)  \n",
    "  - `C = 100`, `kernel = rbf`, `gamma = auto`\n",
    "\n",
    "- **Data set A3** (2 classes/tracks)  \n",
    "  - `C = 10`, `kernel = rbf`, `gamma = scale`\n",
    "\n",
    "- **Data set A4** (3 classes/tracks)  \n",
    "  - `C = 1`, `kernel = rbf`, `gamma = 1`\n",
    "\n",
    "- **Data set A12** (6 classes/tracks)  \n",
    "  - `C = 10`, `kernel = rbf`, `gamma = scale`\n",
    "\n",
    "- **Data set A21** (2 classes/tracks)  \n",
    "  - `C = 100`, `kernel = rbf`, `gamma = auto`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \"evaluate models on all data sets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def evaluate_model_on_dataframes(param_grid, dataframes):\n",
    "    \"\"\"\n",
    "    Evaluates a fixed SVM model configuration on multiple pandas DataFrames using 5-fold cross-validation.\n",
    "\n",
    "    Each DataFrame must have a 'track' column as the target.\n",
    "\n",
    "    Args:\n",
    "        param_grid (dict): Dict with keys 'C', 'kernel', and 'gamma' specifying the model parameters.\n",
    "        dataframes (list): List of pandas DataFrames, each representing one dataset.\n",
    "\n",
    "    Returns:\n",
    "        List of mean accuracy values for each dataset (in the same order).\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    for idx, df in enumerate(dataframes):\n",
    "        print(f\"\\nEvaluating dataset {idx + 1}/{len(dataframes)}...\")\n",
    "\n",
    "        X = df.drop(columns=['track'])\n",
    "        y = df['track']\n",
    "\n",
    "        model = SVC(C=param_grid['C'],\n",
    "                    kernel=param_grid['kernel'],\n",
    "                    gamma=param_grid['gamma'])\n",
    "\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "        mean_accuracy = np.mean(scores)\n",
    "\n",
    "        print(f\"→ Accuracy (mean over 5 folds): {mean_accuracy:.4f}\")\n",
    "        accuracies.append(mean_accuracy)\n",
    "\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrames\n",
    "df_a2 = pd.read_csv(\"data/normalized/A2.csv\")\n",
    "df_a3 = pd.read_csv(\"data/normalized/A3.csv\")\n",
    "df_a4 = pd.read_csv(\"data/normalized/A4.csv\")\n",
    "df_a12 = pd.read_csv(\"data/normalized/A12.csv\")\n",
    "df_a21 = pd.read_csv(\"data/normalized/A21.csv\")\n",
    "\n",
    "# param grids\n",
    "param_grid_A2 = {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'}\n",
    "param_grid_A3 = {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "param_grid_A4 = {'C': 1, 'kernel': 'rbf', 'gamma': 1}\n",
    "param_grid_A12 = {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "param_grid_A21 = {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'}\n",
    "\n",
    "\n",
    "# List of DataFrames\n",
    "datasets = [df_a2, df_a3, df_a4, df_a12, df_a21]\n",
    "\n",
    "# Start evaluation\n",
    "results_model_A2 = evaluate_model_on_dataframes(param_grid_A2, datasets)\n",
    "results_model_A3 = evaluate_model_on_dataframes(param_grid_A3, datasets)\n",
    "results_model_A4 = evaluate_model_on_dataframes(param_grid_A4, datasets)\n",
    "results_model_A12 = evaluate_model_on_dataframes(param_grid_A12, datasets)\n",
    "results_model_A21 = evaluate_model_on_dataframes(param_grid_A21, datasets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical representation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "# Model names\n",
    "model_names = ['Model A2', 'Model A3', 'Model A4', 'Model A12', 'Model A21']\n",
    "results_lists = [results_model_A2, results_model_A3, results_model_A4, results_model_A12, results_model_A21]\n",
    "\n",
    "# New labels for x-axis\n",
    "new_model_labels = [f'Best HP configuration for {name}' for name in model_names]\n",
    "label_map = dict(zip(model_names, new_model_labels))\n",
    "\n",
    "# create DataFrame\n",
    "data = []\n",
    "for model_name, scores in zip(model_names, results_lists):\n",
    "    for idx, score in enumerate(scores):\n",
    "        data.append({'Model': model_name, 'Accuracy': score, 'Dataset Index': f'Dataset {idx+1}'})\n",
    "\n",
    "df_plot = pd.DataFrame(data)\n",
    "df_plot['Model'] = df_plot['Model'].map(label_map)\n",
    "\n",
    "# create Plot \n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Boxplots\n",
    "sns.boxplot(data=df_plot, x='Model', y='Accuracy', showfliers=False,\n",
    "            boxprops=dict(facecolor='white', edgecolor='black'),\n",
    "            medianprops=dict(color='black'),\n",
    "            whiskerprops=dict(color='black'),\n",
    "            capprops=dict(color='black'))\n",
    "\n",
    "# Stripplot\n",
    "palette = sns.color_palette(n_colors=len(results_lists))\n",
    "strip = sns.stripplot(data=df_plot, x='Model', y='Accuracy', hue='Dataset Index',\n",
    "                      jitter=True, dodge=True, marker='o', linewidth=1, edgecolor='gray')\n",
    "\n",
    "# Legend labels\n",
    "legend_labels = [\n",
    "    \"Data A2: 2 classes\",\n",
    "    \"Data A3: 2 classes\",\n",
    "    \"Data A4: 3 classes\",\n",
    "    \"Data A12: 6 classes\",\n",
    "    \"Data A21: 2 classes\"\n",
    "]\n",
    "\n",
    "# configure legend\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w', label=label,\n",
    "           markerfacecolor=palette[i], markeredgecolor='gray', markersize=8)\n",
    "    for i, label in enumerate(legend_labels)\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_handles, title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title('Accuracy Scores of Different Models Across Datasets')\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# save\n",
    "plt.savefig(os.path.join(\"results\", \"follow_up_analysis_plot.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular presentation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# results per model\n",
    "model_results = {\n",
    "    \"Model A2\": results_model_A2,\n",
    "    \"Model A3\": results_model_A3,\n",
    "    \"Model A4\": results_model_A4,\n",
    "    \"Model A12\": results_model_A12,\n",
    "    \"Model A21\": results_model_A21\n",
    "}\n",
    "\n",
    "# SVM-Parameters per model\n",
    "param_configs = {\n",
    "    \"Model A2\": {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'},\n",
    "    \"Model A3\": {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'},\n",
    "    \"Model A4\": {'C': 1, 'kernel': 'rbf', 'gamma': 1},\n",
    "    \"Model A12\": {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'},\n",
    "    \"Model A21\": {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'}\n",
    "}\n",
    "\n",
    "# Dataset names\n",
    "dataset_names = [\"A2\", \"A3\", \"A4\", \"A12\", \"A21\"]\n",
    "\n",
    "# Restructure matrix\n",
    "accuracy_matrix = pd.DataFrame({\n",
    "    model: scores for model, scores in model_results.items()\n",
    "}, index=dataset_names)\n",
    "\n",
    "# Initialize table\n",
    "summary_data = {\n",
    "    \"Dataset\": [],\n",
    "    \"Winning Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"C\": [],\n",
    "    \"Kernel\": [],\n",
    "    \"Gamma\": []\n",
    "}\n",
    "\n",
    "#  Determine the model with the best accuracy for each dataset\n",
    "for dataset in dataset_names:\n",
    "    best_model = accuracy_matrix.loc[dataset].idxmax()\n",
    "    best_accuracy = accuracy_matrix.loc[dataset].max()\n",
    "    params = param_configs[best_model]\n",
    "    \n",
    "    summary_data[\"Dataset\"].append(dataset)\n",
    "    summary_data[\"Winning Model\"].append(best_model)\n",
    "    summary_data[\"Accuracy\"].append(round(best_accuracy, 3))\n",
    "    summary_data[\"C\"].append(params[\"C\"])\n",
    "    summary_data[\"Kernel\"].append(params[\"kernel\"])\n",
    "    summary_data[\"Gamma\"].append(params[\"gamma\"])\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save and show table\n",
    "fig, ax = plt.subplots(figsize=(10, 2.5))\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_summary.values,\n",
    "                 colLabels=df_summary.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "\n",
    "table.scale(1, 2)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "\n",
    "plt.title(\"Winning Hyperparameter Configuration per Dataset\", pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(\"results\", \"winning_hyperparameters_per_dataset.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis\n",
    "For each dataset, a SVM with 5-fold CV was trained and evaluated using the winning hyperparameter configuration from the previous analysis. The evaluation included the calculation of key classification metrics, namely accuracy, micro F1 score, macro F1 score, and the Matthews Correlation Coefficient (MCC). For each of these metrics, the mean and standard deviation is provided.\n",
    "\n",
    "Data Set A2 (2 classes)\n",
    "- **Hyperparameter:** `C = 1`, `kernel = rbf`, `gamma = 1`\n",
    "\n",
    "Data Set A3 (2 classes)\n",
    "- **Hyperparameter:** `C = 1`, `kernel = rbf`, `gamma = 1`\n",
    "\n",
    " Data Set A4 (3 classes)\n",
    "- **Hyperparameter:** `C = 10`, `kernel = rbf`, `gamma = scale`\n",
    "\n",
    "Data Set A12 (6 classes)\n",
    "- **Hyperparameter:** `C = 10`, `kernel = rbf`, `gamma = scale`\n",
    "\n",
    "Data Set A21 (2 classes)\n",
    "- **Hyperparameter:** `C = 100`, `kernel = rbf`, `gamma = auto`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# load data\n",
    "df_a2 = pd.read_csv(\"data/normalized/A2.csv\")\n",
    "df_a3 = pd.read_csv(\"data/normalized/A3.csv\")\n",
    "df_a4 = pd.read_csv(\"data/normalized/A4.csv\")\n",
    "df_a12 = pd.read_csv(\"data/normalized/A12.csv\")\n",
    "df_a21 = pd.read_csv(\"data/normalized/A21.csv\")\n",
    "\n",
    "# define Data sets and hyperparameters\n",
    "datasets = {\n",
    "    \"A2\": (df_a2, {'C': 1, 'kernel': 'rbf', 'gamma': 1}),\n",
    "    \"A3\": (df_a3, {'C': 1, 'kernel': 'rbf', 'gamma': 1}),\n",
    "    \"A4\": (df_a4, {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}),\n",
    "    \"A12\": (df_a12, {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}),\n",
    "    \"A21\": (df_a21, {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'}),\n",
    "}\n",
    "\n",
    "results = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, (df, params) in datasets.items():\n",
    "    logging.info(f\"Process data set {name} with parameters {params}\")\n",
    "    \n",
    "    X = df.drop(columns=['track']).values\n",
    "    y = df['track'].values\n",
    "    \n",
    "    model = SVC(**params, random_state=42)\n",
    "\n",
    "    acc_scores = []\n",
    "    micro_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    mcc_scores = []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        y_true = y[test_idx]\n",
    "\n",
    "        acc_scores.append(accuracy_score(y_true, y_pred))\n",
    "        micro_f1_scores.append(f1_score(y_true, y_pred, average='micro'))\n",
    "        macro_f1_scores.append(f1_score(y_true, y_pred, average='macro'))\n",
    "        mcc_scores.append(matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    def fmt(mean, std):\n",
    "        return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "    results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Accuracy\": fmt(np.mean(acc_scores), np.std(acc_scores, ddof=1)),\n",
    "        \"Micro F1\": fmt(np.mean(micro_f1_scores), np.std(micro_f1_scores, ddof=1)),\n",
    "        \"Macro F1\": fmt(np.mean(macro_f1_scores), np.std(macro_f1_scores, ddof=1)),\n",
    "        \"MCC\": fmt(np.mean(mcc_scores), np.std(mcc_scores, ddof=1))\n",
    "    })\n",
    "\n",
    "# Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSVM Evaluation Results:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(\"results/svm_evaluation_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular presentation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(10, len(results_df) * 0.6 + 1))\n",
    "ax.axis('off')\n",
    "\n",
    "# create table\n",
    "table = ax.table(cellText=results_df.values,\n",
    "                 colLabels=results_df.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "\n",
    "# styling\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# save and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/svm_evaluation_results.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
