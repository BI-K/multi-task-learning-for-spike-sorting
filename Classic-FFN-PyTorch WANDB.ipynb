{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Prep"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T10:30:03.334868Z","iopub.status.busy":"2024-07-10T10:30:03.334549Z","iopub.status.idle":"2024-07-10T10:30:13.163038Z","shell.execute_reply":"2024-07-10T10:30:13.161739Z","shell.execute_reply.started":"2024-07-10T10:30:03.334833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-15 10:43:10,823\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"]},{"name":"stdout","output_type":"stream","text":["Using device: mps\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import wandb\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import f1_score, matthews_corrcoef\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import psutil\n","import time\n","import logging\n","from datetime import datetime\n","import random\n","import os\n","import shutil\n","import itertools\n","\n","import ray\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.air import session\n","from ray.tune.integration.keras import TuneReportCallback\n","from ray.tune.search.optuna import OptunaSearch\n","from ray.tune.tuner import Tuner, TuneConfig\n","from ray.train import RunConfig\n","from ray.tune import Trainable\n","from ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n","\n","import helper\n","\n","# Initialize Ray\n","ray.init(ignore_reinit_error=True)\n","\n","# Check if MPS (Apple Silicon) or CUDA (Nvidia) GPU is available\n","if torch.backends.mps.is_available():\n","    device = torch.device('mps')\n","elif torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Existing Data Files Check "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def clear_folder(folder_path):\n","    for item in os.listdir(folder_path):\n","        item_path = os.path.join(folder_path, item)\n","        if item == '.gitignore':\n","            continue\n","        if os.path.isfile(item_path):\n","            os.remove(item_path)\n","        elif os.path.isdir(item_path):\n","            shutil.rmtree(item_path)\n","\n","clear_folder(\"ray_results\")\n","clear_folder(\"outputs\")\n","clear_folder(\"results\")"]},{"cell_type":"markdown","metadata":{},"source":["## Simple FNN"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class Classic_FFN(nn.Module):\n","    def __init__(self, num_classes, hidden_layers=[30, 15], dropout_rate=0.5, activation='relu'):\n","        super(Classic_FFN, self).__init__()\n","\n","        self.activation = activation\n","\n","        self.shared_layers = nn.ModuleList()\n","\n","        input_dim = 30\n","\n","        for hidden_layer in hidden_layers:\n","            self.shared_layers.append(nn.Linear(input_dim, hidden_layer))\n","            self.shared_layers.append(nn.Dropout(dropout_rate))\n","            input_dim = hidden_layer\n","\n","        self.output_layer = nn.Linear(input_dim, num_classes)\n","\n","    def forward(self, x):\n","        for layer in self.shared_layers:\n","            if isinstance(layer, nn.Linear):\n","                x = layer(x)\n","                x = self.apply_activation(x)\n","            elif isinstance(layer, nn.Dropout):\n","                x = layer(x)\n","            \n","        x = self.output_layer(x)\n","        return x\n","\n","    def apply_activation(self, x):\n","        if self.activation == 'relu':\n","            return F.relu(x)\n","        elif self.activation == 'tanh':\n","            return torch.tanh(x)\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","        else:\n","            raise ValueError(f'Invalid activation: {self.activation}')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_model(num_classes, hidden_layers=[30,15], dropout_rate=0.5, activation='relu'):\n","    model = Classic_FFN(num_classes=num_classes, hidden_layers=hidden_layers, dropout_rate=dropout_rate, activation=activation)\n","    model.to(device)  # Move the model to the selected device\n","    return model\n","\n","# def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n","#     # Convert data to PyTorch tensors and move to the selected device\n","#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n","#     y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long).to(device)  # Convert one-hot to class indices\n","\n","#     # Create a DataLoader for the training data\n","#     train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","#     train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    \n","#     # Define loss function and optimizer\n","#     criterion = nn.CrossEntropyLoss()\n","#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    \n","#     # Training loop\n","#     for epoch in range(num_epochs):\n","#         model.train()  # Set the model to training mode\n","#         for X_batch, y_batch in train_loader:\n","#             optimizer.zero_grad()  # Clear the gradients\n","#             outputs = model(X_batch)  # Forward pass\n","#             loss = criterion(outputs, y_batch)  # Calculate loss\n","#             loss.backward()  # Backward pass\n","#             optimizer.step()  # Update weights\n","    \n","#     return model\n","\n","def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n","    # Convert data to PyTorch tensors and move to the selected device\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long).to(device)  # Convert one-hot to class indices\n","\n","    # Create a DataLoader for the training data\n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    \n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    \n","    # Custom run name using trial ID and other config parameters\n","    run_name = f\"trial_{session.get_trial_id()}\"\n","    \n","    # Initialize WandB with custom run name\n","    wandb.init(project=\"Classic FFN PyTorch - Train Loss Logging\", reinit=True, name=run_name)\n","    \n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        running_loss = 0.0\n","        for X_batch, y_batch in train_loader:\n","            optimizer.zero_grad()  # Clear the gradients\n","            outputs = model(X_batch)  # Forward pass\n","            loss = criterion(outputs, y_batch)  # Calculate loss\n","            loss.backward()  # Backward pass\n","            optimizer.step()  # Update weights\n","            \n","            # If dataset is A12 then we take running loss += x * loss.item()\n","            running_loss += loss.item()\n","\n","        # Calculate average loss over the epoch\n","        avg_loss = running_loss / len(train_loader)\n","        \n","        # Log the loss and epoch to WandB\n","        wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n","    \n","    return model\n","\n","\n","def evaluate(model, X_test, y_test):\n","    # Convert data to PyTorch tensors and move to the selected device\n","    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n","    \n","    # Set model to evaluation mode\n","    model.eval()\n","    \n","    # Disable gradient calculation for inference\n","    with torch.no_grad():\n","        outputs = model(X_test_tensor)  # Forward pass\n","        y_pred_classes = torch.argmax(outputs, dim=1).cpu().numpy()  # Get predicted classes and move to CPU\n","        y_test_classes = torch.argmax(y_test_tensor, dim=1).cpu().numpy()  # Get true classes and move to CPU\n","        \n","        # Calculate accuracy\n","        accuracy = (y_pred_classes == y_test_classes).mean()\n","        \n","        # Calculate F1 scores and MCC\n","        f1_macro = f1_score(y_test_classes, y_pred_classes, average='macro')\n","        f1_micro = f1_score(y_test_classes, y_pred_classes, average='micro')\n","        mcc = matthews_corrcoef(y_test_classes, y_pred_classes)\n","    \n","    return accuracy, f1_macro, f1_micro, mcc"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def evaluate_model_on_dataset_one_split(config, split, split_index):\n","    test_set = split[split_index]\n","    indices = [0, 1, 2, 3, 4]\n","    indices.remove(split_index)\n","    train_splits = [split[i] for i in indices]\n","    train_set = pd.concat(train_splits, axis=0)\n","\n","    classes = train_set['track'].unique()\n","    num_classes = len(classes)\n","    #print(\"number of classes: \" + str(num_classes))\n","\n","    one_hot_columns = train_set['track'].unique()\n","    one_hot = pd.get_dummies(train_set['track'])\n","    train_set = train_set.drop('track', axis=1)\n","    train_set = train_set.join(one_hot).astype(float)\n","\n","    one_hot_columns = test_set['track'].unique()\n","    one_hot = pd.get_dummies(test_set['track'])\n","    test_set = test_set.drop('track', axis=1)\n","    test_set = test_set.join(one_hot).astype(float)\n","\n","    X_train = train_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_train = train_set[one_hot_columns].values.reshape(-1, num_classes)\n","    X_test = test_set.drop(columns=one_hot_columns).values.reshape(-1, 30)\n","    y_test = test_set[one_hot_columns].values.reshape(-1, num_classes)\n","\n","    model = create_model(num_classes, activation=config['activation'], hidden_layers=config['hidden_layers'], dropout_rate=config['dropout_rate'])\n","\n","    # Train the model and collect performance data\n","    model= train(model=model, X_train=X_train, y_train=y_train, num_epochs=config['epochs'], batch_size=config['batch_size'], learning_rate=config['learning_rate'])\n","    # Evaluate the model and collect performance data\n","    accuracy, f1_macro, f1_micro, mcc = evaluate(model=model, X_test=X_test, y_test=y_test)\n","\n","\n","    return accuracy, f1_macro, f1_micro, mcc, num_classes"]},{"cell_type":"markdown","metadata":{},"source":["## 5-Fold-Cross Validation"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df_data_spike_1_split = pd.DataFrame()\n","df_data_spike_full_split = pd.DataFrame()\n","\n","data_spike_exec_1_split_dict = dict()\n","data_spike_exec_full_split_dict = dict()\n","\n","\n","best_params_list_getting = []\n","\n","def custom_trial_dirname(trial):\n","    return f\"trial_{trial.trial_id}\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def train_and_evaluate(config, splits, splits_name):\n","    \n","    global data_spike_exec_1_split_dict\n","    global data_spike_exec_full_split_dict\n","    global df_data_spike_1_split\n","    global results_dir\n","    \n","    min_accuracy = 1\n","    max_accuracy = 0\n","    avg_accuracy = 0\n","    min_f1_macro = 1\n","    max_f1_macro = 0\n","    avg_f1_macro = 0\n","    min_f1_micro = 1\n","    max_f1_micro = 0\n","    avg_f1_micro = 0\n","    min_mcc = 1\n","    max_mcc = -1\n","    avg_mcc = 0\n","\n","    num_classes = 0\n","    \n","    accuracies = []\n","    f1_macro_scores = []\n","    f1_micro_scores = []\n","    mcc_scores = []\n","\n","    session_id_for_df = session.get_trial_id()\n","    \n","    for i in range(5):\n","        accuracy, f1_macro, f1_micro, mcc, num_classes = evaluate_model_on_dataset_one_split(config, splits, i)\n","        accuracies.append(accuracy)\n","        f1_macro_scores.append(f1_macro)\n","        f1_micro_scores.append(f1_micro)\n","        mcc_scores.append(mcc)\n","        \n","        avg_accuracy += accuracy\n","        avg_f1_macro += f1_macro\n","        avg_f1_micro += f1_micro\n","        min_accuracy = min(min_accuracy, accuracy)\n","        max_accuracy = max(max_accuracy, accuracy)\n","        min_f1_macro = min(min_f1_macro, f1_macro)\n","        max_f1_macro = max(max_f1_macro, f1_macro)\n","        min_f1_micro = min(min_f1_micro, f1_micro)\n","        max_f1_micro = max(max_f1_micro, f1_micro)\n","        avg_mcc += mcc\n","        min_mcc = min(min_mcc, mcc)\n","        max_mcc = max(max_mcc, mcc)\n","\n","\n","        \n","        data_spike_exec_1_split_dict[splits_name + \"_\" + str(i+1) + \"_\" + session_id_for_df] = {\n","            \"number of classes\": num_classes,\n","            \"accuracy\": accuracy,\n","            \"macro f1\": f1_macro,\n","            \"micro_f1\": f1_micro,\n","            \"mcc\": mcc,\n","            \"config\": str(config)\n","        }\n","        \n","       \n","    temp_df = pd.DataFrame.from_dict(data_spike_exec_1_split_dict, orient='index')\n","    df_data_spike_1_split = pd.concat([df_data_spike_1_split, temp_df], axis=0)\n","\n","        # df_data_spike_1_split = pd.concat([df_data_spike_1_split, pd.DataFrame.from_dict(data_spike_exec_1_split_dict)], axis=1)\n","\n","        \n","    avg_accuracy /= 5\n","    avg_f1_macro /= 5\n","    avg_f1_micro /= 5\n","    avg_mcc /= 5\n","    \n","    data_spike_exec_full_split_dict[splits_name + \"_\" + session_id_for_df] = {\n","        \"number of classes\": num_classes,\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"mean_f1_micro\": np.mean(f1_micro_scores),\n","        \"min_mcc\": min_mcc,\n","        \"max_mcc\": max_mcc,\n","        \"mean_mcc\": np.mean(mcc_scores),\n","        \"std_accuracy\": np.std(accuracies),\n","        \"std_f1_macro\": np.std(f1_macro_scores),\n","        \"std_f1_micro\": np.std(f1_micro_scores),\n","        \"std_mcc\": np.std(mcc_scores),\n","        \"config\": str(config)\n","    }\n","\n","    # print(f\"Mean accuracy of the model {splits_name + '_' + session_id_for_df}: {avg_accuracy}\")\n","\n","    df_data_spike_full_split = pd.DataFrame.from_dict(data_spike_exec_full_split_dict, orient='index')\n","\n","    # Load existing data from file if it exists and append new data\n","    full_split_path = os.path.join(helper.results_dir, f'output_full_{splits_name}.pkl')\n","    if os.path.exists(full_split_path):\n","        df_existing_full = pd.read_pickle(full_split_path)\n","        df_data_spike_full_split = pd.concat([df_existing_full, df_data_spike_full_split], axis=0)\n","    else:\n","        print(f\"No existing full split data found at {full_split_path}, creating new file.\")\n","    \n","    # Save the updated full split data to file\n","    df_data_spike_full_split.to_pickle(full_split_path)\n","\n","    # Save the 1 split data using the full path\n","    split_path = os.path.join(helper.results_dir, f'output_{splits_name}.pkl')\n","    if os.path.exists(split_path):\n","        df_existing_1_spike = pd.read_pickle(split_path)\n","        df_data_spike_1_split = pd.concat([df_existing_1_spike, df_data_spike_1_split], axis=0)\n","    else:\n","        print(f\"No existing 1 split data found at {split_path}, creating new file\")\n","\n","    # Save the updated 1 split data to file\n","    df_data_spike_1_split.to_pickle(split_path)\n","\n","    time.sleep(5)\n","    \n","    session.report({\n","        \"min_accuracy\": min_accuracy,\n","        \"max_accuracy\": max_accuracy,\n","        \"mean_accuracy\": np.mean(accuracies),\n","        \"min_f1_macro\": min_f1_macro,\n","        \"max_f1_macro\": max_f1_macro,\n","        \"mean_f1_macro\": np.mean(f1_macro_scores),\n","        \"min_f1_micro\": min_f1_micro,\n","        \"max_f1_micro\": max_f1_micro,\n","        \"mean_f1_micro\": np.mean(f1_micro_scores)\n","    })"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def generate_hidden_layers_config(min_layers=2, max_layers=5, min_nodes=30, max_nodes=90, step=5):\n","    possible_layers = []\n","    for num_layers in range(min_layers, max_layers + 1):\n","        layer_configurations = list(itertools.product(range(min_nodes, max_nodes + 1, step), repeat=num_layers))\n","        possible_layers.extend(layer_configurations)\n","    return possible_layers\n","\n","def five_fold_cross_validation(splits, splits_name):\n","\n","    global best_params_list_getting\n","\n","    hidden_layers_options = generate_hidden_layers_config()\n","\n","    config = {\n","        \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\"]),\n","        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n","        \"batch_size\": tune.choice([32, 64, 128]),\n","        \"hidden_layers\": tune.choice(hidden_layers_options),\n","        \"epochs\": tune.choice([10, 20, 30, 40, 50]),\n","        \"dropout_rate\": tune.uniform(0.2, 0.5)\n","    }\n","    \n","    scheduler = ASHAScheduler(\n","        metric=\"mean_accuracy\",\n","        mode=\"max\",\n","        max_t=10,\n","        grace_period=1,\n","        reduction_factor=2\n","    )\n","    \n","    search_alg = OptunaSearch(metric=\"mean_accuracy\", mode=\"max\")\n","    \n","    analysis = tune.run(\n","        tune.with_parameters(train_and_evaluate, splits=splits, splits_name=splits_name),\n","        resources_per_trial={\"cpu\": 10, \"gpu\": 0, \"accelerator_type:RTX\": 0},\n","        config=config,\n","        scheduler=scheduler,\n","        search_alg=search_alg,\n","        num_samples=5,\n","        verbose=1,\n","        storage_path=helper.ray_results_dir,\n","        trial_dirname_creator=custom_trial_dirname,\n","        callbacks=[WandbLoggerCallback(\n","            project=\"Classic FFN PyTorch - Test WandB\",\n","            api_key=\"06649006f62a4b3df7b4f3a86fa05340fb7a736f\",\n","            log_config=True\n","        )]\n","    )\n","\n","    best_config_data_ray_tune = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n","    print(\"Best hyperparameters found were: \", best_config_data_ray_tune)\n","    best_params_list_getting.append(best_config_data_ray_tune)\n","    \n","    return analysis"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-15 10:47:59</td></tr>\n","<tr><td>Running for: </td><td>00:04:20.99        </td></tr>\n","<tr><td>Memory:      </td><td>12.7/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.850865351332641<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_768fd1ba</td><td>TERMINATED</td><td>127.0.0.1:52941</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.356236</td><td style=\"text-align: right;\">      50</td><td>(85, 30, 45, 50, 30)</td><td style=\"text-align: right;\">     0.00019462</td><td style=\"text-align: right;\">0.789408</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         46.3966</td><td style=\"text-align: right;\">      0.666667</td><td style=\"text-align: right;\">      0.869159</td><td style=\"text-align: right;\">      0.66097 </td></tr>\n","<tr><td>train_and_evaluate_551e8fd0</td><td>TERMINATED</td><td>127.0.0.1:53298</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.362048</td><td style=\"text-align: right;\">      10</td><td>(90, 45, 30, 55, 40)</td><td style=\"text-align: right;\">     0.00217526</td><td style=\"text-align: right;\">0.850865</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         44.0141</td><td style=\"text-align: right;\">      0.787037</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.787019</td></tr>\n","<tr><td>train_and_evaluate_1716bfed</td><td>TERMINATED</td><td>127.0.0.1:53628</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.327639</td><td style=\"text-align: right;\">      20</td><td>(85, 70, 50, 80)    </td><td style=\"text-align: right;\">     0.00267441</td><td style=\"text-align: right;\">0.858307</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         45.1929</td><td style=\"text-align: right;\">      0.805556</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.805539</td></tr>\n","<tr><td>train_and_evaluate_e835aaec</td><td>TERMINATED</td><td>127.0.0.1:53978</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.409707</td><td style=\"text-align: right;\">      20</td><td>(50, 55, 60, 85, 90)</td><td style=\"text-align: right;\">     0.00320027</td><td style=\"text-align: right;\">0.865767</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         53.4583</td><td style=\"text-align: right;\">      0.814815</td><td style=\"text-align: right;\">      0.897196</td><td style=\"text-align: right;\">      0.814815</td></tr>\n","<tr><td>train_and_evaluate_1c1bd280</td><td>TERMINATED</td><td>127.0.0.1:54510</td><td>sigmoid     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.397514</td><td style=\"text-align: right;\">      10</td><td>(50, 60, 50, 90, 40)</td><td style=\"text-align: right;\">     0.00294558</td><td style=\"text-align: right;\">0.800588</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         44.1633</td><td style=\"text-align: right;\">      0.685185</td><td style=\"text-align: right;\">      0.850467</td><td style=\"text-align: right;\">      0.684211</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_768fd1ba/wandb/run-20240815_104344-a4w0vc5w\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Syncing run trial_768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/a4w0vc5w\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss █████▇▇▇▇█▇▇▇▇▇▇▇▆▇▆▆▅▅▅▅▅▅▅▄▄▄▃▂▄▂▃▃▂▂▁\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss 0.56089\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run trial_768fd1ba at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/a4w0vc5w\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104344-a4w0vc5w/logs\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/driver_artifacts/trial_768fd1ba/wandb/run-20240815_104345-768fd1ba\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Syncing run train_and_evaluate_768fd1ba\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_768fd1ba/wandb/run-20240815_104350-1bh2fbnp\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Syncing run trial_768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1bh2fbnp\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss █████▇█▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▃▅▃▃▃▃▂▃▃▁▁▂▂\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss 0.58778\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run trial_768fd1ba at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1bh2fbnp\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104350-1bh2fbnp/logs\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_768fd1ba/wandb/run-20240815_104358-v4btfk2z\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Syncing run trial_768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/v4btfk2z\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss █████▇▇█▇▇▇▇█▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▃▄▄▄▄▃▂▂▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss 0.56887\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run trial_768fd1ba at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/v4btfk2z\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104358-v4btfk2z/logs\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_768fd1ba/wandb/run-20240815_104407-jvqpmeo6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Syncing run trial_768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jvqpmeo6\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss ███▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▂▃▂▂▁▁▂\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb:  loss 0.56977\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run trial_768fd1ba at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jvqpmeo6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104407-jvqpmeo6/logs\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_768fd1ba/wandb/run-20240815_104416-6ay5jqku\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: Syncing run trial_768fd1ba\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6ay5jqku\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=52941)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A2.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=52941)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A2.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-15 10:44:30,592\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.712 s, which may be a performance bottleneck.\n","2024-08-15 10:44:30,593\tWARNING util.py:201 -- The `process_trial_result` operation took 0.713 s, which may be a performance bottleneck.\n","2024-08-15 10:44:30,593\tWARNING util.py:201 -- Processing trial results took 0.713 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:44:30,594\tWARNING util.py:201 -- The `process_trial_result` operation took 0.714 s, which may be a performance bottleneck.\n","2024-08-15 10:44:31,287\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 30, 45, 50, 30)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:      config/dropout_rate 0.35624\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:     config/learning_rate 0.00019\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_accuracy 0.86916\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_f1_macro 0.86887\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             max_f1_micro 0.86916\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_accuracy 0.78941\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_f1_macro 0.78526\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:            mean_f1_micro 0.78941\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_accuracy 0.66667\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_f1_macro 0.66097\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             min_f1_micro 0.66667\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:       time_since_restore 46.39658\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:         time_this_iter_s 46.39658\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:             time_total_s 46.39658\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:                timestamp 1723711469\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: 🚀 View run train_and_evaluate_768fd1ba at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/768fd1ba\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104345-768fd1ba/logs\n","\u001b[36m(_WandbLoggingActor pid=52951)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_551e8fd0/wandb/run-20240815_104436-4xl8k0rf\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Syncing run trial_551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/4xl8k0rf\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss ██▇▆▅▄▃▂▂▁\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss 0.41226\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run trial_551e8fd0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/4xl8k0rf\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104436-4xl8k0rf/logs\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/driver_artifacts/trial_551e8fd0/wandb/run-20240815_104436-551e8fd0\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Syncing run train_and_evaluate_551e8fd0\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_551e8fd0/wandb/run-20240815_104440-5wkekb6w\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Syncing run trial_551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/5wkekb6w\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss █▇▆▆▄▄▂▃▁▁\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss 0.39612\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run trial_551e8fd0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/5wkekb6w\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104440-5wkekb6w/logs\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_551e8fd0/wandb/run-20240815_104448-02crbfbz\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Syncing run trial_551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/02crbfbz\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss ██▇▆▄▃▃▂▁▁\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss 0.39621\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run trial_551e8fd0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/02crbfbz\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104448-02crbfbz/logs\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: | Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_551e8fd0/wandb/run-20240815_104456-rsgo2t8g\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Syncing run trial_551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rsgo2t8g\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss █▇▇▆▅▃▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb:  loss 0.40863\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run trial_551e8fd0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rsgo2t8g\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104456-rsgo2t8g/logs\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_551e8fd0/wandb/run-20240815_104506-ovj7sc4y\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: Syncing run trial_551e8fd0\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ovj7sc4y\n","2024-08-15 10:45:19,728\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.793 s, which may be a performance bottleneck.\n","2024-08-15 10:45:19,729\tWARNING util.py:201 -- The `process_trial_result` operation took 0.794 s, which may be a performance bottleneck.\n","2024-08-15 10:45:19,729\tWARNING util.py:201 -- Processing trial results took 0.795 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:45:19,729\tWARNING util.py:201 -- The `process_trial_result` operation took 0.795 s, which may be a performance bottleneck.\n","2024-08-15 10:45:20,428\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 45, 30, 55, 40)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:      config/dropout_rate 0.36205\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            config/epochs 10\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:     config/learning_rate 0.00218\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_accuracy 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_f1_macro 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             max_f1_micro 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_accuracy 0.85087\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_f1_macro 0.85005\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:            mean_f1_micro 0.85087\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_accuracy 0.78704\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_f1_macro 0.78702\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             min_f1_micro 0.78704\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:       time_since_restore 44.01414\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:         time_this_iter_s 44.01414\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:             time_total_s 44.01414\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:                timestamp 1723711518\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: 🚀 View run train_and_evaluate_551e8fd0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/551e8fd0\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104436-551e8fd0/logs\n","\u001b[36m(_WandbLoggingActor pid=53317)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1716bfed/wandb/run-20240815_104526-rci28ttb\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Syncing run trial_1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rci28ttb\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss ████▇▇▇▇▅▄▄▂▂▂▁▂▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss 0.31956\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run trial_1716bfed at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rci28ttb\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104526-rci28ttb/logs\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/driver_artifacts/trial_1716bfed/wandb/run-20240815_104525-1716bfed\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Syncing run train_and_evaluate_1716bfed\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1716bfed/wandb/run-20240815_104531-lug1xbrw\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Syncing run trial_1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lug1xbrw\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss ██████▇▆▅▄▃▃▃▂▂▂▁▂▁▁\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss 0.32182\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run trial_1716bfed at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lug1xbrw\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104531-lug1xbrw/logs\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1716bfed/wandb/run-20240815_104539-39l4jw3c\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Syncing run trial_1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/39l4jw3c\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss ███▇█▇▇▇▆▅▃▃▂▂▂▁▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss 0.34598\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run trial_1716bfed at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/39l4jw3c\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104539-39l4jw3c/logs\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1716bfed/wandb/run-20240815_104547-e0fbjn83\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Syncing run trial_1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/e0fbjn83\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss ███▇█▇▇▆▅▄▄▃▃▂▁▁▁▁▂▁\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb:  loss 0.33062\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run trial_1716bfed at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/e0fbjn83\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104547-e0fbjn83/logs\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1716bfed/wandb/run-20240815_104555-65k92wsc\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: Syncing run trial_1716bfed\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53628)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/65k92wsc\n","2024-08-15 10:46:10,062\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.769 s, which may be a performance bottleneck.\n","2024-08-15 10:46:10,063\tWARNING util.py:201 -- The `process_trial_result` operation took 0.770 s, which may be a performance bottleneck.\n","2024-08-15 10:46:10,063\tWARNING util.py:201 -- Processing trial results took 0.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:46:10,064\tWARNING util.py:201 -- The `process_trial_result` operation took 0.771 s, which may be a performance bottleneck.\n","2024-08-15 10:46:10,794\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (85, 70, 50, 80)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:      config/dropout_rate 0.32764\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            config/epochs 20\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:     config/learning_rate 0.00267\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_accuracy 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_f1_macro 0.89705\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             max_f1_micro 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_accuracy 0.85831\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_f1_macro 0.85786\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:            mean_f1_micro 0.85831\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_accuracy 0.80556\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_f1_macro 0.80554\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             min_f1_micro 0.80556\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:       time_since_restore 45.19286\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:         time_this_iter_s 45.19286\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:             time_total_s 45.19286\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:                timestamp 1723711569\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: 🚀 View run train_and_evaluate_1716bfed at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/1716bfed\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104525-1716bfed/logs\n","\u001b[36m(_WandbLoggingActor pid=53642)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_e835aaec/wandb/run-20240815_104615-0wk66ucf\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Syncing run trial_e835aaec\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/0wk66ucf\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss ██▇▇█▇▇▅▃▃▃▂▂▂▂▂▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss 0.35774\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run trial_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/0wk66ucf\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104615-0wk66ucf/logs\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Tracking run with wandb version 0.17.6\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_e835aaec/wandb/run-20240815_104621-iy0ijxzc\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Syncing run trial_e835aaec\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/iy0ijxzc\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \\ 0.006 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: | 0.006 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: / 0.006 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - 0.006 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss ███▇█▇▇▆▄▃▃▂▂▂▁▂▁▂▂▁\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss 0.34984\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run trial_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/iy0ijxzc\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104621-iy0ijxzc/logs\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_e835aaec/wandb/run-20240815_104628-9hbu9acl\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Syncing run trial_e835aaec\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/9hbu9acl\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss ██▇▇▇▇▇▆▄▂▃▂▂▂▂▁▂▁▁▂\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss 0.38835\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run trial_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/9hbu9acl\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104628-9hbu9acl/logs\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_e835aaec/wandb/run-20240815_104643-njeeawdy\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Syncing run trial_e835aaec\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/njeeawdy\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss ████▇▇▇▅▄▃▃▃▃▃▂▁▁▂▂▁\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss 0.31076\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run trial_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/njeeawdy\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104643-njeeawdy/logs\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_e835aaec/wandb/run-20240815_104653-f6x76ule\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Syncing run trial_e835aaec\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/f6x76ule\n","2024-08-15 10:47:08,434\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.038 s, which may be a performance bottleneck.\n","2024-08-15 10:47:08,436\tWARNING util.py:201 -- The `process_trial_result` operation took 1.041 s, which may be a performance bottleneck.\n","2024-08-15 10:47:08,437\tWARNING util.py:201 -- Processing trial results took 1.042 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:47:08,437\tWARNING util.py:201 -- The `process_trial_result` operation took 1.042 s, which may be a performance bottleneck.\n","2024-08-15 10:47:09,265\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 55, 60, 85, 90)}\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \\ 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss ██████▇▅▄▄▃▃▂▂▃▂▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb:  loss 0.32013\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: 🚀 View run trial_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/f6x76ule\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104653-f6x76ule/logs\n","\u001b[36m(train_and_evaluate pid=53978)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:      config/dropout_rate 0.40971\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            config/epochs 20\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:     config/learning_rate 0.0032\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             max_accuracy 0.8972\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:            mean_accuracy 0.86577\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             min_accuracy 0.81481\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:       time_since_restore 53.45828\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:         time_this_iter_s 53.45828\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:             time_total_s 53.45828\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:                timestamp 1723711627\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1c1bd280/wandb/run-20240815_104715-yr57jcke\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Syncing run trial_1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/yr57jcke\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: 🚀 View run train_and_evaluate_e835aaec at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/e835aaec\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104615-e835aaec/logs\n","\u001b[36m(_WandbLoggingActor pid=53991)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss ██▇██▇▇▆▄▁\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss 0.47555\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run trial_1c1bd280 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/yr57jcke\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104715-yr57jcke/logs\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/driver_artifacts/trial_1c1bd280/wandb/run-20240815_104715-1c1bd280\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Syncing run train_and_evaluate_1c1bd280\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1c1bd280/wandb/run-20240815_104721-vgf2n6hh\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Syncing run trial_1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/vgf2n6hh\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss ███▇▇▇▇▆▃▁\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss 0.49459\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run trial_1c1bd280 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/vgf2n6hh\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104721-vgf2n6hh/logs\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1c1bd280/wandb/run-20240815_104728-2zxwhbq0\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Syncing run trial_1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2zxwhbq0\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss ██████▇▆▂▁\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss 0.46277\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run trial_1c1bd280 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2zxwhbq0\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104728-2zxwhbq0/logs\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1c1bd280/wandb/run-20240815_104736-g3163h8n\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Syncing run trial_1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/g3163h8n\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss ██████▇▆▄▁\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb:  loss 0.49647\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run trial_1c1bd280 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/g3163h8n\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104736-g3163h8n/logs\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-43-36/train_and_evaluate_2024-08-15_10-43-36/working_dirs/trial_1c1bd280/wandb/run-20240815_104744-iu0b9sr9\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: Syncing run trial_1c1bd280\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=54510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/iu0b9sr9\n","2024-08-15 10:47:58,536\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.652 s, which may be a performance bottleneck.\n","2024-08-15 10:47:58,537\tWARNING util.py:201 -- The `process_trial_result` operation took 0.653 s, which may be a performance bottleneck.\n","2024-08-15 10:47:58,537\tWARNING util.py:201 -- Processing trial results took 0.654 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:47:58,537\tWARNING util.py:201 -- The `process_trial_result` operation took 0.654 s, which may be a performance bottleneck.\n","2024-08-15 10:47:59,304\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 60, 50, 90, 40)}\n","2024-08-15 10:47:59,425\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-15_10-43-36' in 0.1191s.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:      config/dropout_rate 0.39751\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            config/epochs 10\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:     config/learning_rate 0.00295\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_accuracy 0.85047\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_f1_macro 0.85045\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             max_f1_micro 0.85047\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_accuracy 0.80059\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_f1_macro 0.79973\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:            mean_f1_micro 0.80059\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_accuracy 0.68519\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_f1_macro 0.68421\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             min_f1_micro 0.68519\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:       time_since_restore 44.16327\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:         time_this_iter_s 44.16327\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:             time_total_s 44.16327\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:                timestamp 1723711677\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: 🚀 View run train_and_evaluate_1c1bd280 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/1c1bd280\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104715-1c1bd280/logs\n","\u001b[36m(_WandbLoggingActor pid=54518)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","2024-08-15 10:48:02,547\tINFO tune.py:1041 -- Total run time: 268.01 seconds (260.87 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'sigmoid', 'learning_rate': 0.00320027036491653, 'batch_size': 32, 'hidden_layers': (50, 55, 60, 85, 90), 'epochs': 20, 'dropout_rate': 0.40970736826155685}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a2_splits, \"A2\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-15 10:53:35</td></tr>\n","<tr><td>Running for: </td><td>00:04:33.47        </td></tr>\n","<tr><td>Memory:      </td><td>12.5/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.8497308488612836<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_065f6212</td><td>TERMINATED</td><td>127.0.0.1:55020</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.327081</td><td style=\"text-align: right;\">      40</td><td>(75, 60, 90, 85, 55)</td><td style=\"text-align: right;\">    0.0031063  </td><td style=\"text-align: right;\">0.849731</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         46.9484</td><td style=\"text-align: right;\">      0.826087</td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.825169</td></tr>\n","<tr><td>train_and_evaluate_cad84ecc</td><td>TERMINATED</td><td>127.0.0.1:55366</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.450409</td><td style=\"text-align: right;\">      40</td><td>(45, 40, 65, 50, 40)</td><td style=\"text-align: right;\">    0.000120216</td><td style=\"text-align: right;\">0.734079</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         51.4124</td><td style=\"text-align: right;\">      0.652174</td><td style=\"text-align: right;\">      0.797101</td><td style=\"text-align: right;\">      0.648557</td></tr>\n","<tr><td>train_and_evaluate_9f8fa340</td><td>TERMINATED</td><td>127.0.0.1:55718</td><td>relu        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.258378</td><td style=\"text-align: right;\">      50</td><td>(40, 40, 80, 75, 85)</td><td style=\"text-align: right;\">    0.000583513</td><td style=\"text-align: right;\">0.872878</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         52.0812</td><td style=\"text-align: right;\">      0.811594</td><td style=\"text-align: right;\">      0.927536</td><td style=\"text-align: right;\">      0.811594</td></tr>\n","<tr><td>train_and_evaluate_56f45700</td><td>TERMINATED</td><td>127.0.0.1:56078</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.420654</td><td style=\"text-align: right;\">      20</td><td>(50, 80, 85, 90, 50)</td><td style=\"text-align: right;\">    0.000664991</td><td style=\"text-align: right;\">0.621408</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         42.9628</td><td style=\"text-align: right;\">      0.492754</td><td style=\"text-align: right;\">      0.782609</td><td style=\"text-align: right;\">      0.330097</td></tr>\n","<tr><td>train_and_evaluate_3e4f5c4e</td><td>TERMINATED</td><td>127.0.0.1:56379</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.204474</td><td style=\"text-align: right;\">      50</td><td>(35, 40, 80, 35, 75)</td><td style=\"text-align: right;\">    0.00129494 </td><td style=\"text-align: right;\">0.872795</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.3762</td><td style=\"text-align: right;\">      0.826087</td><td style=\"text-align: right;\">      0.898551</td><td style=\"text-align: right;\">      0.82605 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_065f6212/wandb/run-20240815_104908-8uq6u5y3\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Syncing run trial_065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8uq6u5y3\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss ██▆▅▄▃▃▃▄▃▂▃▃▃▂▂▃▂▃▂▂▃▂▂▂▁▁▂▁▂▁▂▂▁▁▁▂▁▂▁\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss 0.29315\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run trial_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8uq6u5y3\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104908-8uq6u5y3/logs\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/driver_artifacts/trial_065f6212/wandb/run-20240815_104909-065f6212\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Syncing run train_and_evaluate_065f6212\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_065f6212/wandb/run-20240815_104915-xh4g43kx\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Syncing run trial_065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/xh4g43kx\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss █▇▄▄▅▄▄▃▂▃▄▃▃▂▃▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▁▃▃▂▂▂▁▁▂▁\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss 0.22745\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run trial_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/xh4g43kx\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104915-xh4g43kx/logs\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_065f6212/wandb/run-20240815_104923-263m02a3\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Syncing run trial_065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/263m02a3\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \\ 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss █▆▄▅▄▃▃▃▃▃▂▃▃▃▃▂▃▂▂▂▂▂▃▃▃▂▂▂▁▂▁▁▂▂▁▂▁▂▁▂\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss 0.27343\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run trial_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/263m02a3\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104923-263m02a3/logs\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_065f6212/wandb/run-20240815_104930-818lthwb\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Syncing run trial_065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/818lthwb\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss █▆▅▅▅▄▃▄▅▃▃▄▂▂▃▂▃▃▃▂▂▂▂▃▃▂▃▂▂▂▂▁▂▂▂▂▂▄▂▂\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss 0.28475\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run trial_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/818lthwb\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104930-818lthwb/logs\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_065f6212/wandb/run-20240815_104941-se33hfq7\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Syncing run trial_065f6212\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/se33hfq7\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=55020)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A3.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A3.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-15 10:49:55,049\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.585 s, which may be a performance bottleneck.\n","2024-08-15 10:49:55,050\tWARNING util.py:201 -- The `process_trial_result` operation took 0.586 s, which may be a performance bottleneck.\n","2024-08-15 10:49:55,050\tWARNING util.py:201 -- Processing trial results took 0.586 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:49:55,050\tWARNING util.py:201 -- The `process_trial_result` operation took 0.586 s, which may be a performance bottleneck.\n","2024-08-15 10:49:55,854\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 60, 90, 85, 55)}\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:      config/dropout_rate 0.32708\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:     config/learning_rate 0.00311\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_accuracy 0.89855\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_f1_macro 0.89778\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             max_f1_micro 0.89855\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_accuracy 0.84973\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_f1_macro 0.84903\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:            mean_f1_micro 0.84973\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_accuracy 0.82609\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_f1_macro 0.82517\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             min_f1_micro 0.82609\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:       time_since_restore 46.94837\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:         time_this_iter_s 46.94837\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:             time_total_s 46.94837\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:                timestamp 1723711794\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: 🚀 View run train_and_evaluate_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/065f6212\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104909-065f6212/logs\n","\u001b[36m(_WandbLoggingActor pid=55048)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: | 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss █▇▅▅▄▃▄▃▄▃▃▃▂▃▃▃▂▂▄▃▂▂▂▂▁▁▂▂▂▃▃▂▂▂▂▂▁▁▂▂\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb:  loss 0.27915\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_cad84ecc/wandb/run-20240815_104959-pzaq3sf6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Syncing run trial_cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/pzaq3sf6\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: 🚀 View run trial_065f6212 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/se33hfq7\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104941-se33hfq7/logs\n","\u001b[36m(train_and_evaluate pid=55020)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/driver_artifacts/trial_cad84ecc/wandb/run-20240815_105000-cad84ecc\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Syncing run train_and_evaluate_cad84ecc\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss ▇▇██▆█▇█▇▆▇▇▇▆▇▆▆▇▆▇▆▇▆▆▆▆▅▆▄▅▅▅▄▃▂▃▂▃▁▁\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss 0.60899\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run trial_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/pzaq3sf6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_104959-pzaq3sf6/logs\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_cad84ecc/wandb/run-20240815_105006-ou418rsq\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Syncing run trial_cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ou418rsq\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss ███▇▇▇▇██▇▇▆▇▇▆▆▇▇▆▆▆▅▆▆▅▅▅▅▄▄▄▃▄▃▂▁▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss 0.5243\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run trial_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ou418rsq\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105006-ou418rsq/logs\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_cad84ecc/wandb/run-20240815_105015-bixwedwh\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Syncing run trial_cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bixwedwh\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss █▇██████▇▇▇█▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▃▃▂▃▃▂▁\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss 0.53232\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run trial_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bixwedwh\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105015-bixwedwh/logs\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_cad84ecc/wandb/run-20240815_105025-pj7nejrr\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Syncing run trial_cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/pj7nejrr\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss ▆▇▇█▇▆▆▆▆▆▆▇▆▆▅▆▆▆▅▆▅▅▅▅▅▄▅▅▄▅▃▃▃▁▃▁▃▁▂▁\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss 0.62665\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run trial_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/pj7nejrr\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105025-pj7nejrr/logs\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_cad84ecc/wandb/run-20240815_105034-6ejkudak\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Syncing run trial_cad84ecc\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6ejkudak\n","2024-08-15 10:50:50,802\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.580 s, which may be a performance bottleneck.\n","2024-08-15 10:50:50,803\tWARNING util.py:201 -- The `process_trial_result` operation took 0.581 s, which may be a performance bottleneck.\n","2024-08-15 10:50:50,803\tWARNING util.py:201 -- Processing trial results took 0.581 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:50:50,804\tWARNING util.py:201 -- The `process_trial_result` operation took 0.582 s, which may be a performance bottleneck.\n","2024-08-15 10:50:51,478\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (45, 40, 65, 50, 40)}\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss █▇█▇█▇██▇▇▇▇▆▇▇▆▇▇▆▆▆▆▆▅▆▅▅▅▅▅▅▄▄▅▃▄▂▁▃▂\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb:  loss 0.58132\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: 🚀 View run trial_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6ejkudak\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105034-6ejkudak/logs\n","\u001b[36m(train_and_evaluate pid=55366)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:      config/dropout_rate 0.45041\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:     config/learning_rate 0.00012\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             max_accuracy 0.7971\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:            mean_accuracy 0.73408\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             min_accuracy 0.65217\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:       time_since_restore 51.41237\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:         time_this_iter_s 51.41237\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:             time_total_s 51.41237\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:                timestamp 1723711850\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_9f8fa340/wandb/run-20240815_105056-bdsppm7o\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Syncing run trial_9f8fa340\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bdsppm7o\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: 🚀 View run train_and_evaluate_cad84ecc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/cad84ecc\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105000-cad84ecc/logs\n","\u001b[36m(_WandbLoggingActor pid=55374)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/driver_artifacts/trial_9f8fa340/wandb/run-20240815_105056-9f8fa340\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Syncing run train_and_evaluate_9f8fa340\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/9f8fa340\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss ██▇▇▇▇▆▆▄▄▄▅▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss 0.22301\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run trial_9f8fa340 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bdsppm7o\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105056-bdsppm7o/logs\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_9f8fa340/wandb/run-20240815_105102-02bxkiui\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Syncing run trial_9f8fa340\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/02bxkiui\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss ████▇▇▆▅▅▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▃▂▂▂▁▂▂▂▁▂▁▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss 0.20855\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run trial_9f8fa340 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/02bxkiui\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105102-02bxkiui/logs\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_9f8fa340/wandb/run-20240815_105113-31e0fb2y\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Syncing run trial_9f8fa340\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/31e0fb2y\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss █████▇▇▇▆▆▅▅▅▄▅▄▄▃▄▄▃▃▃▃▃▂▄▂▂▂▃▂▂▂▃▂▂▂▂▁\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss 0.16273\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run trial_9f8fa340 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/31e0fb2y\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105113-31e0fb2y/logs\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_9f8fa340/wandb/run-20240815_105122-1levd3g6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Syncing run trial_9f8fa340\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1levd3g6\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss ████▇▇▇▆▆▅▅▅▄▃▃▄▄▃▂▃▂▂▃▂▃▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb:  loss 0.2634\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run trial_9f8fa340 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1levd3g6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105122-1levd3g6/logs\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_9f8fa340/wandb/run-20240815_105132-y3gyp0ur\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: Syncing run trial_9f8fa340\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=55718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/y3gyp0ur\n","2024-08-15 10:51:47,765\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.759 s, which may be a performance bottleneck.\n","2024-08-15 10:51:47,765\tWARNING util.py:201 -- The `process_trial_result` operation took 0.760 s, which may be a performance bottleneck.\n","2024-08-15 10:51:47,765\tWARNING util.py:201 -- Processing trial results took 0.760 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:51:47,766\tWARNING util.py:201 -- The `process_trial_result` operation took 0.761 s, which may be a performance bottleneck.\n","2024-08-15 10:51:48,577\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (40, 40, 80, 75, 85)}\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: - 0.007 MB of 0.016 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:      config/dropout_rate 0.25838\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:     config/learning_rate 0.00058\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_accuracy 0.92754\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_f1_macro 0.92698\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             max_f1_micro 0.92754\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_accuracy 0.87288\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_f1_macro 0.87238\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:            mean_f1_micro 0.87288\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_accuracy 0.81159\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_f1_macro 0.81159\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             min_f1_micro 0.81159\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:       time_since_restore 52.0812\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:         time_this_iter_s 52.0812\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:             time_total_s 52.0812\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:                timestamp 1723711907\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: 🚀 View run train_and_evaluate_9f8fa340 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/9f8fa340\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105056-9f8fa340/logs\n","\u001b[36m(_WandbLoggingActor pid=55726)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_56f45700/wandb/run-20240815_105153-8vvjuxur\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Syncing run trial_56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8vvjuxur\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss █▇▇█▆█▇▇▆▆▇▆▄▃▆▇▂▁▃▁\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss 0.6155\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run trial_56f45700 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8vvjuxur\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105153-8vvjuxur/logs\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/driver_artifacts/trial_56f45700/wandb/run-20240815_105153-56f45700\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Syncing run train_and_evaluate_56f45700\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_56f45700/wandb/run-20240815_105157-cqrftzkm\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Syncing run trial_56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/cqrftzkm\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss ▇█▆▆▅▆▆▆▄▅▅▅▄▃▄▄▂▂▁▂\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss 0.6631\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run trial_56f45700 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/cqrftzkm\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105157-cqrftzkm/logs\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_56f45700/wandb/run-20240815_105204-bvllfx4w\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Syncing run trial_56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bvllfx4w\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss █▇▇▆▇▆▇▆▅▅▃▄▄▄▃▁▂▃▃▁\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss 0.61589\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run trial_56f45700 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/bvllfx4w\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105204-bvllfx4w/logs\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_56f45700/wandb/run-20240815_105213-nqhqyuz4\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Syncing run trial_56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/nqhqyuz4\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss ▇▇▆▇█▇▆▇▇▆▅▆▅▁▂▂▆▁▂▃\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb:  loss 0.67175\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run trial_56f45700 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/nqhqyuz4\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105213-nqhqyuz4/logs\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: | Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_56f45700/wandb/run-20240815_105220-n0pdt9y3\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: Syncing run trial_56f45700\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56078)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/n0pdt9y3\n","2024-08-15 10:52:35,466\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.611 s, which may be a performance bottleneck.\n","2024-08-15 10:52:35,467\tWARNING util.py:201 -- The `process_trial_result` operation took 0.613 s, which may be a performance bottleneck.\n","2024-08-15 10:52:35,467\tWARNING util.py:201 -- Processing trial results took 0.613 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:52:35,468\tWARNING util.py:201 -- The `process_trial_result` operation took 0.613 s, which may be a performance bottleneck.\n","2024-08-15 10:52:36,163\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (50, 80, 85, 90, 50)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:      config/dropout_rate 0.42065\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            config/epochs 20\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:     config/learning_rate 0.00066\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_accuracy 0.78261\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_f1_macro 0.77794\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             max_f1_micro 0.78261\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_accuracy 0.62141\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_f1_macro 0.54577\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:            mean_f1_micro 0.62141\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_accuracy 0.49275\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_f1_macro 0.3301\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             min_f1_micro 0.49275\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:       time_since_restore 42.96281\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:         time_this_iter_s 42.96281\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:             time_total_s 42.96281\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:                timestamp 1723711954\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: 🚀 View run train_and_evaluate_56f45700 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/56f45700\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105153-56f45700/logs\n","\u001b[36m(_WandbLoggingActor pid=56086)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_3e4f5c4e/wandb/run-20240815_105240-w88zpr85\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Syncing run trial_3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/w88zpr85\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/driver_artifacts/trial_3e4f5c4e/wandb/run-20240815_105240-3e4f5c4e\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Syncing run train_and_evaluate_3e4f5c4e\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss █▇▆▅▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▃▃▃▂▂▁▂▂▃▂▂▂▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss 0.22149\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run trial_3e4f5c4e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/w88zpr85\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105240-w88zpr85/logs\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_3e4f5c4e/wandb/run-20240815_105247-ouzsh04m\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Syncing run trial_3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ouzsh04m\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss ██▇▅▄▄▄▄▃▄▃▃▃▃▃▂▃▂▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss 0.26363\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run trial_3e4f5c4e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ouzsh04m\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105247-ouzsh04m/logs\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: | Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_3e4f5c4e/wandb/run-20240815_105257-6q7op0me\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Syncing run trial_3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6q7op0me\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss █▇▆▅▄▃▄▃▃▃▃▂▂▃▂▂▂▂▂▁▂▃▂▂▁▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss 0.22665\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run trial_3e4f5c4e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6q7op0me\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105257-6q7op0me/logs\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_3e4f5c4e/wandb/run-20240815_105308-t9njpjjk\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Syncing run trial_3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/t9njpjjk\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss █▇▆▅▄▃▄▄▃▃▃▂▂▂▃▃▂▁▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▂▂▁▂▁▁\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb:  loss 0.2633\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run trial_3e4f5c4e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/t9njpjjk\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105308-t9njpjjk/logs\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-49-01/train_and_evaluate_2024-08-15_10-49-01/working_dirs/trial_3e4f5c4e/wandb/run-20240815_105318-3dnoluqv\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: Syncing run trial_3e4f5c4e\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/3dnoluqv\n","2024-08-15 10:53:35,067\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.711 s, which may be a performance bottleneck.\n","2024-08-15 10:53:35,068\tWARNING util.py:201 -- The `process_trial_result` operation took 0.712 s, which may be a performance bottleneck.\n","2024-08-15 10:53:35,068\tWARNING util.py:201 -- Processing trial results took 0.713 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:53:35,068\tWARNING util.py:201 -- The `process_trial_result` operation took 0.713 s, which may be a performance bottleneck.\n","2024-08-15 10:53:35,789\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 40, 80, 35, 75)}\n","2024-08-15 10:53:35,932\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-15_10-49-01' in 0.1407s.\n","wandb:                                                                                \n","2024-08-15 10:53:38,594\tINFO tune.py:1041 -- Total run time: 278.18 seconds (273.33 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'relu', 'learning_rate': 0.0005835127081408091, 'batch_size': 32, 'hidden_layers': (40, 40, 80, 75, 85), 'epochs': 50, 'dropout_rate': 0.2583779379085752}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:      config/dropout_rate 0.20447\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:     config/learning_rate 0.00129\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_accuracy 0.89855\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_f1_macro 0.89847\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             max_f1_micro 0.89855\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_accuracy 0.8728\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_f1_macro 0.87261\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:            mean_f1_micro 0.8728\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_accuracy 0.82609\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_f1_macro 0.82605\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             min_f1_micro 0.82609\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:       time_since_restore 55.37617\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:         time_this_iter_s 55.37617\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:             time_total_s 55.37617\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:                timestamp 1723712014\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: 🚀 View run train_and_evaluate_3e4f5c4e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/3e4f5c4e\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105240-3e4f5c4e/logs\n","\u001b[36m(_WandbLoggingActor pid=56387)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"]}],"source":["analysis = five_fold_cross_validation(helper.a3_splits, \"A3\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-15 10:59:27</td></tr>\n","<tr><td>Running for: </td><td>00:05:46.59        </td></tr>\n","<tr><td>Memory:      </td><td>12.3/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.594655196910399<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_e701240e</td><td>TERMINATED</td><td>127.0.0.1:56853</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.273032</td><td style=\"text-align: right;\">      10</td><td>(55, 85, 40, 80)    </td><td style=\"text-align: right;\">    0.00104988 </td><td style=\"text-align: right;\">0.652611</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.603 </td><td style=\"text-align: right;\">      0.615776</td><td style=\"text-align: right;\">      0.722646</td><td style=\"text-align: right;\">      0.618467</td></tr>\n","<tr><td>train_and_evaluate_29625fd6</td><td>TERMINATED</td><td>127.0.0.1:57270</td><td>sigmoid     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.477479</td><td style=\"text-align: right;\">      40</td><td>(35, 45, 50, 85, 70)</td><td style=\"text-align: right;\">    0.000945259</td><td style=\"text-align: right;\">0.509713</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.7913</td><td style=\"text-align: right;\">      0.413706</td><td style=\"text-align: right;\">      0.562341</td><td style=\"text-align: right;\">      0.36652 </td></tr>\n","<tr><td>train_and_evaluate_bb13f387</td><td>TERMINATED</td><td>127.0.0.1:57828</td><td>relu        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.445226</td><td style=\"text-align: right;\">      30</td><td>(60, 70, 55, 60, 45)</td><td style=\"text-align: right;\">    0.000344299</td><td style=\"text-align: right;\">0.594655</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.3774</td><td style=\"text-align: right;\">      0.502538</td><td style=\"text-align: right;\">      0.648855</td><td style=\"text-align: right;\">      0.486101</td></tr>\n","<tr><td>train_and_evaluate_e204b1d6</td><td>TERMINATED</td><td>127.0.0.1:58307</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.347636</td><td style=\"text-align: right;\">      40</td><td>(35, 55, 60, 65, 40)</td><td style=\"text-align: right;\">    0.000814525</td><td style=\"text-align: right;\">0.668385</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.0725</td><td style=\"text-align: right;\">      0.624365</td><td style=\"text-align: right;\">      0.712468</td><td style=\"text-align: right;\">      0.615275</td></tr>\n","<tr><td>train_and_evaluate_4c226548</td><td>TERMINATED</td><td>127.0.0.1:58846</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.296887</td><td style=\"text-align: right;\">      30</td><td>(75, 90, 80, 60, 60)</td><td style=\"text-align: right;\">    0.000143816</td><td style=\"text-align: right;\">0.349426</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         54.1745</td><td style=\"text-align: right;\">      0.333333</td><td style=\"text-align: right;\">      0.378173</td><td style=\"text-align: right;\">      0.166667</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e701240e/wandb/run-20240815_105346-fx7pby2i\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Syncing run trial_e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fx7pby2i\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/driver_artifacts/trial_e701240e/wandb/run-20240815_105346-e701240e\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Syncing run train_and_evaluate_e701240e\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss █▅▄▃▂▂▁▂▂▁\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss 0.8233\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run trial_e701240e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fx7pby2i\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105346-fx7pby2i/logs\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e701240e/wandb/run-20240815_105353-xp9izses\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Syncing run trial_e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/xp9izses\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss █▅▄▃▃▂▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss 0.82561\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run trial_e701240e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/xp9izses\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105353-xp9izses/logs\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: | Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e701240e/wandb/run-20240815_105403-jszm4ef9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Syncing run trial_e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jszm4ef9\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss █▅▄▃▃▂▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss 0.81703\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run trial_e701240e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jszm4ef9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105403-jszm4ef9/logs\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e701240e/wandb/run-20240815_105415-2por4m0j\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Syncing run trial_e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2por4m0j\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch ▁▂▃▃▄▅▆▆▇█\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss █▅▃▃▃▃▃▁▂▁\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: epoch 9\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb:  loss 0.81019\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run trial_e701240e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2por4m0j\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105415-2por4m0j/logs\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e701240e/wandb/run-20240815_105425-zdj9t73g\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: Syncing run trial_e701240e\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/zdj9t73g\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=56853)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A4.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=56853)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A4.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-15 10:54:41,373\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.615 s, which may be a performance bottleneck.\n","2024-08-15 10:54:41,374\tWARNING util.py:201 -- The `process_trial_result` operation took 0.616 s, which may be a performance bottleneck.\n","2024-08-15 10:54:41,374\tWARNING util.py:201 -- Processing trial results took 0.617 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:54:41,374\tWARNING util.py:201 -- The `process_trial_result` operation took 0.617 s, which may be a performance bottleneck.\n","2024-08-15 10:54:42,069\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 85, 40, 80)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:      config/dropout_rate 0.27303\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            config/epochs 10\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:     config/learning_rate 0.00105\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_accuracy 0.72265\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_f1_macro 0.71058\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             max_f1_micro 0.72265\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_accuracy 0.65261\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_f1_macro 0.64858\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:            mean_f1_micro 0.65261\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_accuracy 0.61578\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_f1_macro 0.61847\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             min_f1_micro 0.61578\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:       time_since_restore 55.603\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:         time_this_iter_s 55.603\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:             time_total_s 55.603\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:                timestamp 1723712080\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: 🚀 View run train_and_evaluate_e701240e at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/e701240e\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105346-e701240e/logs\n","\u001b[36m(_WandbLoggingActor pid=56861)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_29625fd6/wandb/run-20240815_105446-vlxpgmud\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Syncing run trial_29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/vlxpgmud\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/driver_artifacts/trial_29625fd6/wandb/run-20240815_105447-29625fd6\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Syncing run train_and_evaluate_29625fd6\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss ███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▄▂▂▂▂▁▂▁▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss 0.9851\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run trial_29625fd6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/vlxpgmud\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105446-vlxpgmud/logs\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_29625fd6/wandb/run-20240815_105457-6ir0mgs3\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Syncing run trial_29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6ir0mgs3\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss ███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▄▃▃▂▂▁▂▁▂▁▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss 0.98089\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run trial_29625fd6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6ir0mgs3\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105457-6ir0mgs3/logs\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_29625fd6/wandb/run-20240815_105511-wv4dx4ll\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Syncing run trial_29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/wv4dx4ll\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss ████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▃▃▂▃▂▂▂▂▂▁▂\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss 0.93501\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run trial_29625fd6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/wv4dx4ll\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105511-wv4dx4ll/logs\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_29625fd6/wandb/run-20240815_105525-26u5tlsa\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Syncing run trial_29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/26u5tlsa\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss █▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▄▃▂▂▂▂▁▂▂▂▂▂▁▁▂\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb:  loss 0.97975\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run trial_29625fd6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/26u5tlsa\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105525-26u5tlsa/logs\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_29625fd6/wandb/run-20240815_105540-lkfj9543\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: Syncing run trial_29625fd6\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lkfj9543\n","2024-08-15 10:55:59,782\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.622 s, which may be a performance bottleneck.\n","2024-08-15 10:55:59,782\tWARNING util.py:201 -- The `process_trial_result` operation took 0.623 s, which may be a performance bottleneck.\n","2024-08-15 10:55:59,783\tWARNING util.py:201 -- Processing trial results took 0.624 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:55:59,783\tWARNING util.py:201 -- The `process_trial_result` operation took 0.624 s, which may be a performance bottleneck.\n","2024-08-15 10:56:00,483\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 45, 50, 85, 70)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:      config/dropout_rate 0.47748\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:     config/learning_rate 0.00095\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_accuracy 0.56234\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_f1_macro 0.49737\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             max_f1_micro 0.56234\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_accuracy 0.50971\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_f1_macro 0.43476\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:            mean_f1_micro 0.50971\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_accuracy 0.41371\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_f1_macro 0.36652\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             min_f1_micro 0.41371\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:       time_since_restore 73.79131\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:         time_this_iter_s 73.79131\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:             time_total_s 73.79131\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:                timestamp 1723712159\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: 🚀 View run train_and_evaluate_29625fd6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/29625fd6\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105447-29625fd6/logs\n","\u001b[36m(_WandbLoggingActor pid=57284)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_bb13f387/wandb/run-20240815_105604-i7cpzac8\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Syncing run trial_bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/i7cpzac8\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/driver_artifacts/trial_bb13f387/wandb/run-20240815_105605-bb13f387\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Syncing run train_and_evaluate_bb13f387\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss ██▇▇▇▇▆▆▅▆▅▅▅▄▄▄▄▃▃▂▂▃▂▂▂▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss 0.88975\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run trial_bb13f387 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/i7cpzac8\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105604-i7cpzac8/logs\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_bb13f387/wandb/run-20240815_105613-gm79x9gg\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Syncing run trial_bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/gm79x9gg\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss ███▇▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▁▂▁▁\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss 0.85595\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run trial_bb13f387 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/gm79x9gg\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105613-gm79x9gg/logs\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_bb13f387/wandb/run-20240815_105625-lhptq2ev\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Syncing run trial_bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lhptq2ev\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss ███▇▇▇▆▆▆▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss 0.84241\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run trial_bb13f387 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lhptq2ev\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105625-lhptq2ev/logs\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_bb13f387/wandb/run-20240815_105637-4pm4d08i\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Syncing run trial_bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/4pm4d08i\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss ████▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▂▁\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb:  loss 0.86169\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run trial_bb13f387 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/4pm4d08i\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105637-4pm4d08i/logs\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_bb13f387/wandb/run-20240815_105650-3f2ctnyh\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: Syncing run trial_bb13f387\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=57828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/3f2ctnyh\n","2024-08-15 10:57:08,475\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.629 s, which may be a performance bottleneck.\n","2024-08-15 10:57:08,476\tWARNING util.py:201 -- The `process_trial_result` operation took 0.630 s, which may be a performance bottleneck.\n","2024-08-15 10:57:08,477\tWARNING util.py:201 -- Processing trial results took 0.630 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:57:08,477\tWARNING util.py:201 -- The `process_trial_result` operation took 0.631 s, which may be a performance bottleneck.\n","2024-08-15 10:57:09,176\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 70, 55, 60, 45)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:      config/dropout_rate 0.44523\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            config/epochs 30\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:     config/learning_rate 0.00034\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_accuracy 0.64885\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_f1_macro 0.62137\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             max_f1_micro 0.64885\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_accuracy 0.59466\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_f1_macro 0.57309\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:            mean_f1_micro 0.59466\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_accuracy 0.50254\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_f1_macro 0.4861\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             min_f1_micro 0.50254\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:       time_since_restore 64.37743\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:         time_this_iter_s 64.37743\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:             time_total_s 64.37743\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:                timestamp 1723712227\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: 🚀 View run train_and_evaluate_bb13f387 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/bb13f387\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105605-bb13f387/logs\n","\u001b[36m(_WandbLoggingActor pid=57836)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e204b1d6/wandb/run-20240815_105713-66738uk5\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Syncing run trial_e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/66738uk5\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/driver_artifacts/trial_e204b1d6/wandb/run-20240815_105714-e204b1d6\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Syncing run train_and_evaluate_e204b1d6\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss █▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▂▁▁▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss 0.8376\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run trial_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/66738uk5\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105713-66738uk5/logs\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e204b1d6/wandb/run-20240815_105724-7pcz5knp\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Syncing run trial_e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/7pcz5knp\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss █▇▅▅▄▄▄▃▄▄▃▃▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss 0.81953\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run trial_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/7pcz5knp\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105724-7pcz5knp/logs\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e204b1d6/wandb/run-20240815_105738-sizdnvrd\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Syncing run trial_e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/sizdnvrd\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss █▇▆▅▅▄▄▄▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss 0.79904\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run trial_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/sizdnvrd\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105738-sizdnvrd/logs\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e204b1d6/wandb/run-20240815_105752-71jvi170\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Syncing run trial_e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/71jvi170\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss █▇▆▅▅▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss 0.80387\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run trial_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/71jvi170\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105752-71jvi170/logs\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_e204b1d6/wandb/run-20240815_105806-r72vpexd\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Syncing run trial_e204b1d6\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/r72vpexd\n","2024-08-15 10:58:27,149\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.611 s, which may be a performance bottleneck.\n","2024-08-15 10:58:27,150\tWARNING util.py:201 -- The `process_trial_result` operation took 0.612 s, which may be a performance bottleneck.\n","2024-08-15 10:58:27,150\tWARNING util.py:201 -- Processing trial results took 0.612 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:58:27,150\tWARNING util.py:201 -- The `process_trial_result` operation took 0.613 s, which may be a performance bottleneck.\n","2024-08-15 10:58:27,838\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 55, 60, 65, 40)}\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss █▇▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb:  loss 0.78442\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: 🚀 View run trial_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/r72vpexd\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105806-r72vpexd/logs\n","\u001b[36m(train_and_evaluate pid=58307)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:      config/dropout_rate 0.34764\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:     config/learning_rate 0.00081\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             max_accuracy 0.71247\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:            mean_accuracy 0.66838\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             min_accuracy 0.62437\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:       time_since_restore 74.07249\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:         time_this_iter_s 74.07249\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:             time_total_s 74.07249\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:                timestamp 1723712306\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_4c226548/wandb/run-20240815_105832-qniuxf4y\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Syncing run trial_4c226548\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qniuxf4y\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: - 0.002 MB of 0.016 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: 🚀 View run train_and_evaluate_e204b1d6 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/e204b1d6\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105714-e204b1d6/logs\n","\u001b[36m(_WandbLoggingActor pid=58318)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/driver_artifacts/trial_4c226548/wandb/run-20240815_105833-4c226548\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Syncing run train_and_evaluate_4c226548\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/4c226548\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss █▅▆▃▅▃▅▂▄▅▄▅▄▅▅▄▄▅▄▃▄▃▃▃▄▃▃▂▁▅\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss 1.11249\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run trial_4c226548 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qniuxf4y\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105832-qniuxf4y/logs\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_4c226548/wandb/run-20240815_105840-7luhpnkc\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Syncing run trial_4c226548\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/7luhpnkc\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss █▆▄▃▃▃▄▃▃▃▂▃▂▃▃▂▃▃▃▃▁▁▃▄▂▂▂▄▂▁\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss 1.10121\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run trial_4c226548 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/7luhpnkc\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105840-7luhpnkc/logs\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_4c226548/wandb/run-20240815_105850-hq02y3w0\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Syncing run trial_4c226548\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/hq02y3w0\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss ▅▄▆▄▇▄▂█▅▄▅▅▅▂▁▅▆▂▃▆▃▄▅▄▂▄▄▅▃▅\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss 1.1138\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run trial_4c226548 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/hq02y3w0\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105850-hq02y3w0/logs\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_4c226548/wandb/run-20240815_105900-tffq24i3\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Syncing run trial_4c226548\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/tffq24i3\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss ▇▇▄▆▆▄█▄▂▃▆▅▄▄▅▅▂▅▄▁█▅▄▄▆▃▄▄▅▄\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb:  loss 1.10632\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run trial_4c226548 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/tffq24i3\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105900-tffq24i3/logs\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-53-40/train_and_evaluate_2024-08-15_10-53-40/working_dirs/trial_4c226548/wandb/run-20240815_105910-ubzxa3v4\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: Syncing run trial_4c226548\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=58846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ubzxa3v4\n","2024-08-15 10:59:26,463\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.599 s, which may be a performance bottleneck.\n","2024-08-15 10:59:26,464\tWARNING util.py:201 -- The `process_trial_result` operation took 0.600 s, which may be a performance bottleneck.\n","2024-08-15 10:59:26,464\tWARNING util.py:201 -- Processing trial results took 0.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 10:59:26,464\tWARNING util.py:201 -- The `process_trial_result` operation took 0.600 s, which may be a performance bottleneck.\n","2024-08-15 10:59:27,162\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (75, 90, 80, 60, 60)}\n","2024-08-15 10:59:27,283\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-15_10-53-40' in 0.1184s.\n","wandb:                                                                                \n","2024-08-15 10:59:30,116\tINFO tune.py:1041 -- Total run time: 351.46 seconds (346.47 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.0008145249780491403, 'batch_size': 64, 'hidden_layers': (35, 55, 60, 65, 40), 'epochs': 40, 'dropout_rate': 0.3476360245730923}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:      config/dropout_rate 0.29689\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            config/epochs 30\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:     config/learning_rate 0.00014\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_accuracy 0.37817\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_f1_macro 0.18293\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             max_f1_micro 0.37817\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_accuracy 0.34943\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_f1_macro 0.17256\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:            mean_f1_micro 0.34943\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_accuracy 0.33333\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_f1_macro 0.16667\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             min_f1_micro 0.33333\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:       time_since_restore 54.17446\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:         time_this_iter_s 54.17446\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:             time_total_s 54.17446\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:                timestamp 1723712365\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: 🚀 View run train_and_evaluate_4c226548 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/4c226548\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105833-4c226548/logs\n","\u001b[36m(_WandbLoggingActor pid=58860)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"]}],"source":["analysis = five_fold_cross_validation(helper.a4_splits, \"A4\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-15 11:06:13</td></tr>\n","<tr><td>Running for: </td><td>00:06:40.92        </td></tr>\n","<tr><td>Memory:      </td><td>12.2/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.2846621413557189<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_c944a56d</td><td>TERMINATED</td><td>127.0.0.1:59324</td><td>relu        </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.442309</td><td style=\"text-align: right;\">      40</td><td>(60, 55, 75, 90)    </td><td style=\"text-align: right;\">    0.00314501 </td><td style=\"text-align: right;\">0.281588</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.5472</td><td style=\"text-align: right;\">      0.26615 </td><td style=\"text-align: right;\">      0.318653</td><td style=\"text-align: right;\">     0.240487 </td></tr>\n","<tr><td>train_and_evaluate_93943072</td><td>TERMINATED</td><td>127.0.0.1:59753</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.251601</td><td style=\"text-align: right;\">      50</td><td>(55, 75, 70, 50, 60)</td><td style=\"text-align: right;\">    0.000128824</td><td style=\"text-align: right;\">0.28932 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        115.616 </td><td style=\"text-align: right;\">      0.266839</td><td style=\"text-align: right;\">      0.307494</td><td style=\"text-align: right;\">     0.247697 </td></tr>\n","<tr><td>train_and_evaluate_a54f62f0</td><td>TERMINATED</td><td>127.0.0.1:60590</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.389246</td><td style=\"text-align: right;\">      30</td><td>(35, 65, 40, 60, 90)</td><td style=\"text-align: right;\">    0.00069954 </td><td style=\"text-align: right;\">0.164599</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         54.1289</td><td style=\"text-align: right;\">      0.155039</td><td style=\"text-align: right;\">      0.168394</td><td style=\"text-align: right;\">     0.0447427</td></tr>\n","<tr><td>train_and_evaluate_7e95f75a</td><td>TERMINATED</td><td>127.0.0.1:61006</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.244749</td><td style=\"text-align: right;\">      50</td><td>(60, 80, 35, 75, 55)</td><td style=\"text-align: right;\">    0.0081922  </td><td style=\"text-align: right;\">0.285717</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.2865</td><td style=\"text-align: right;\">      0.256477</td><td style=\"text-align: right;\">      0.331606</td><td style=\"text-align: right;\">     0.218082 </td></tr>\n","<tr><td>train_and_evaluate_0ae8dae0</td><td>TERMINATED</td><td>127.0.0.1:61626</td><td>tanh        </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.343834</td><td style=\"text-align: right;\">      20</td><td>(90, 70, 70, 90, 45)</td><td style=\"text-align: right;\">    0.000892002</td><td style=\"text-align: right;\">0.284662</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.3936</td><td style=\"text-align: right;\">      0.253886</td><td style=\"text-align: right;\">      0.30491 </td><td style=\"text-align: right;\">     0.238774 </td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_c944a56d/wandb/run-20240815_105937-57jldx1q\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Syncing run trial_c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/57jldx1q\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/driver_artifacts/trial_c944a56d/wandb/run-20240815_105938-c944a56d\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Syncing run train_and_evaluate_c944a56d\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss ███▇▆▇▅▅▅▅▆▅▄▅▅▄▃▄▄▃▃▃▃▃▃▂▃▂▃▂▂▂▁▂▁▃▂▃▂▂\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss 1.62839\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run trial_c944a56d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/57jldx1q\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105937-57jldx1q/logs\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_c944a56d/wandb/run-20240815_105945-wkyrnoou\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Syncing run trial_c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/wkyrnoou\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss ██▇▇▇▆▆▅▆▅▅▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▁▃▂▂▂▂▃▁▂▂▁▂\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss 1.63653\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run trial_c944a56d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/wkyrnoou\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105945-wkyrnoou/logs\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_c944a56d/wandb/run-20240815_105954-6qsoxzcv\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Syncing run trial_c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6qsoxzcv\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss ██▇▆▆▅▄▅▅▄▄▄▄▅▄▃▃▄▂▂▃▄▃▃▂▃▂▂▂▂▂▁▁▂▂▁▂▂▁▂\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss 1.63443\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run trial_c944a56d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/6qsoxzcv\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105954-6qsoxzcv/logs\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_c944a56d/wandb/run-20240815_110005-uozs7r5m\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Syncing run trial_c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/uozs7r5m\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss █▇▆▆▆▆▅▅▄▄▄▄▄▃▃▃▄▄▃▄▃▄▂▃▃▄▃▃▂▂▁▂▁▃▂▂▁▂▁▂\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb:  loss 1.63087\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run trial_c944a56d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/uozs7r5m\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110005-uozs7r5m/logs\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_c944a56d/wandb/run-20240815_110017-pmnrgi5a\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: Syncing run trial_c944a56d\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/pmnrgi5a\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=59324)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A12.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=59324)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A12.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-15 11:00:32,885\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.669 s, which may be a performance bottleneck.\n","2024-08-15 11:00:32,886\tWARNING util.py:201 -- The `process_trial_result` operation took 0.671 s, which may be a performance bottleneck.\n","2024-08-15 11:00:32,886\tWARNING util.py:201 -- Processing trial results took 0.672 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:00:32,887\tWARNING util.py:201 -- The `process_trial_result` operation took 0.672 s, which may be a performance bottleneck.\n","2024-08-15 11:00:33,577\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 55, 75, 90)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:      config/dropout_rate 0.44231\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:     config/learning_rate 0.00315\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_accuracy 0.31865\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_f1_macro 0.31286\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             max_f1_micro 0.31865\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_accuracy 0.28159\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_f1_macro 0.25908\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:            mean_f1_micro 0.28159\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_accuracy 0.26615\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_f1_macro 0.24049\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             min_f1_micro 0.26615\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:       time_since_restore 55.54717\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:         time_this_iter_s 55.54717\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:             time_total_s 55.54717\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:                timestamp 1723712432\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: 🚀 View run train_and_evaluate_c944a56d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/c944a56d\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_105938-c944a56d/logs\n","\u001b[36m(_WandbLoggingActor pid=59344)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_93943072/wandb/run-20240815_110037-d2s249zs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Syncing run trial_93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/d2s249zs\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/driver_artifacts/trial_93943072/wandb/run-20240815_110038-93943072\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Syncing run train_and_evaluate_93943072\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss ███▇▇▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▃▃▂▂▂▁▂▁▂▂▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss 1.63117\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run trial_93943072 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/d2s249zs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110037-d2s249zs/logs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_93943072/wandb/run-20240815_110056-q2ad3eqt\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Syncing run trial_93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/q2ad3eqt\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss ███▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▂▂▃▂▃▂▂▂▂▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss 1.62859\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run trial_93943072 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/q2ad3eqt\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110056-q2ad3eqt/logs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_93943072/wandb/run-20240815_110119-lbhebtb8\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Syncing run trial_93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lbhebtb8\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss ████▇▇▇▇▆▆▆▅▄▅▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss 1.622\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run trial_93943072 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lbhebtb8\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110119-lbhebtb8/logs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_93943072/wandb/run-20240815_110141-qssg5kz2\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Syncing run trial_93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qssg5kz2\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss ██▇▇▇▆▆▆▅▅▅▅▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▁▁▂▁▂▁▂▁\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb:  loss 1.62039\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run trial_93943072 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qssg5kz2\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110141-qssg5kz2/logs\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_93943072/wandb/run-20240815_110204-wwjo6z0d\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: Syncing run trial_93943072\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=59753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/wwjo6z0d\n","2024-08-15 11:02:32,835\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.623 s, which may be a performance bottleneck.\n","2024-08-15 11:02:32,836\tWARNING util.py:201 -- The `process_trial_result` operation took 0.624 s, which may be a performance bottleneck.\n","2024-08-15 11:02:32,836\tWARNING util.py:201 -- Processing trial results took 0.625 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:02:32,836\tWARNING util.py:201 -- The `process_trial_result` operation took 0.625 s, which may be a performance bottleneck.\n","2024-08-15 11:02:33,526\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 75, 70, 50, 60)}\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: - 0.002 MB of 0.007 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:      config/dropout_rate 0.2516\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:     config/learning_rate 0.00013\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_accuracy 0.30749\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_f1_macro 0.28454\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             max_f1_micro 0.30749\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_accuracy 0.28932\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_f1_macro 0.269\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:            mean_f1_micro 0.28932\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_accuracy 0.26684\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_f1_macro 0.2477\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             min_f1_micro 0.26684\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:       time_since_restore 115.61634\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:         time_this_iter_s 115.61634\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:             time_total_s 115.61634\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:                timestamp 1723712552\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: 🚀 View run train_and_evaluate_93943072 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/93943072\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110038-93943072/logs\n","\u001b[36m(_WandbLoggingActor pid=59775)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_a54f62f0/wandb/run-20240815_110238-h8kjdq1u\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Syncing run trial_a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/h8kjdq1u\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/driver_artifacts/trial_a54f62f0/wandb/run-20240815_110238-a54f62f0\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Syncing run train_and_evaluate_a54f62f0\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss █▆▆▅▆▄▅▂▄▄▄▄▃▄▄▂▃▃▁▁▃▁▂▂▃▃▂▄▁▃\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss 1.80386\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run trial_a54f62f0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/h8kjdq1u\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110238-h8kjdq1u/logs\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_a54f62f0/wandb/run-20240815_110244-0jqgqg27\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Syncing run trial_a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/0jqgqg27\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss ▇█▆█▇▅▄▆█▆▃▆▄▅▄▄▃▅▇▄▅▃▂▁▄▄▄▃▄▂\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss 1.79525\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run trial_a54f62f0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/0jqgqg27\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110244-0jqgqg27/logs\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_a54f62f0/wandb/run-20240815_110254-56zrzgq6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Syncing run trial_a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/56zrzgq6\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss █▄▄▆▄▃▂▅▃▃▄▄▂▃▃▃▅▃▃▃▂▃▂▄▂▂▃▃▁▃\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss 1.80214\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run trial_a54f62f0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/56zrzgq6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110254-56zrzgq6/logs\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_a54f62f0/wandb/run-20240815_110305-lpt6oyk6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Syncing run trial_a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lpt6oyk6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss █▅▄▄▄▄▃▄▄▁▂▃▂▅▂▃▅▂▂▁▂▂▄▃▁▂▂▂▂▂\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: epoch 29\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb:  loss 1.80079\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run trial_a54f62f0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lpt6oyk6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110305-lpt6oyk6/logs\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_a54f62f0/wandb/run-20240815_110315-fvy433cj\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: Syncing run trial_a54f62f0\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=60590)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fvy433cj\n","2024-08-15 11:03:31,590\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.674 s, which may be a performance bottleneck.\n","2024-08-15 11:03:31,591\tWARNING util.py:201 -- The `process_trial_result` operation took 0.675 s, which may be a performance bottleneck.\n","2024-08-15 11:03:31,591\tWARNING util.py:201 -- Processing trial results took 0.675 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:03:31,591\tWARNING util.py:201 -- The `process_trial_result` operation took 0.676 s, which may be a performance bottleneck.\n","2024-08-15 11:03:32,295\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (35, 65, 40, 60, 90)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:      config/dropout_rate 0.38925\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            config/epochs 30\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:     config/learning_rate 0.0007\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_accuracy 0.16839\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_f1_macro 0.04804\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             max_f1_micro 0.16839\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_accuracy 0.1646\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_f1_macro 0.04711\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:            mean_f1_micro 0.1646\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_accuracy 0.15504\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_f1_macro 0.04474\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             min_f1_micro 0.15504\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:       time_since_restore 54.1289\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:         time_this_iter_s 54.1289\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:             time_total_s 54.1289\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:                timestamp 1723712610\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: 🚀 View run train_and_evaluate_a54f62f0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/a54f62f0\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110238-a54f62f0/logs\n","\u001b[36m(_WandbLoggingActor pid=60598)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_7e95f75a/wandb/run-20240815_110337-ihwl7h59\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Syncing run trial_7e95f75a\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ihwl7h59\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/driver_artifacts/trial_7e95f75a/wandb/run-20240815_110337-7e95f75a\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Syncing run train_and_evaluate_7e95f75a\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/7e95f75a\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss ██▆▆▅▆▅▄▅▄▂▃▅▄▄▃▂▂▂▃▃▃▄▃▂▁▂▂▂▁▂▂▂▂▃▂▁▁▂▂\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss 1.62451\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run trial_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ihwl7h59\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110337-ihwl7h59/logs\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_7e95f75a/wandb/run-20240815_110349-f8pigisp\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Syncing run trial_7e95f75a\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/f8pigisp\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss █▇▅▅▄▄▅▄▄▃▄▃▄▄▂▃▃▃▃▂▃▃▂▂▃▃▂▁▂▂▂▂▂▂▂▁▂▂▁▂\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss 1.63159\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run trial_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/f8pigisp\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110349-f8pigisp/logs\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_7e95f75a/wandb/run-20240815_110405-tcso9jxn\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Syncing run trial_7e95f75a\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/tcso9jxn\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss █▇▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▃▂▂▃▂▂▂▁▂▃▂▂▂▂▁▂▁▂▂\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss 1.62294\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run trial_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/tcso9jxn\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110405-tcso9jxn/logs\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_7e95f75a/wandb/run-20240815_110421-84cn9cow\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Syncing run trial_7e95f75a\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/84cn9cow\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss █▇▆▆▄▅▄▃▅▃▃▃▃▃▃▃▂▃▃▃▂▂▂▂▄▃▂▂▁▁▁▃▁▂▂▂▁▂▂▂\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss 1.63435\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run trial_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/84cn9cow\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110421-84cn9cow/logs\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_7e95f75a/wandb/run-20240815_110437-ol7juvyl\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Syncing run trial_7e95f75a\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ol7juvyl\n","2024-08-15 11:04:58,718\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.630 s, which may be a performance bottleneck.\n","2024-08-15 11:04:58,719\tWARNING util.py:201 -- The `process_trial_result` operation took 0.631 s, which may be a performance bottleneck.\n","2024-08-15 11:04:58,719\tWARNING util.py:201 -- Processing trial results took 0.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:04:58,720\tWARNING util.py:201 -- The `process_trial_result` operation took 0.632 s, which may be a performance bottleneck.\n","2024-08-15 11:04:59,426\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (60, 80, 35, 75, 55)}\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \\ 0.002 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: | 0.015 MB of 0.015 MB uploaded\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss █▆▅▅▅▄▅▃▄▂▃▃▂▃▂▃▃▂▂▃▃▄▃▂▃▃▂▂▂▂▂▂▃▁▁▁▁▂▂▁\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb:  loss 1.61977\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: 🚀 View run trial_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ol7juvyl\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110437-ol7juvyl/logs\n","\u001b[36m(train_and_evaluate pid=61006)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:      config/dropout_rate 0.24475\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:     config/learning_rate 0.00819\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             max_accuracy 0.33161\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:            mean_accuracy 0.28572\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             min_accuracy 0.25648\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:       time_since_restore 82.28653\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:         time_this_iter_s 82.28653\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:             time_total_s 82.28653\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:                timestamp 1723712698\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_0ae8dae0/wandb/run-20240815_110504-fvqaefg9\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Syncing run trial_0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fvqaefg9\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: \\ 0.002 MB of 0.016 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: 🚀 View run train_and_evaluate_7e95f75a at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/7e95f75a\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110337-7e95f75a/logs\n","\u001b[36m(_WandbLoggingActor pid=61019)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/driver_artifacts/trial_0ae8dae0/wandb/run-20240815_110504-0ae8dae0\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Syncing run train_and_evaluate_0ae8dae0\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss █▇▆▆▅▄▃▃▃▂▃▂▂▂▁▁▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss 1.61829\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run trial_0ae8dae0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fvqaefg9\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110504-fvqaefg9/logs\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_0ae8dae0/wandb/run-20240815_110514-el2ofuye\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Syncing run trial_0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/el2ofuye\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss █▇▆▅▅▄▄▄▃▃▃▂▃▂▃▁▂▂▁▁\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss 1.62162\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run trial_0ae8dae0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/el2ofuye\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110514-el2ofuye/logs\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_0ae8dae0/wandb/run-20240815_110528-nmcq8rw3\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Syncing run trial_0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/nmcq8rw3\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss █▇▆▆▃▄▃▃▃▂▃▂▂▂▂▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss 1.62786\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run trial_0ae8dae0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/nmcq8rw3\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110528-nmcq8rw3/logs\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_0ae8dae0/wandb/run-20240815_110540-fzu8r7fn\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Syncing run trial_0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fzu8r7fn\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss █▇▅▅▃▃▄▃▃▃▃▂▂▁▂▂▁▁▁▁\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb:  loss 1.61399\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run trial_0ae8dae0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fzu8r7fn\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110540-fzu8r7fn/logs\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_10-59-31/train_and_evaluate_2024-08-15_10-59-31/working_dirs/trial_0ae8dae0/wandb/run-20240815_110553-946altp5\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: Syncing run trial_0ae8dae0\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=61626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/946altp5\n","2024-08-15 11:06:12,147\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.655 s, which may be a performance bottleneck.\n","2024-08-15 11:06:12,147\tWARNING util.py:201 -- The `process_trial_result` operation took 0.656 s, which may be a performance bottleneck.\n","2024-08-15 11:06:12,147\tWARNING util.py:201 -- Processing trial results took 0.656 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:06:12,148\tWARNING util.py:201 -- The `process_trial_result` operation took 0.657 s, which may be a performance bottleneck.\n","2024-08-15 11:06:12,836\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (90, 70, 70, 90, 45)}\n","2024-08-15 11:06:13,083\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-15_10-59-31' in 0.1321s.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:        config/batch_size 32\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:      config/dropout_rate 0.34383\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            config/epochs 20\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:     config/learning_rate 0.00089\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_accuracy 0.30491\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_f1_macro 0.2801\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             max_f1_micro 0.30491\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_accuracy 0.28466\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_f1_macro 0.26436\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:            mean_f1_micro 0.28466\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_accuracy 0.25389\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_f1_macro 0.23877\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             min_f1_micro 0.25389\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:       time_since_restore 68.39364\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:         time_this_iter_s 68.39364\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:             time_total_s 68.39364\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:                timestamp 1723712771\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: 🚀 View run train_and_evaluate_0ae8dae0 at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/0ae8dae0\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110504-0ae8dae0/logs\n","\u001b[36m(_WandbLoggingActor pid=61640)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","2024-08-15 11:06:16,130\tINFO tune.py:1041 -- Total run time: 405.95 seconds (400.79 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.0001288241705962323, 'batch_size': 32, 'hidden_layers': (55, 75, 70, 50, 60), 'epochs': 50, 'dropout_rate': 0.2516006506162699}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a12_splits, \"A12\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2024-08-15 11:10:46</td></tr>\n","<tr><td>Running for: </td><td>00:04:28.00        </td></tr>\n","<tr><td>Memory:      </td><td>12.6/16.0 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.6272955974842768<br>Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th>hidden_layers       </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  min_accuracy</th><th style=\"text-align: right;\">  max_accuracy</th><th style=\"text-align: right;\">  min_f1_macro</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_and_evaluate_5867577c</td><td>TERMINATED</td><td>127.0.0.1:62164</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.469843</td><td style=\"text-align: right;\">      40</td><td>(70, 60, 90, 80, 35)</td><td style=\"text-align: right;\">    0.00649806 </td><td style=\"text-align: right;\">0.621617</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         51.5089</td><td style=\"text-align: right;\">      0.533333</td><td style=\"text-align: right;\">      0.72381 </td><td style=\"text-align: right;\">      0.396481</td></tr>\n","<tr><td>train_and_evaluate_c243c28d</td><td>TERMINATED</td><td>127.0.0.1:62567</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.228491</td><td style=\"text-align: right;\">      40</td><td>(55, 30, 75, 45, 50)</td><td style=\"text-align: right;\">    0.00299685 </td><td style=\"text-align: right;\">0.69389 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         48.6831</td><td style=\"text-align: right;\">      0.647619</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.595462</td></tr>\n","<tr><td>train_and_evaluate_af34135c</td><td>TERMINATED</td><td>127.0.0.1:62965</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.351425</td><td style=\"text-align: right;\">      20</td><td>(55, 45, 45, 90, 90)</td><td style=\"text-align: right;\">    0.00117155 </td><td style=\"text-align: right;\">0.627296</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         43.0945</td><td style=\"text-align: right;\">      0.590476</td><td style=\"text-align: right;\">      0.669811</td><td style=\"text-align: right;\">      0.512472</td></tr>\n","<tr><td>train_and_evaluate_477694ee</td><td>TERMINATED</td><td>127.0.0.1:63316</td><td>sigmoid     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.288142</td><td style=\"text-align: right;\">      40</td><td>(70, 50, 80, 80, 40)</td><td style=\"text-align: right;\">    0.000188535</td><td style=\"text-align: right;\">0.585606</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         46.2719</td><td style=\"text-align: right;\">      0.504762</td><td style=\"text-align: right;\">      0.733333</td><td style=\"text-align: right;\">      0.335443</td></tr>\n","<tr><td>train_and_evaluate_dca945fc</td><td>TERMINATED</td><td>127.0.0.1:63694</td><td>tanh        </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.369109</td><td style=\"text-align: right;\">      50</td><td>(55, 40, 90, 75, 30)</td><td style=\"text-align: right;\">    0.000437566</td><td style=\"text-align: right;\">0.650099</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         53.3641</td><td style=\"text-align: right;\">      0.571429</td><td style=\"text-align: right;\">      0.698113</td><td style=\"text-align: right;\">      0.498674</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_5867577c/wandb/run-20240815_110623-g21ve12z\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Syncing run trial_5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/g21ve12z\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/driver_artifacts/trial_5867577c/wandb/run-20240815_110624-5867577c\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Syncing run train_and_evaluate_5867577c\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss ███▆▅▅▆▆▄▃▄▆▄▄▄▄▃▄▁▂▃▃▅▃▄▃▃▂▂▂▂▃▁▁▁▃▂▂▃▃\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss 0.64592\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run trial_5867577c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/g21ve12z\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110623-g21ve12z/logs\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_5867577c/wandb/run-20240815_110629-206gjh5c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Syncing run trial_5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/206gjh5c\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss █▇▇▆▆▅▆▅▃▃▆▄▄▂▅▄▄▃▃▄▃▄▃▄▄▁▂▂▅▄▃▂▂▂▃▁▄▅▄▂\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss 0.60227\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run trial_5867577c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/206gjh5c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110629-206gjh5c/logs\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_5867577c/wandb/run-20240815_110639-jotwn7q7\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Syncing run trial_5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jotwn7q7\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss ███▇▆▆▅▆▅▄▃▃▄▅▄▅▅▅▄▃▄▄▃▃▂▃▄▃▂▂▄▃▃▁▃▃▃▄▃▃\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss 0.63034\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run trial_5867577c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/jotwn7q7\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110639-jotwn7q7/logs\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_5867577c/wandb/run-20240815_110648-2ga9meje\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Syncing run trial_5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2ga9meje\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss ▇▇▆█▆▅▅▆▄▅▆▄▃▅▆▆▃▄▅▃▃▄▂▂▄▃▄▂▃▂▄▃▃▃▂▄▃▁▁▃\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb:  loss 0.62328\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run trial_5867577c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/2ga9meje\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110648-2ga9meje/logs\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: | Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_5867577c/wandb/run-20240815_110657-hvqfc902\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: Syncing run trial_5867577c\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/hvqfc902\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_and_evaluate pid=62164)\u001b[0m No existing full split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_full_A21.pkl, creating new file.\n","\u001b[36m(train_and_evaluate pid=62164)\u001b[0m No existing 1 split data found at /Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/results/output_A21.pkl, creating new file\n"]},{"name":"stderr","output_type":"stream","text":["2024-08-15 11:07:14,689\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.624 s, which may be a performance bottleneck.\n","2024-08-15 11:07:14,689\tWARNING util.py:201 -- The `process_trial_result` operation took 0.625 s, which may be a performance bottleneck.\n","2024-08-15 11:07:14,690\tWARNING util.py:201 -- Processing trial results took 0.625 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:07:14,690\tWARNING util.py:201 -- The `process_trial_result` operation took 0.626 s, which may be a performance bottleneck.\n","2024-08-15 11:07:15,382\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 60, 90, 80, 35)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:      config/dropout_rate 0.46984\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:     config/learning_rate 0.0065\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_accuracy 0.72381\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_f1_macro 0.63038\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             max_f1_micro 0.72381\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_accuracy 0.62162\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_f1_macro 0.53606\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:            mean_f1_micro 0.62162\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_accuracy 0.53333\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_f1_macro 0.39648\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             min_f1_micro 0.53333\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:       time_since_restore 51.50891\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:         time_this_iter_s 51.50891\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:             time_total_s 51.50891\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:                timestamp 1723712834\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: 🚀 View run train_and_evaluate_5867577c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/5867577c\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110624-5867577c/logs\n","\u001b[36m(_WandbLoggingActor pid=62172)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_c243c28d/wandb/run-20240815_110720-72dnc2qc\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Syncing run trial_c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/72dnc2qc\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss █▇▇▇▆▇▆▆▆▅▄▄▅▄▃▂▄▅▄▄▃▂▃▂▁▂▃▁▂▂▅▃▂▃▂▂▄▄▂▃\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss 0.56695\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/driver_artifacts/trial_c243c28d/wandb/run-20240815_110720-c243c28d\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Syncing run train_and_evaluate_c243c28d\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run trial_c243c28d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/72dnc2qc\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110720-72dnc2qc/logs\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_c243c28d/wandb/run-20240815_110726-22wbyzft\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Syncing run trial_c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/22wbyzft\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss █▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▃▄▄▃▄▃▃▃▃▂▃▃▃▄▃▃▃▂▃▃▂▂▁\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss 0.47148\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run trial_c243c28d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/22wbyzft\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110726-22wbyzft/logs\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_c243c28d/wandb/run-20240815_110735-hrngsqws\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Syncing run trial_c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/hrngsqws\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss ██▇▇▇▆▅▄▆▆▅▅▃▄▄▅▃▄▂▃▃▃▃▃▃▄▄▄▃▂▂▃▃▂▂▁▂▃▂▁\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss 0.53292\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run trial_c243c28d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/hrngsqws\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110735-hrngsqws/logs\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_c243c28d/wandb/run-20240815_110744-fkdg7sxe\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Syncing run trial_c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fkdg7sxe\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss ██▇▇▆▆▅▄▆▄▅▄▄▄▅▄▅▆▅▃▃▄▄▃▃▄▄▄▃▃▂▃▄▅▃▂▁▂▄▂\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb:  loss 0.51994\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run trial_c243c28d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fkdg7sxe\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110744-fkdg7sxe/logs\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_c243c28d/wandb/run-20240815_110753-ajt418yx\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: Syncing run trial_c243c28d\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62567)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ajt418yx\n","2024-08-15 11:08:08,423\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.651 s, which may be a performance bottleneck.\n","2024-08-15 11:08:08,424\tWARNING util.py:201 -- The `process_trial_result` operation took 0.652 s, which may be a performance bottleneck.\n","2024-08-15 11:08:08,425\tWARNING util.py:201 -- Processing trial results took 0.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:08:08,425\tWARNING util.py:201 -- The `process_trial_result` operation took 0.652 s, which may be a performance bottleneck.\n","2024-08-15 11:08:09,120\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 30, 75, 45, 50)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:      config/dropout_rate 0.22849\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:     config/learning_rate 0.003\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_accuracy 0.73333\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_f1_macro 0.73136\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             max_f1_micro 0.73333\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_accuracy 0.69389\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_f1_macro 0.65151\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:            mean_f1_micro 0.69389\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_accuracy 0.64762\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_f1_macro 0.59546\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             min_f1_micro 0.64762\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:       time_since_restore 48.68311\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:         time_this_iter_s 48.68311\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:             time_total_s 48.68311\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:                timestamp 1723712887\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: 🚀 View run train_and_evaluate_c243c28d at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/c243c28d\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110720-c243c28d/logs\n","\u001b[36m(_WandbLoggingActor pid=62575)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_af34135c/wandb/run-20240815_110813-3k70hz6h\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Syncing run trial_af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/3k70hz6h\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss ██▇█▅▆▅▅▅▄▃▃▃▄▃▁▂▁▁▁\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss 0.61878\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run trial_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/3k70hz6h\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110813-3k70hz6h/logs\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/driver_artifacts/trial_af34135c/wandb/run-20240815_110813-af34135c\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Syncing run train_and_evaluate_af34135c\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_af34135c/wandb/run-20240815_110818-begy1qot\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Syncing run trial_af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/begy1qot\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss ██▇▆▆▆▅▅▄▄▃▅▂▄▁▂▁▂▁▂\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss 0.60013\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run trial_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/begy1qot\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110818-begy1qot/logs\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_af34135c/wandb/run-20240815_110825-5ikw9a4w\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Syncing run trial_af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/5ikw9a4w\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss ▇██▇▇▇▇▅▅▅▇▅▄▄▂▅▂▁▅▄\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss 0.63602\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run trial_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/5ikw9a4w\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110825-5ikw9a4w/logs\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_af34135c/wandb/run-20240815_110834-ctynyam8\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Syncing run trial_af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ctynyam8\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss ██▇▇▇▆▆▇▆▆▅▄▄▄▃▃▁▃▃▂\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss 0.60723\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run trial_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ctynyam8\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110834-ctynyam8/logs\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_af34135c/wandb/run-20240815_110842-1sv9duyk\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Syncing run trial_af34135c\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1sv9duyk\n","2024-08-15 11:08:56,133\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.803 s, which may be a performance bottleneck.\n","2024-08-15 11:08:56,134\tWARNING util.py:201 -- The `process_trial_result` operation took 0.805 s, which may be a performance bottleneck.\n","2024-08-15 11:08:56,135\tWARNING util.py:201 -- Processing trial results took 0.805 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:08:56,135\tWARNING util.py:201 -- The `process_trial_result` operation took 0.805 s, which may be a performance bottleneck.\n","2024-08-15 11:08:56,865\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 45, 45, 90, 90)}\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss █▇▆▇▅▅▆▄▄▄▄▃▄▂▂▃▃▂▁▄\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: epoch 19\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb:  loss 0.64068\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: 🚀 View run trial_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/1sv9duyk\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110842-1sv9duyk/logs\n","\u001b[36m(train_and_evaluate pid=62965)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:      config/dropout_rate 0.35143\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            config/epochs 20\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:     config/learning_rate 0.00117\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             max_accuracy 0.66981\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:            mean_accuracy 0.6273\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             min_accuracy 0.59048\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:       time_since_restore 43.09446\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:         time_this_iter_s 43.09446\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:             time_total_s 43.09446\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:                timestamp 1723712935\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_477694ee/wandb/run-20240815_110902-fr6407ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Syncing run trial_477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fr6407ee\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: 🚀 View run train_and_evaluate_af34135c at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/af34135c\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110813-af34135c/logs\n","\u001b[36m(_WandbLoggingActor pid=62973)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss █▅▇▃▄▂▃▃▂▃▃▃▄▆▃▃▄▂▃▄▄▂▄▃▃▄▃▃▁▄▄▄▄▁▁▃▂▄▄▂\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss 0.66924\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run trial_477694ee at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/fr6407ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110902-fr6407ee/logs\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/driver_artifacts/trial_477694ee/wandb/run-20240815_110902-477694ee\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Syncing run train_and_evaluate_477694ee\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_477694ee/wandb/run-20240815_110908-aq9tkbb0\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Syncing run trial_477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/aq9tkbb0\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss ▅▅█▆▆▅▅▅▄▄▂▃▄▅▆▅▅▃▄▇▄▃▆▅▃▂▄▄▄▃▃▃▄▄▃▃▆▆▁▂\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss 0.67115\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run trial_477694ee at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/aq9tkbb0\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110908-aq9tkbb0/logs\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_477694ee/wandb/run-20240815_110916-8rpwu699\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Syncing run trial_477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8rpwu699\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss █▆▆▂▅▃▄▁▅▄▃▃▄▃▂▆▃▆▂▁▄▃▃▄▃▃▇▂▂▅▁▄▁▃▄▂▂▃▂▁\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss 0.68444\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run trial_477694ee at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/8rpwu699\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110916-8rpwu699/logs\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_477694ee/wandb/run-20240815_110924-rgm0inle\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Syncing run trial_477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rgm0inle\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: - 0.002 MB of 0.006 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss ██▆▅▆▆▅▃▅▄▅▆▁▃▃▃▂▃▄▃▄▄▂▃▃▆▅▁▅▃▄▅▂▄▄▂▄▂▂▂\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: epoch 39\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb:  loss 0.67827\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run trial_477694ee at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/rgm0inle\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110924-rgm0inle/logs\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_477694ee/wandb/run-20240815_110932-ieil4cos\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: Syncing run trial_477694ee\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/ieil4cos\n","2024-08-15 11:09:47,536\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.707 s, which may be a performance bottleneck.\n","2024-08-15 11:09:47,537\tWARNING util.py:201 -- The `process_trial_result` operation took 0.709 s, which may be a performance bottleneck.\n","2024-08-15 11:09:47,538\tWARNING util.py:201 -- Processing trial results took 0.709 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:09:47,538\tWARNING util.py:201 -- The `process_trial_result` operation took 0.710 s, which may be a performance bottleneck.\n","2024-08-15 11:09:48,233\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (70, 50, 80, 80, 40)}\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:        config/batch_size 128\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:      config/dropout_rate 0.28814\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            config/epochs 40\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:     config/learning_rate 0.00019\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_accuracy 0.73333\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_f1_macro 0.42308\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             max_f1_micro 0.73333\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_accuracy 0.58561\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_f1_macro 0.36788\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:            mean_f1_micro 0.58561\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_accuracy 0.50476\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_f1_macro 0.33544\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             min_f1_micro 0.50476\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:       time_since_restore 46.27189\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:         time_this_iter_s 46.27189\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:             time_total_s 46.27189\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:                timestamp 1723712986\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: 🚀 View run train_and_evaluate_477694ee at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/477694ee\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110902-477694ee/logs\n","\u001b[36m(_WandbLoggingActor pid=63326)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_dca945fc/wandb/run-20240815_110952-cx3mrhca\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Syncing run trial_dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/cx3mrhca\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Currently logged in as: bhanu-prasanna2001. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss █▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▄▅▄▃▄▄▄▃▃▄▅▃▃▃▂▁▂▂▁▂▃▁▂\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss 0.59677\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run trial_dca945fc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/cx3mrhca\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110952-cx3mrhca/logs\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/driver_artifacts/trial_dca945fc/wandb/run-20240815_110952-dca945fc\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Syncing run train_and_evaluate_dca945fc\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_dca945fc/wandb/run-20240815_110958-lq3aok49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Syncing run trial_dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lq3aok49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - 0.002 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss █▇▇██▇▇▇▇▆▆▇▆▆▆▅▅▅▅▆▄▃▃▃▄▄▄▃▅▄▃▅▃▂▃▂▃▁▂▂\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss 0.58525\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run trial_dca945fc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/lq3aok49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110958-lq3aok49/logs\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_dca945fc/wandb/run-20240815_111008-zolulbv8\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Syncing run trial_dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/zolulbv8\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss ███▇█▇▇█▇▇▆▆▆▆▆▇▆▆▅▆▅▄▄▅▄▄▆▄▃▃▂▄▁▂▁▄▂▃▁▁\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss 0.59359\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run trial_dca945fc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/zolulbv8\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_111008-zolulbv8/logs\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_dca945fc/wandb/run-20240815_111019-qiy2yaq1\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Syncing run trial_dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qiy2yaq1\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - 0.015 MB of 0.015 MB uploaded\n","wandb:                                                                                \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run history:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss ███▇▇▇▇▆▇▅▆▆▆▆▅▅▆▅▄▄▃▃▄▄▄▂▂▃▃▂▃▁▃▁▂▂▃▃▂▁\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run summary:\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: epoch 49\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb:  loss 0.57874\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run trial_dca945fc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/qiy2yaq1\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_111019-qiy2yaq1/logs\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: - Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: \\ Waiting for wandb.init()...\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Tracking run with wandb version 0.17.6\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-15_10-43-09_737131_52755/artifacts/2024-08-15_11-06-17/train_and_evaluate_2024-08-15_11-06-17/working_dirs/trial_dca945fc/wandb/run-20240815_111028-kar0ymij\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: Syncing run trial_dca945fc\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging\n","\u001b[36m(train_and_evaluate pid=63694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Train%20Loss%20Logging/runs/kar0ymij\n","2024-08-15 11:10:45,243\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.682 s, which may be a performance bottleneck.\n","2024-08-15 11:10:45,243\tWARNING util.py:201 -- The `process_trial_result` operation took 0.684 s, which may be a performance bottleneck.\n","2024-08-15 11:10:45,244\tWARNING util.py:201 -- Processing trial results took 0.684 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2024-08-15 11:10:45,244\tWARNING util.py:201 -- The `process_trial_result` operation took 0.684 s, which may be a performance bottleneck.\n","2024-08-15 11:10:46,059\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hidden_layers': (55, 40, 90, 75, 30)}\n","2024-08-15 11:10:46,179\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/bhanuprasanna/Documents/Uniklinik-Koln/MTL/spike-sorting-multi-task/ray_results/train_and_evaluate_2024-08-15_11-06-17' in 0.1184s.\n","wandb:                                                                                \n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Run history:\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:        config/batch_size ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:      config/dropout_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            config/epochs ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:     config/learning_rate ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: iterations_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_accuracy ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_f1_macro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_f1_micro ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:       time_since_restore ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:         time_this_iter_s ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             time_total_s ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:                timestamp ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:       training_iteration ▁\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Run summary:\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:        config/batch_size 64\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:      config/dropout_rate 0.36911\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            config/epochs 50\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:     config/learning_rate 0.00044\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: iterations_since_restore 1\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_accuracy 0.69811\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_f1_macro 0.66839\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             max_f1_micro 0.69811\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_accuracy 0.6501\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_f1_macro 0.59922\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:            mean_f1_micro 0.6501\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_accuracy 0.57143\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_f1_macro 0.49867\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             min_f1_micro 0.57143\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:       time_since_restore 53.36411\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:         time_this_iter_s 53.36411\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:             time_total_s 53.36411\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:                timestamp 1723713044\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb:       training_iteration 1\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: \n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: 🚀 View run train_and_evaluate_dca945fc at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB/runs/dca945fc\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/bhanu-prasanna2001/Classic%20FFN%20PyTorch%20-%20Test%20WandB\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: Find logs at: ./wandb/run-20240815_110952-dca945fc/logs\n","\u001b[36m(_WandbLoggingActor pid=63707)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","2024-08-15 11:10:49,110\tINFO tune.py:1041 -- Total run time: 272.92 seconds (267.88 seconds for the tuning loop).\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'activation': 'tanh', 'learning_rate': 0.00299684677031004, 'batch_size': 64, 'hidden_layers': (55, 30, 75, 45, 50), 'epochs': 40, 'dropout_rate': 0.22849097764163756}\n"]}],"source":["analysis = five_fold_cross_validation(helper.a21_splits, \"A21\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["df_dict_A2 = pd.read_pickle(\"results/output_A2.pkl\")\n","df_dict_A3 = pd.read_pickle(\"results/output_A3.pkl\")\n","df_dict_A4 = pd.read_pickle(\"results/output_A4.pkl\")\n","df_dict_A12 = pd.read_pickle(\"results/output_A12.pkl\")\n","df_dict_A21 = pd.read_pickle(\"results/output_A21.pkl\")\n","\n","df_dict = pd.concat([df_dict_A2, df_dict_A3, df_dict_A4, df_dict_A12, df_dict_A21])\n","\n","df_dict = df_dict.loc[:,~df_dict.columns.duplicated()].copy()\n","df_dict = df_dict.reset_index().rename(columns={'index': 'name'})\n","df_dict = df_dict.drop_duplicates()\n","\n","df_dict.to_csv('outputs/Classic_FFN_one_split_metrics_system.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df_dict_full_A2 = pd.read_pickle(\"results/output_full_A2.pkl\")\n","df_dict_full_A3 = pd.read_pickle(\"results/output_full_A3.pkl\")\n","df_dict_full_A4 = pd.read_pickle(\"results/output_full_A4.pkl\")\n","df_dict_full_A12 = pd.read_pickle(\"results/output_full_A12.pkl\")\n","df_dict_full_A21 = pd.read_pickle(\"results/output_full_A21.pkl\")\n","\n","df_full_dict = pd.concat([df_dict_full_A2, df_dict_full_A3, df_dict_full_A4, df_dict_full_A12, df_dict_full_A21])\n","\n","df_full_dict = df_full_dict.loc[:, ~df_full_dict.columns.duplicated()].copy()\n","df_full_dict = df_full_dict.reset_index().rename(columns={'index': 'name'})\n","df_full_dict = df_full_dict.drop_duplicates()\n","\n","df_full_dict.to_csv(\"outputs/Classic_FFN_full_split_metrics_system.csv\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_dict_full_A2.to_csv(\"outputs/Classic_FFN_full_split_metrics_A2.csv\")\n","df_dict_full_A3.to_csv(\"outputs/Classic_FFN_full_split_metrics_A3.csv\")\n","df_dict_full_A4.to_csv(\"outputs/Classic_FFN_full_split_metrics_A4.csv\")\n","df_dict_full_A12.to_csv(\"outputs/Classic_FFN_full_split_metrics_A12.csv\")\n","df_dict_full_A21.to_csv(\"outputs/Classic_FFN_full_split_metrics_A21.csv\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean accuracy for config {'activation': 'sigmoid', 'learning_rate': 0.00320027036491653, 'batch_size': 32, 'hidden_layers': (50, 55, 60, 85, 90), 'epochs': 20, 'dropout_rate': 0.40970736826155685} is 0.8657667012807199\n","Mean accuracy for config {'activation': 'relu', 'learning_rate': 0.0005835127081408091, 'batch_size': 32, 'hidden_layers': (40, 40, 80, 75, 85), 'epochs': 50, 'dropout_rate': 0.2583779379085752} is 0.8728778467908903\n","Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.0008145249780491403, 'batch_size': 64, 'hidden_layers': (35, 55, 60, 65, 40), 'epochs': 40, 'dropout_rate': 0.3476360245730923} is 0.6683845468283798\n","Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.0001288241705962323, 'batch_size': 32, 'hidden_layers': (55, 75, 70, 50, 60), 'epochs': 50, 'dropout_rate': 0.2516006506162699} is 0.2893199983933807\n","Mean accuracy for config {'activation': 'tanh', 'learning_rate': 0.00299684677031004, 'batch_size': 64, 'hidden_layers': (55, 30, 75, 45, 50), 'epochs': 40, 'dropout_rate': 0.22849097764163756} is 0.6938903863432164\n","\n","\n","The best config is {'activation': 'relu', 'learning_rate': 0.0005835127081408091, 'batch_size': 32, 'hidden_layers': (40, 40, 80, 75, 85), 'epochs': 50, 'dropout_rate': 0.2583779379085752} with a mean accuracy of 0.8728778467908903\n"]}],"source":["# print the configs and results for the hyperparemters wiht the highest mean accuracy\n","# read df_full_dict and print columns with configs in best_params_list_getting\n","df_full_dict = pd.read_csv(\"outputs/Classic_FFN_full_split_metrics_system.csv\")\n","if 'Unnamed: 0' in df_full_dict.columns:\n","    df_full_dict.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","config_acc_dict = {}\n","for good_param in best_params_list_getting:\n","    # get the row witht the highest mean mean accuracy\n","    mean_accuracy = df_full_dict.loc[df_full_dict['config'] == str(good_param)]['mean_accuracy'].mean()\n","    config_acc_dict[str(good_param)] = mean_accuracy\n","    print(f\"Mean accuracy for config {good_param} is {mean_accuracy}\")\n","\n","# get the best config\n","best_config = max(config_acc_dict, key=config_acc_dict.get)\n","print(f\"\\n\\nThe best config is {best_config} with a mean accuracy of {config_acc_dict[best_config]}\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>number of classes</th>\n","      <th>min_accuracy</th>\n","      <th>max_accuracy</th>\n","      <th>mean_accuracy</th>\n","      <th>min_f1_macro</th>\n","      <th>max_f1_macro</th>\n","      <th>mean_f1_macro</th>\n","      <th>min_f1_micro</th>\n","      <th>max_f1_micro</th>\n","      <th>mean_f1_micro</th>\n","      <th>min_mcc</th>\n","      <th>max_mcc</th>\n","      <th>mean_mcc</th>\n","      <th>std_accuracy</th>\n","      <th>std_f1_macro</th>\n","      <th>std_f1_micro</th>\n","      <th>std_mcc</th>\n","      <th>config</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A2_e835aaec</td>\n","      <td>2</td>\n","      <td>0.814815</td>\n","      <td>0.897196</td>\n","      <td>0.865767</td>\n","      <td>0.814815</td>\n","      <td>0.896618</td>\n","      <td>0.865600</td>\n","      <td>0.814815</td>\n","      <td>0.897196</td>\n","      <td>0.865767</td>\n","      <td>0.629630</td>\n","      <td>0.806152</td>\n","      <td>0.734311</td>\n","      <td>0.032146</td>\n","      <td>0.031998</td>\n","      <td>0.032146</td>\n","      <td>0.066947</td>\n","      <td>{'activation': 'sigmoid', 'learning_rate': 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A3_9f8fa340</td>\n","      <td>2</td>\n","      <td>0.811594</td>\n","      <td>0.927536</td>\n","      <td>0.872878</td>\n","      <td>0.811594</td>\n","      <td>0.926984</td>\n","      <td>0.872380</td>\n","      <td>0.811594</td>\n","      <td>0.927536</td>\n","      <td>0.872878</td>\n","      <td>0.623529</td>\n","      <td>0.863900</td>\n","      <td>0.751752</td>\n","      <td>0.038055</td>\n","      <td>0.038057</td>\n","      <td>0.038055</td>\n","      <td>0.077365</td>\n","      <td>{'activation': 'relu', 'learning_rate': 0.0005...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A4_e204b1d6</td>\n","      <td>3</td>\n","      <td>0.624365</td>\n","      <td>0.712468</td>\n","      <td>0.668385</td>\n","      <td>0.615275</td>\n","      <td>0.708924</td>\n","      <td>0.661755</td>\n","      <td>0.624365</td>\n","      <td>0.712468</td>\n","      <td>0.668385</td>\n","      <td>0.464322</td>\n","      <td>0.569499</td>\n","      <td>0.509967</td>\n","      <td>0.027969</td>\n","      <td>0.030233</td>\n","      <td>0.027969</td>\n","      <td>0.034400</td>\n","      <td>{'activation': 'tanh', 'learning_rate': 0.0008...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A12_93943072</td>\n","      <td>6</td>\n","      <td>0.266839</td>\n","      <td>0.307494</td>\n","      <td>0.289320</td>\n","      <td>0.247697</td>\n","      <td>0.284539</td>\n","      <td>0.269003</td>\n","      <td>0.266839</td>\n","      <td>0.307494</td>\n","      <td>0.289320</td>\n","      <td>0.122156</td>\n","      <td>0.171728</td>\n","      <td>0.149201</td>\n","      <td>0.017598</td>\n","      <td>0.015025</td>\n","      <td>0.017598</td>\n","      <td>0.021320</td>\n","      <td>{'activation': 'tanh', 'learning_rate': 0.0001...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A21_c243c28d</td>\n","      <td>2</td>\n","      <td>0.647619</td>\n","      <td>0.733333</td>\n","      <td>0.693890</td>\n","      <td>0.595462</td>\n","      <td>0.731360</td>\n","      <td>0.651514</td>\n","      <td>0.647619</td>\n","      <td>0.733333</td>\n","      <td>0.693890</td>\n","      <td>0.276208</td>\n","      <td>0.471365</td>\n","      <td>0.345755</td>\n","      <td>0.035011</td>\n","      <td>0.051874</td>\n","      <td>0.035011</td>\n","      <td>0.078715</td>\n","      <td>{'activation': 'tanh', 'learning_rate': 0.0029...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           name  number of classes  min_accuracy  max_accuracy  mean_accuracy  \\\n","0   A2_e835aaec                  2      0.814815      0.897196       0.865767   \n","1   A3_9f8fa340                  2      0.811594      0.927536       0.872878   \n","2   A4_e204b1d6                  3      0.624365      0.712468       0.668385   \n","3  A12_93943072                  6      0.266839      0.307494       0.289320   \n","4  A21_c243c28d                  2      0.647619      0.733333       0.693890   \n","\n","   min_f1_macro  max_f1_macro  mean_f1_macro  min_f1_micro  max_f1_micro  \\\n","0      0.814815      0.896618       0.865600      0.814815      0.897196   \n","1      0.811594      0.926984       0.872380      0.811594      0.927536   \n","2      0.615275      0.708924       0.661755      0.624365      0.712468   \n","3      0.247697      0.284539       0.269003      0.266839      0.307494   \n","4      0.595462      0.731360       0.651514      0.647619      0.733333   \n","\n","   mean_f1_micro   min_mcc   max_mcc  mean_mcc  std_accuracy  std_f1_macro  \\\n","0       0.865767  0.629630  0.806152  0.734311      0.032146      0.031998   \n","1       0.872878  0.623529  0.863900  0.751752      0.038055      0.038057   \n","2       0.668385  0.464322  0.569499  0.509967      0.027969      0.030233   \n","3       0.289320  0.122156  0.171728  0.149201      0.017598      0.015025   \n","4       0.693890  0.276208  0.471365  0.345755      0.035011      0.051874   \n","\n","   std_f1_micro   std_mcc                                             config  \n","0      0.032146  0.066947  {'activation': 'sigmoid', 'learning_rate': 0.0...  \n","1      0.038055  0.077365  {'activation': 'relu', 'learning_rate': 0.0005...  \n","2      0.027969  0.034400  {'activation': 'tanh', 'learning_rate': 0.0008...  \n","3      0.017598  0.021320  {'activation': 'tanh', 'learning_rate': 0.0001...  \n","4      0.035011  0.078715  {'activation': 'tanh', 'learning_rate': 0.0029...  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["best_performers = df_full_dict.loc[df_full_dict.groupby(df_full_dict['name'].str.split('_').str[0])['mean_accuracy'].idxmax()]\n","best_performers = best_performers.sort_index().reset_index(drop=True)\n","\n","best_performers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5354302,"sourceId":8905505,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
